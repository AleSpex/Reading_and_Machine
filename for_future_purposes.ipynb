{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0aba3c-eb2b-463c-bfd2-aaa885e2df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR THE FUTURE\n",
    "#NEURAL COLLABORATIVE FILTERING?\n",
    "\n",
    "#NEURAL COLLABORATIVE FILTERING\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dot, concatenate, Dropout, Dense, BatchNormalization, StringLookup\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow import tensordot\n",
    "\n",
    "latent_dim = 10\n",
    "\n",
    "#Input definition (symbolic layers)\n",
    "book_input = Input(shape=[1],name='book-input')\n",
    "user_input = Input(shape=[1], name='user-input')\n",
    "\n",
    "#Define lookups for books and users\n",
    "#book_lookup = StringLookup(vocabulary=vocabulary_titles, mask_token=None)(book_input)\n",
    "#user_lookup = StringLookup(vocabulary=vocabulary_users, mask_token=None)(user_input)\n",
    "\n",
    "#Perceptrons layers for non linearity + Stringlookup for identifier vocabulary\n",
    "book_embedding_mlp = Embedding(num_books + 1, latent_dim, name='book-embedding-mlp')(book_input)\n",
    "book_vec_mlp = Flatten(name='flatten-book-mlp')(book_embedding_mlp)\n",
    "\n",
    "user_embedding_mlp = Embedding(num_users + 1, latent_dim, name='user-embedding-mlp')(user_input)\n",
    "user_vec_mlp = Flatten(name='flatten-user-mlp')(user_embedding_mlp)\n",
    "\n",
    "#Embeddings for matrix factorization to concatenate to perceptrons embeddings\n",
    "book_embedding_mf = Embedding(num_books + 1, latent_dim, name='book-embedding-mf')(book_input)\n",
    "book_vec_mf = Flatten(name='flatten-book-mf')(book_embedding_mf)\n",
    "\n",
    "user_embedding_mf = Embedding(num_users + 1, latent_dim, name='user-embedding-mf')(user_input)\n",
    "user_vec_mf = Flatten(name='flatten-user-mf')(user_embedding_mf)\n",
    "\n",
    "# CONCATENATION + DROPOUT LAYERS + DENSE LAYERS\n",
    "concat = concatenate([book_vec_mlp, user_vec_mlp], name='concatenate_embedd_mlp')\n",
    "concat_dropout = Dropout(0.2)(concat)\n",
    "fc_1 = Dense(100, name='fc-1', activation='relu')(concat_dropout)\n",
    "fc_1_bn = BatchNormalization(name='batch-norm-1')(fc_1)\n",
    "fc_1_dropout = Dropout(0.2)(fc_1_bn)\n",
    "fc_2 = Dense(50, name='fc-2', activation='relu')(fc_1_dropout)\n",
    "fc_2_bn = BatchNormalization(name='batch-norm-2')(fc_2)\n",
    "fc_2_dropout = Dropout(0.2)(fc_2_bn)\n",
    "\n",
    "# COMBINE PREDICTIONS\n",
    "pred_mlp = Dense(10, name='pred-mlp', activation='relu')(fc_2_dropout)\n",
    "pred_mf = Dot(axes=1, name='pred-mf')([book_vec_mf, user_vec_mf]) #DOT PRODUCT FOR MATRIX FACTORIZATION\n",
    "#print(pred_mf.shape)\n",
    "combine_mlp_mf = concatenate([pred_mf, pred_mlp], name='combine-mlp-mf')\n",
    "\n",
    "# OUTPUT LAYERS, ONE SIGMOID EXIT\n",
    "result = Dense(1, name='result', activation='sigmoid')(combine_mlp_mf)\n",
    "\n",
    "model = Model([user_input, book_input], result)\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc42b10-6576-4b3f-b3f5-d462223d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the future\n",
    "#Extended book vector in retrieval?\n",
    "from typing import Dict, Text\n",
    "\n",
    "class BookTower(tf.keras.layers.Layer):\n",
    "  def __init__(self, embedding_dimension):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.embedding_dimension = embedding_dimension\n",
    "    \n",
    "    self.identifier_user = tf.keras.layers.StringLookup(\n",
    "        vocabulary=users_vocabulary_2, mask_token=None)\n",
    "    self.identifier_book = tf.keras.layers.StringLookup(\n",
    "        vocabulary=titles_vocabulary_2, mask_token=None)\n",
    "    self.moods = tf.keras.layers.StringLookup(\n",
    "        vocabulary=mood_vocabulary, mask_token=None)\n",
    "    self.genres = tf.keras.layers.StringLookup(\n",
    "        vocabulary=genres_vocabulary, mask_token=None)\n",
    "    self.emotions = tf.keras.layers.StringLookup(\n",
    "        vocabulary=emotion_vocabulary, mask_token=None)\n",
    "    self.keywords = tf.keras.layers.StringLookup(\n",
    "        vocabulary=keyword_vocabulary, mask_token=None)#, output_mode='multi_hot')\n",
    "    \n",
    "    self.embedding_book = tf.keras.layers.Embedding(len(titles_vocabulary_2) + len(mood_vocabulary) + len(emotion_vocabulary) + 1 + len(genres_vocabulary) + len(keyword_vocabulary), self.embedding_dimension)    \n",
    "    \n",
    "  def call(self, list_features): \n",
    "    #BOOKS\n",
    "    keyword_lookup = self.keywords(list_features[1])\n",
    "    mood_lookup = self.moods(list_features[2])\n",
    "    emotion_lookup = self.emotions(list_features[3])\n",
    "    genre_lookup = self.genres(list_features[4])  \n",
    "    book_lookup = self.identifier_book(list_features[0])\n",
    "    \n",
    "    #BOOK CONCAT\n",
    "    concats = tf.concat([book_lookup, mood_lookup, emotion_lookup, genre_lookup, keyword_lookup], axis=0)\n",
    "    book_embeddings = self.embedding_book(concats)\n",
    "    return book_embeddings\n",
    "\n",
    "class UsersBooksModel2(tfrs.Model):\n",
    "\n",
    "  def __init__(self, embedding_dimension, features: Dict[Text, tf.Tensor]):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.embedding_dimension = embedding_dimension\n",
    "    self.layers_size = layers\n",
    "    \n",
    "    self.book_tower = BookTower(self.embedding_dimension)\n",
    "    \n",
    "    #Embedding per utenti\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=users_vocabulary_2, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(users_vocabulary_2) + 1, self.embedding_dimension),\n",
    "    ])\n",
    "    \n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "      metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates=\n",
    "            TFdata_books_features.batch(128).cache().map(self.book_tower)\n",
    "      )\n",
    "    )\n",
    "    if layers != None:\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        for layer_size in layers[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "        # No activation for the last layer.\n",
    "        for layer_size in layers[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "            \n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:   \n",
    "    if self.layers_size == None:\n",
    "        user_embeddings = self.user_embeddings(features['person_id'])\n",
    "        book_embeddings = self.book_tower([features['title'], features['keyword_string'], features['mood_string'], features['emotion_string'], features['genre_string']])\n",
    "    else:\n",
    "        user_embeddings = self.dense_layers(self.user_embeddings(features['person_id']))\n",
    "        book_embeddings = self.dense_layers(self.book_tower(features['title'], features['keyword_string'], features['mood_string'], features['emotion_string'], features['genre_string']))\n",
    "    return self.task(user_embeddings, book_embeddings, compute_metrics = not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b3b05-b636-45d1-ab23-e01c31820b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extended user vector with personalized layer\n",
    "\n",
    "class UserTower(tf.keras.layers.Layer):\n",
    "  def __init__(self, user_inputs, embedding_dimension, genre_features=None): #user_inputs Ã¨ il vocabolario di utenti\n",
    "    super().__init__()\n",
    "    self.genre_features = genre_features\n",
    "    self.embedding_dimension = embedding_dimension\n",
    "    self.user_inputs = user_inputs\n",
    "    self.list_lookups = [tf.keras.layers.StringLookup(vocabulary=self.user_inputs, mask_token=None)]\n",
    "    self.embed = tf.keras.layers.Embedding(len(self.user_inputs) + 1, self.embedding_dimension)\n",
    "    if genre_features is not None:\n",
    "        if isinstance(genre_features, list):     \n",
    "            lookup_users = tf.keras.layers.StringLookup(vocabulary=genre_features, mask_token=None)\n",
    "            self.list_lookups.append(lookup_users)\n",
    "            self.global_pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        else:\n",
    "            raise Exception(\"string_features deve essere una lista\")\n",
    "    \n",
    "  def call(self, x):    \n",
    "    output = self.list_lookups[0](x[0])\n",
    "    lookups = [output]\n",
    "    if self.genre_features == None:\n",
    "        output = self.embed(output)\n",
    "        return output\n",
    "    else:     \n",
    "        #if isinstance(genre_features_1, list):\n",
    "        for index, data in enumerate(x[1:]):\n",
    "            output = self.list_lookups[1](x[index + 1])\n",
    "            lookups.append(output)   \n",
    "\n",
    "        concat = tf.concat([[lookup for lookup in lookups]], axis=0)\n",
    "        transp = self.embed(tf.transpose(concat))\n",
    "        return self.global_pooling(transp)\n",
    "\n",
    "x = UserTower(unique_users, 4, unique_books_genres) #USE OF USER TOWER\n",
    "emb = x([['Alessandro', '1', '2', '3']])\n",
    "print(emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
