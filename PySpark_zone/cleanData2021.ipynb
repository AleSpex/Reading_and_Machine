{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341f9886-9da6-4867-b8a9-31db685a5b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Anobii data\n",
      "Raws data have been read\n",
      "Filter Anobii data (select books in italian, drop comics and books with a low number of ratings)\n"
     ]
    }
   ],
   "source": [
    "##Python .py code from .jpynb:\n",
    "from pyspark.sql.functions import rand, col\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.types import IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import row_number, lit, dense_rank, concat_ws\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "#############################################\n",
    "#            read data                      # \n",
    "#############################################\n",
    "print(\"Read Anobii data\")\n",
    "\n",
    "# Read table which contains for each item_id (id of each book), the id of the author (author_id) who wrote that book\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/author_item.csv\"\n",
    "DFauthorbooks = spark.read.csv(file,header=True) #MOSTRA I LIBRI RELATIVI AGLI AUTORI\n",
    "\n",
    "# Read table which contains for each author_id (id of each auhtor), the info of that author\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/author_display.csv\"\n",
    "DFdisplay = spark.read.csv(file,header=True) #MOSTRA GLI AUTORI DEL DATABASE\n",
    "\n",
    "# Read table which contains the mapping of the language (language=11 means italian)\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/language_mapping.csv\"\n",
    "DFlanguages = spark.read.csv(file,header=True)\n",
    "\n",
    "# Read table which contains the info about items, i.e. books\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/item.csv\"\n",
    "DFitems_anobii = spark.read.csv(file,header=True) \n",
    "# DFitems_anobii.show(1, False)\n",
    "\n",
    "# Read table which contains the rate given by a person to a book\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/link_person_item.csv\" #item_review ha il voto dato dall'utente a quel libro\n",
    "DFinfo = spark.read.csv(file,header=True)\n",
    "# DFinfo.show(1, False)\n",
    "\n",
    "# Read table which contains the genre of each book\n",
    "file = \"anobii_genres/new_genres5.csv\"\n",
    "DFgenres = spark.read.csv(file, header=True)\n",
    "#DFgenres.filter(DFgenres.itemid == \"1981748\").show(10, False)\n",
    "\n",
    "file = \"anobii_genres/content.csv\"\n",
    "DFdescriptions = spark.read.csv(file, header=True)\n",
    "#DFdescriptions.show(1, False)\n",
    "\n",
    "#DFdescriptions.filter(DFdescriptions.item_id==\"2015441\").show(2, False)\n",
    "#FUNZIONI\n",
    "file = \"/data/SMARTDATA/books/anobii/link_person_item_comment.csv\"\n",
    "DFreviews = spark.read.csv(file, header=True)\n",
    "#DFreviews.show(1, False)\n",
    "\n",
    "def title_and_subtitle(line): #Usata per splittare titolo e sottotitolo nel database delle biblioteche e renderli più simili ad anobii\n",
    "    if len(line.title.split(\" : \")) > 1: #Il libro possiede un sottotitolo\n",
    "        title = line.title.split(\" : \")[0]\n",
    "        sub_title = line.title.split(\" : \")[1]\n",
    "        return Row(line.manifestation_id_new, line.patron_id_md5, title, sub_title, line.author, 4, \"None\", 4, \"None\", line.ISBNISSN_new, \"bct\") #NE APPROFITTO ANCHE PER INSERIRE VOTO E CAMPI PER IL MERGE (PER ORA NULL)\n",
    "    else: #Il libro non possiede un sottotitolo\n",
    "        return Row(line.manifestation_id_new, line.patron_id_md5, line.title, \"None\", line.author, 4, \"None\", 4, \"None\", line.ISBNISSN_new, \"bct\")\n",
    "\n",
    "def create_user_dictionary(rdd): #Used to assign an integer id to each user of the rdd (to get the rows of the CSR)\n",
    "    rdd = rdd.map(lambda x: (str(x.person_id), list(x))).sortByKey()\n",
    "    user_dictionary = rdd.countByKey()\n",
    "    i = 0\n",
    "    for key in user_dictionary.keys():\n",
    "        user_dictionary[key] = i\n",
    "        i += 1\n",
    "    return user_dictionary\n",
    "\n",
    "def create_book_dictionary(rdd): #Used to assign an integer id to each book of the rdd (to get the column of the CSR)\n",
    "    rdd = rdd.map(lambda x: (x.book_id, list(x))).sortByKey()\n",
    "    book_dictionary = rdd.countByKey()\n",
    "    i = 0\n",
    "    for key in book_dictionary.keys():\n",
    "        book_dictionary[key] = i\n",
    "        i += 1\n",
    "    return book_dictionary\n",
    "\n",
    "def addDataType1(line):\n",
    "    return Row(line.item_id, line.person_id, line.title, str(line.sub_title), line.item_review, line.total_review, line.average_rating, str(line.total_votes), line.isbn, \"anobii\")\n",
    "\n",
    "\n",
    "def addRowColumnId(line): #Funzione usata per aggiungere gli indici per ogni libro e utente\n",
    "    book_number = book_dictionary[line.book_id] \n",
    "    user_number = user_dictionary[line.person_id]\n",
    "    return Row(line.book_id,\n",
    "               line.title,\n",
    "               line.sub_title,\n",
    "               line.total_wishlist,\n",
    "               line.no_of_page,\n",
    "               line.publication_date,\n",
    "               line.publisher,\n",
    "               line.binding,\n",
    "               line.edition,\n",
    "               line.product_type,\n",
    "               line.total_votes,\n",
    "               line.data_type,\n",
    "               line.person_id,\n",
    "               line.item_review,\n",
    "               line.author,\n",
    "               line.genre,\n",
    "               line.encrypt_item_id,\n",
    "               line.isbn,\n",
    "               line.total_count,\n",
    "               line.average_rating,\n",
    "               line.total_review,\n",
    "               str(user_number), \n",
    "               str(book_number))\n",
    "    return Row(line.person_id, line.title, str(line.sub_title), line.item_review, line.data_type, line.book_id, line.author, line.genre, line.encrypt_item_id, line.total_votes_or_loans, line.average_rating, line.total_review, line.isbn, str(user_number), str(book_number))\n",
    "\n",
    "def title_and_subtitle_books(line): #FUNZIONE USATA PER DIVIDERE TITOLO E SOTTOTITOLO NEL DATASET DEI LIBRI DELLE BIBLIOTECHE E PER ELIMINARE EDITION_DATE ED EDITION_LANGUAGE\n",
    "    if len(line.title.split(\" : \")) > 1:\n",
    "        title = line.title.split(\" : \")[0]\n",
    "        sub_title = line.title.split(\" : \")[1]\n",
    "    else:\n",
    "        title = line.title\n",
    "        sub_title = \"None\"\n",
    "    #\"title\", \"sub_title\", \"author\", \"publisher\", \"book_id\"\n",
    "    return Row(title, sub_title, line.author, line.publisher, line.manifestation_id_new, line.ISBNISSN_new)\n",
    "\n",
    "def concatenateContentsDescriptions(line1, line2):\n",
    "    #print(line1)\n",
    "    #Caso in cui vi è più di una descrizione presente per uno stesso libro\n",
    "    return Row(book_id=line1.book_id, title=line1.title, sub_title=line1.sub_title, \n",
    "               total_wishlist=line1.total_wishlist, no_of_page=line1.no_of_page, publication_date=line1.publication_date, \n",
    "               publisher=line1.publisher, binding=line1.binding, edition=line1.edition, product_type=line1.product_type, \n",
    "               total_votes=line1.total_votes, data_type=line1.data_type, author=line1.author, genre=line1.genre, \n",
    "               encrypt_item_id=line1.encrypt_item_id, isbn=line1.isbn, total_count=line1.total_count, average_rating=line1.average_rating, \n",
    "               total_review=line1.total_review, book_index=line1.book_index, content=line1.content + \" \" + line2.content, content2=line1.content2)\n",
    "\n",
    "def toSingleDescriptions(line1):\n",
    "    #print(line1)\n",
    "    return Row(book_id=line1[1].book_id, title=line1[1].title, sub_title=line1[1].sub_title, \n",
    "               total_wishlist=line1[1].total_wishlist, no_of_page=line1[1].no_of_page, publication_date=line1[1].publication_date, \n",
    "               publisher=line1[1].publisher, binding=line1[1].binding, edition=line1[1].edition, product_type=line1[1].product_type, \n",
    "               total_votes=line1[1].total_votes, data_type=line1[1].data_type, author=line1[1].author, genre=line1[1].genre, \n",
    "               encrypt_item_id=line1[1].encrypt_item_id, isbn=line1[1].isbn, total_count=line1[1].total_count, average_rating=line1[1].average_rating, \n",
    "               total_review=line1[1].total_review, book_index=line1[1].book_index, content=line1[1].content, content2=line1[1].content2)\n",
    "\n",
    "def concatenateContentsComments(line1, line2):\n",
    "    #print(line1)\n",
    "    #Caso in cui dobbiamo concatenare i commenti di un libro alla sua descrizione\n",
    "    return Row(book_id=line1.book_id, title=line1.title, sub_title=line1.sub_title, \n",
    "               total_wishlist=line1.total_wishlist, no_of_page=line1.no_of_page, publication_date=line1.publication_date, \n",
    "               publisher=line1.publisher, binding=line1.binding, edition=line1.edition, product_type=line1.product_type, \n",
    "               total_votes=line1.total_votes, data_type=line1.data_type, author=line1.author, genre=line1.genre, \n",
    "               encrypt_item_id=line1.encrypt_item_id, isbn=line1.isbn, total_count=line1.total_count, average_rating=line1.average_rating, \n",
    "               total_review=line1.total_review, book_index=line1.book_index, content=line1.content, content2=line1.content2, comment_content = str(line1.comment_content) + \" \" + str(line2.comment_content))\n",
    "\n",
    "def mergeDescriptionComments(line1):\n",
    "    return Row(book_id=line1.book_id, title=line1.title, sub_title=line1.sub_title, \n",
    "               total_wishlist=line1.total_wishlist, no_of_page=line1.no_of_page, publication_date=line1.publication_date, \n",
    "               publisher=line1.publisher, binding=line1.binding, edition=line1.edition, product_type=line1.product_type, \n",
    "               total_votes=line1.total_votes, data_type=line1.data_type, author=line1.author, genre=line1.genre, \n",
    "               encrypt_item_id=line1.encrypt_item_id, isbn=line1.isbn, total_count=line1.total_count, average_rating=line1.average_rating, \n",
    "               total_review=line1.total_review, book_index=line1.book_index, content2=line1.content2, content=str(line1.content) + \" \" + str(line1.comment_content))\n",
    "\n",
    "def toSingleComments(line1):\n",
    "    #print(line1)\n",
    "    return Row(book_id=line1[1].book_id, title=line1[1].title, sub_title=line1[1].sub_title, \n",
    "               total_wishlist=line1[1].total_wishlist, no_of_page=line1[1].no_of_page, publication_date=line1[1].publication_date, \n",
    "               publisher=line1[1].publisher, binding=line1[1].binding, edition=line1[1].edition, product_type=line1[1].product_type, \n",
    "               total_votes=line1[1].total_votes, data_type=line1[1].data_type, author=line1[1].author, genre=line1[1].genre, \n",
    "               encrypt_item_id=line1[1].encrypt_item_id, isbn=line1[1].isbn, total_count=line1[1].total_count, average_rating=line1[1].average_rating, \n",
    "               total_review=line1[1].total_review, book_index=line1[1].book_index, content2=line1[1].content2, content=line1[1].content, comment_content = line1[1].comment_content)\n",
    "print(\"Raws data have been read\")\n",
    "#these are the columns which are needed for our work, for the item table. The others can be dropped\n",
    "DFitems_anobii_cols_to_keep = [\n",
    "                        \"item_id\",\\\n",
    "                        \"isbn\",\\\n",
    "#                         \"family_id\",\\\n",
    "                        \"title\",\\\n",
    "                        \"sub_title\",\\\n",
    "                        # \"barcode\",\\\n",
    "                        # \"image_source\",\\\n",
    "                        # \"image_width\",\\\n",
    "                        # \"image_height\",\\\n",
    "                        \"no_of_page\",\\\n",
    "                        \"publication_date\",\\\n",
    "                        \"publisher\",\\\n",
    "                        \"binding\",\\\n",
    "                        \"edition\",\\\n",
    "                        # \"reading_level\",\\\n",
    "                        # \"height\",\\\n",
    "                        # \"height_unit\",\\\n",
    "                        # \"length\",\\\n",
    "                        # \"length_unit\",\\\n",
    "                        # \"width\",\\\n",
    "                        # \"width_unit\",\\\n",
    "                        # \"weight\",\\\n",
    "                        # \"weight_unit\",\\\n",
    "                        # \"salesrank\",\\\n",
    "                        # \"item_popularity\",\\\n",
    "                        # \"check_same_family\",\\\n",
    "                        # \"check_internal_family\",\\\n",
    "                        # \"last_update\",\\\n",
    "                        \"average_rating\",\\\n",
    "                        \"total_review\",\\\n",
    "                        \"product_type\",\\\n",
    "                        # \"title_foreign\",\\\n",
    "                        # \"image_process\",\\\n",
    "                        # \"amazon\",\\\n",
    "                        # \"google\",\\\n",
    "                        \"encrypt_item_id\",\\\n",
    "                        # \"pid\",\\\n",
    "                        # \"binding_id\",\\\n",
    "                        # \"problem\",\\\n",
    "                        \"language\",\\\n",
    "                        # \"language_correct\",\\\n",
    "                        # \"ean\",\\\n",
    "                        # \"family_head\",\\\n",
    "                        # \"google_time\",\\\n",
    "                        # \"total_world\",\\\n",
    "                        # \"lock\",\\\n",
    "                        # \"volumes\",\\\n",
    "                        # \"publication_country_id\",\\\n",
    "                        # \"added_date\",\\\n",
    "                        # \"publishing_status\",\\\n",
    "                        # \"total_libraries\",\\\n",
    "                        # \"unavailable_date\",\\\n",
    "                        # \"table_of_contents_html\",\\\n",
    "                        # \"product_form_detail\",\\\n",
    "                        # \"total_edition_ratings\",\\\n",
    "                        # \"epub_url\",\\\n",
    "                        # \"imprint_name\",\\\n",
    "                        # \"is_sellable\",\\\n",
    "                        # \"total_topics\",\\\n",
    "                        # \"publisher_id\",\\\n",
    "                        \"total_wishlist\",\\\n",
    "                        # \"embargo_date\",\\\n",
    "                        # \"data_source\",\\\n",
    "                        # \"ebook_type\",\\\n",
    "                        # \"has_ebook\",\\\n",
    "                        # \"fulfillment_book_id\",\\\n",
    "                        # \"ebook_filesize\",\\\n",
    "                        # \"sample_status\",\\\n",
    "                        # \"sample_url\",\\\n",
    "                        # \"sample_filesize\",\\\n",
    "                      ]\n",
    "DFitems_anobii = DFitems_anobii.select(DFitems_anobii_cols_to_keep)\n",
    "print(\"Filter Anobii data (select books in italian, drop comics and books with a low number of ratings)\")\n",
    "#FILTRAGGIO 1: linguaggio italiano\n",
    "#Mi basterà filtrare da item.csv tutte le tuple con language diverso da 11.\n",
    "\n",
    "DFfilteredlanguageitems = DFitems_anobii.filter(DFitems_anobii.language == \"11\")\n",
    "DFitems_anobii = DFitems_anobii.select(DFitems_anobii_cols_to_keep)\n",
    "#DFfilteredlanguageitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 2: solo \"hardcover\" e \"paperback\"\n",
    "\n",
    "DFfilteredbindingitems = DFfilteredlanguageitems.filter((DFfilteredlanguageitems.binding == \"Paperback\") | (DFfilteredlanguageitems.binding == \"Hardcover\"))\n",
    "#DFfilteredbindingitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 3: eliminare i fumetti (per quanto possibile)\n",
    "#Userò come parole chiave personaggi famosi Disney quali Topolino e Paperino e i fumetti Bonelli (che fra gli italiani\n",
    "#sono probabilmente i più gettonati)\n",
    "\n",
    "DFfilteredmagazineitems = DFfilteredbindingitems.filter(~(DFfilteredbindingitems.title.contains(\"Topolino\")) | (DFfilteredbindingitems.title.contains(\"Paperino\"))|(DFfilteredbindingitems.title.contains(\"Tex\"))|(DFfilteredbindingitems.title.contains(\"Dylan Dog\"))|(DFfilteredbindingitems.title.contains(\"Nathan Never\"))|(DFfilteredbindingitems.title.contains(\"Zagor\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cbad8b-6ad5-47e9-b96f-e9a110184389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFgenres.filter(DFgenres.votes.isNull()).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e51d1714-dab7-43e3-8ba0-c433d7a4c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESSING DATAFRAME GENRES#\n",
    "\n",
    "#Fase 1: eliminare le tuple con SelfHelp, Textbook e Reference come genere\n",
    "#DFgenres.filter(DFgenres.name == \"Textbook\").show(1, False)\n",
    "DFgenres = DFgenres.filter((DFgenres.name != \"SelfHelp\") & (DFgenres.name != \"Reference\") & (DFgenres.name != \"Textbook\"))\n",
    "\n",
    "#Fase 1.2: eliminare le tuple con votes = null (esistono)\n",
    "\n",
    "DFgenres = DFgenres.filter(~(DFgenres.votes.isNull()))\n",
    "#DFgenres.filter(DFgenres.votes.isNull()).show(10, False)\n",
    "#Fase 2: mappare i vari generi alla corrispondente aggregazione scelta da Greta e aggregare eventuali duplicati\n",
    "#NOTA: usiamo un RDD\n",
    "\n",
    "RDDgenres = DFgenres.rdd\n",
    "\n",
    "def mapGenres(line):\n",
    "    transformed_line = line.name\n",
    "    if transformed_line == \"Cooking-Food&Wine\" or transformed_line == \"Games\" or transformed_line == \"Crafts&Hobbies\" or transformed_line == \"Home&Gardening\" or transformed_line == \"Music\" or transformed_line == \"Art-Architecture&Photography\" or transformed_line == \"Sports-Outdoors&Adventure\" or transformed_line == \"Entertainment\":\n",
    "        transformed_line = \"FreeTime\"\n",
    "    elif transformed_line == \"Professional&Technical\" or transformed_line == \"Computer&Technology\" or transformed_line == \"Law\" or transformed_line == \"Medicine\" or transformed_line == \"Business&Economics\" or transformed_line == \"ForeignLanguageStudy\" or transformed_line == \"Education&Teaching\":\n",
    "        transformed_line = \"Professional&Technical\"\n",
    "    elif transformed_line == \"Health-Mind&Body\" or transformed_line == \"Religion&Spirituality\":\n",
    "        transformed_line = \"Health-Mind&Body\"\n",
    "    elif transformed_line == \"Family-Sex&Relationships\" or transformed_line == \"Gay&Lesbian\":\n",
    "        transformed_line = \"Family-Sex&Relationships\"\n",
    "    elif transformed_line == \"Science&Nature\" or transformed_line == \"Pets\":\n",
    "        transformed_line = \"Science&Nature\"\n",
    "    elif transformed_line == \"Children\" or transformed_line == \"Teens\":\n",
    "        transformed_line = \"Children&Teens\"\n",
    "        \n",
    "    return Row(id=line.id, familyid=line.familyid, itemid=line.itemid, categoryid=line.categoryid, slug=line.slug, name=transformed_line, languageid=line.languageid, votes = line.votes)\n",
    "\n",
    "#NOTA: slug è rimasto invariato nel caso servisse il genere originale da cui è stata fatta l'aggregazione (nel dubbio)\n",
    "\n",
    "RDDgenres_mapped = RDDgenres.map(mapGenres)\n",
    "DFgenres = RDDgenres_mapped.toDF([])\n",
    "\n",
    "#Fase 2.2: aggreghiamo i nuovi generi duplicati sommando i voti (partitionBy)\n",
    "\n",
    "DFgenres = DFgenres.select(\"*\", F.sum(DFgenres.votes).over(Window.partitionBy(DFgenres.itemid, DFgenres.name)).alias(\"new_votes\")).drop(DFgenres.votes)\n",
    "DFgenres = DFgenres.withColumnRenamed(\"new_votes\", \"votes\").dropDuplicates([\"itemid\", \"name\"])\n",
    "\n",
    "#DFgenres.filter(DFgenres.itemid == \"10000108\").show(3, False)\n",
    "\n",
    "#Fase 3: nel caso di libri con più di 3 generi, tagliare facendo rimanere solo i primi 3 generi per voti\n",
    "DFgenres = DFgenres.withColumn(\"rank\", row_number().over(Window.partitionBy(DFgenres.itemid).orderBy(col(\"votes\").desc()))).filter(col(\"rank\") <= 3)\n",
    "#DFgenres.show(20, False)\n",
    "\n",
    "#Fase 4: aggregazione dei libri con più genere sotto un'unica tupla contenente la lista dei suoi generi \n",
    "#(segnarsi i voti di ogni genere se servisse). Torniamo agli RDD\n",
    "\n",
    "DFgenres = DFgenres.select(concat_ws(\"/\", DFgenres.name, DFgenres.votes).alias(\"genre_with_votes\"), \"id\", \"familyid\", \"itemid\", \"categoryid\", \"languageid\", \"slug\")\n",
    "#DFgenres.show(5000, False)\n",
    "\n",
    "RDDgenres = DFgenres.rdd\n",
    "RDDgenres_pair = RDDgenres.map(lambda x: (x.itemid, x))\n",
    "\n",
    "def aggregateGenres(line1, line2):\n",
    "    return Row(id=line1.id, familyid=line1.familyid, itemid=line1.itemid, categoryid=line1.categoryid, slug=line1.slug, languageid=line1.languageid, genre_with_votes = line1.genre_with_votes + \" \" + line2.genre_with_votes)\n",
    "\n",
    "RDDgenres_reduced = RDDgenres_pair.reduceByKey(aggregateGenres)\n",
    "\n",
    "def toSingleGenres(line1):\n",
    "    return Row(id=line1[1].id, familyid=line1[1].familyid, itemid=line1[1].itemid, categoryid=line1[1].categoryid, slug=line1[1].slug, languageid=line1[1].languageid, genre_with_votes = line1[1].genre_with_votes)\n",
    "\n",
    "RDDgenres_single = RDDgenres_reduced.map(toSingleGenres)\n",
    "\n",
    "def toDictGenre(line1):\n",
    "    dict_genres = {}\n",
    "    genre_vote_list = line1.genre_with_votes.split(\" \")\n",
    "    for genre_vote in genre_vote_list:\n",
    "        genre = genre_vote.split(\"/\")[0]\n",
    "        vote = genre_vote.split(\"/\")[1]\n",
    "        dict_genres[genre] = vote\n",
    "\n",
    "    return Row(id=line1.id, familyid=line1.familyid, itemid=line1.itemid, categoryid=line1.categoryid, slug=line1.slug, languageid=line1.languageid, genre_with_votes = dict_genres)\n",
    "\n",
    "RDDgenres_single = RDDgenres_single.map(toDictGenre)\n",
    "DFgenres = RDDgenres_single.toDF([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c6d667e-e4b6-4c77-a8f7-498ba15e57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Concateniamo i contenuti delle descrizioni con quelli dei commenti##\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Filtriamo le recensioni con almeno... 100 parole? Da valutare\n",
    "DFreviews = DFreviews.filter(F.size(F.split('comment_content', ' ')) >= 120)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c7698a2-093c-45f2-beb3-801eed6ba353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFfilteredmagazineitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 4: libri con più di X votes. Per vedere i voti di ogni libro dovrei usare il dataframe link_person_item.csv.\n",
    "#Come possiamo notare, alcune recensione hanno valore 0: questo non vuol dire che il libro è stato valutato\n",
    "#atrocemente ma che l'utente ha letto il libro senza inserire voti.\n",
    "\n",
    "#Dobbiamo dunque scremare tali voti da questo dataframe.\n",
    "\n",
    "# file = \"/data/SMARTDATA/books/anobii/link_person_item.csv\" #MOSTRA IL VOTO DI UN UTENTE A UN LIBRO\n",
    "# DFstars = spark.read.csv(file,header=True)\n",
    "#DFstars.show(50, False)\n",
    "\n",
    "DFstarsfilteredno0 = DFinfo.filter(DFinfo.item_review > 0)\n",
    "#DFstarsfilteredno0.show(20,False)\n",
    "\n",
    "#Contiamo i libri con più voti e visualizziamoli in ordine discendente\n",
    "\n",
    "DFgrouped = DFstarsfilteredno0.groupby(\"item_id\").count().withColumnRenamed(\"count\", \"total_votes\")\\\n",
    "                              .sort(\"total_votes\", ascending=False)\n",
    "#I voti dei libri più votati si aggirano intorno ai 10^4 come ordine di grandezza per cui per ora scelgo 300 come\n",
    "#soglia di voti necessaria a rimanere nel dataframe (soggetto a cambiamenti)\n",
    "DFgroupedfiltered = DFgrouped.filter(DFgrouped['total_votes'] > 100) #NUOVA SOGLIA: 150 PER 2500 LIBRI ANOBII\n",
    "DFjoinbookstars = DFfilteredmagazineitems.join(DFgroupedfiltered, DFfilteredmagazineitems.item_id == DFgroupedfiltered.item_id).drop(DFgroupedfiltered.item_id)\n",
    "# DFjoinbookstars.show(1, False)\n",
    "# DFjoinbookstars:\n",
    "# +-------+----------+-----------------+---------+----------+----------------+---------+---------+-------+----------------+------------+------------+------------------+--------+--------------+-----------+\n",
    "# |item_id|isbn      |title            |sub_title|no_of_page|publication_date|publisher|binding  |edition|average_rating  |total_review|product_type|encrypt_item_id   |language|total_wishlist|total_votes|\n",
    "# +-------+----------+-----------------+---------+----------+----------------+---------+---------+-------+----------------+------------+------------+------------------+--------+--------------+-----------+\n",
    "# |1981748|8845260372|Giulietta squeenz|null     |209       |2008-05-01      |Bompiani |Paperback|null   |3.18773234200743|129         |1           |014f24ec9629744c88|11      |110           |543        |\n",
    "# +-------+----------+-----------------+---------+----------+----------------+---------+---------+-------+----------------+------------+------------+------------------+--------+--------------+-----------+\n",
    "\n",
    "#FILTRAGGIO 5: Pulizie varie ed eventuali\n",
    "#DFfilteredlessthan10items.filter(DFfilteredlessthan10items.total_review > 100).show()\n",
    "\n",
    "#NOTA: prendo i titoli con le minuscole per facilità di merging con le biblioteche\n",
    "DFmanifestations_definitive_anobii = DFjoinbookstars.select(\"item_id\", \\\n",
    "                                                            lower(DFjoinbookstars.title), \\\n",
    "                                                            lower(DFjoinbookstars.sub_title), \\\n",
    "                                                            \"isbn\",\\\n",
    "                                                            \"average_rating\", \\\n",
    "                                                            \"total_review\", \\\n",
    "                                                            \"total_wishlist\", \\\n",
    "                                                            \"no_of_page\",\n",
    "                                                            \"publication_date\",\n",
    "                                                            \"publisher\",\n",
    "                                                            \"binding\",\n",
    "                                                            \"edition\",\n",
    "                                                            \"product_type\",\n",
    "                                                            \"total_votes\",\n",
    "                                                            \"encrypt_item_id\",)\\\n",
    "                                                    .withColumnRenamed(\"lower(title)\", \"title\")\\\n",
    "                                                    .withColumnRenamed(\"lower(sub_title)\", \"sub_title\")\n",
    "# DFmanifestations_definitive_anobii.show(1, False)\n",
    "# DFmanifestations_definitive_anobii: (uguale a DFjoinbookstars ma senza language)\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+\n",
    "# |item_id|title            |sub_title|isbn      |average_rating  |total_review|total_wishlist|no_of_page|publication_date|publisher|binding  |edition|product_type|total_votes|encrypt_item_id   |\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+\n",
    "# |1981748|giulietta squeenz|null     |8845260372|3.18773234200743|129         |110           |209       |2008-05-01      |Bompiani |Paperback|null   |1           |543        |014f24ec9629744c88|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+\n",
    "\n",
    "print(\"Anobii data have been filtered (select books in italian, drop comics and books with a low number of ratings)\")\n",
    "print(\"Integrete with genre and author attributes Anobii data\")\n",
    "#add genres to the item table: this is contained in DFgenres:\n",
    "# +---+--------+-------+----------+------------------+------------------+----------+-----+\n",
    "# |id |familyid|itemid |categoryid|slug              |name              |languageid|votes|\n",
    "# +---+--------+-------+----------+------------------+------------------+----------+-----+\n",
    "# |1  |32423317|2823074|3         |business-economics|Business&Economics|3         |2.0  |\n",
    "# +---+--------+-------+----------+------------------+------------------+----------+-----+\n",
    "# Notice that if a book has more than 1 genre, DFgenres contained more than one row with that item_id, one for genre.\n",
    "# Aggregation of rows with same itemid is needed \n",
    "#DFgenres_aggregated = DFgenres.groupBy('itemid')\\\n",
    "#                              .agg(F.concat_ws(\" / \", F.collect_list('name'))\\\n",
    "#                                    .alias('genres'))\n",
    "# DFgenres_aggregated:\n",
    "# +-------+----------------------------------------------------------------------------------------------+\n",
    "# |itemid |genres                                                                                        |\n",
    "# +-------+----------------------------------------------------------------------------------------------+\n",
    "# |1981748|Humor / Fiction&Literature / Romance / Family-Sex&Relationships / Teens / Philosophy / History|\n",
    "# +-------+----------------------------------------------------------------------------------------------+\n",
    "\n",
    "# join between items and DFgenres_aggregated to have, for each item, its genres\n",
    "DFmanifestations_definitive_anobii_genres = DFmanifestations_definitive_anobii.join(DFgenres,\\\n",
    "                                                                                    DFgenres.itemid == DFmanifestations_definitive_anobii.item_id)\\\n",
    "                                                                              .select(DFmanifestations_definitive_anobii[\"*\"],\\\n",
    "                                                                                      DFgenres.genre_with_votes) \n",
    "\n",
    "# DFmanifestations_definitive_anobii_genres:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+\n",
    "# |item_id|title            |sub_title|isbn      |average_rating  |total_review|total_wishlist|no_of_page|publication_date|publisher|binding  |edition|product_type|total_votes|encrypt_item_id   |genres                                                                                        |\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+\n",
    "# |1981748|giulietta squeenz|null     |8845260372|3.18773234200743|129         |110           |209       |2008-05-01      |Bompiani |Paperback|null   |1           |543        |014f24ec9629744c88|History / Fiction&Literature / Family-Sex&Relationships / Teens / Humor / Romance / Philosophy|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+\n",
    "# add a column which explain the origin of the data (anobii or BCT)\n",
    "DFmanifestations_definitive_anobii_genres = DFmanifestations_definitive_anobii_genres.withColumn('data_type', lit(\"anobii\"))\n",
    "# DFmanifestations_definitive_anobii_genres.show(1, False)\n",
    "# DFmanifestations_definitive_anobii_genres:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+---------+\n",
    "# |item_id|title            |sub_title|isbn      |average_rating  |total_review|total_wishlist|no_of_page|publication_date|publisher|binding  |edition|product_type|total_votes|encrypt_item_id   |genres                                                                                        |data_type|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+---------+\n",
    "# |1981748|giulietta squeenz|null     |8845260372|3.18773234200743|129         |110           |209       |2008-05-01      |Bompiani |Paperback|null   |1           |543        |014f24ec9629744c88|Family-Sex&Relationships / Teens / Humor / Romance / Philosophy / History / Fiction&Literature|anobii   |\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+---------+\n",
    "# add author in the DFmanifestations_definitive_anobii_genres table\n",
    "# find author_id for each book\n",
    "DFmanifestations_anobii_genres_author = DFmanifestations_definitive_anobii_genres.join(DFauthorbooks, \\\n",
    "                                                                                       DFauthorbooks.item_id == DFmanifestations_definitive_anobii_genres.item_id)\\\n",
    "                                                                                 .select(DFmanifestations_definitive_anobii_genres[\"*\"], \\\n",
    "                                                                                         DFauthorbooks[\"author_id\"])\n",
    "# DFmanifestations_anobii_genres_author.show(1, False)\n",
    "# DFmanifestations_anobii_genres_author:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+---------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|author_id|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+---------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Romance / Humor /...|   anobii|   354644|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+---------+\n",
    "# find author (name) for each book (and remove books which has same title but different auhtor?)\n",
    "DFmanifestations_anobii_genres_author = DFmanifestations_anobii_genres_author.join(DFdisplay,\\\n",
    "                                                                                   DFmanifestations_anobii_genres_author.author_id == DFdisplay.author_id)\\\n",
    "                                                                             .select(DFmanifestations_anobii_genres_author[\"*\"], \\\n",
    "                                                                                     DFdisplay[\"author_name\"])\\\n",
    "                                                                             .drop(\"author_id\")\n",
    "\n",
    "# remove books which has same title but different auhtor\n",
    "DFmanifestations_anobii_genres_author = DFmanifestations_anobii_genres_author.select(\"*\", F.min(DFmanifestations_anobii_genres_author.author_name)\\\n",
    "                                                                                           .over(Window.partitionBy(DFmanifestations_anobii_genres_author.item_id))\\\n",
    "                                                                                           .alias(\"author\"))\\\n",
    "                                                                             .drop(\"author_name\")\\\n",
    "                                                                             .dropDuplicates(['item_id', 'author'])\n",
    "# DFmanifestations_anobii_genres_author.show(1, False)\n",
    "# DFmanifestations_anobii_genres_author:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|    author|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Philosophy / Hist...|   anobii|Pulsatilla|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+\n",
    "#join of item and rating tables to obtain person_id, item_id, rating and all attributes of the item in the same row\n",
    "#df1.join(df2, df1.id == df2.id).select(df1[\"*\"],df2[\"other\"])\n",
    "DFloans_definitive_anobii_genres = DFstarsfilteredno0.join(DFmanifestations_anobii_genres_author, \\\n",
    "                                                           DFmanifestations_anobii_genres_author.item_id == DFstarsfilteredno0.item_id)\\\n",
    "                                                     .select(DFmanifestations_anobii_genres_author[\"*\"], \\\n",
    "                                                             DFstarsfilteredno0[\"person_id\"], \\\n",
    "                                                             DFstarsfilteredno0[\"item_review\"])\n",
    "# DFloans_definitive_anobii_genres.show(1, False)\n",
    "# DFloans_definitive_anobii_genres:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|    author|person_id|item_review|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Philosophy / Fict...|   anobii|Pulsatilla|  1244593|          3|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "\n",
    "print(\"Anobii data have been integreted with genre and author attributes\")\n",
    "print(\"Read and filter BCT data\")\n",
    "%run cleanDataBCTwithIBSN.ipynb #Così a quanto pare\n",
    "DFmanifestations_definitive_bct = DFmanifestations_definitive\n",
    "DFloans_definitive_bct = DFloans_definitive\n",
    "# DFmanifestations_definitive_bct.show(1, False)\n",
    "# DFmanifestations_definitive_bct:\n",
    "# +----------------+------------+---------+-------------+---------+--------------------+------------+\n",
    "# |edition_language|edition_date|    title|       author|publisher|manifestation_id_new|ISBNISSN_new|\n",
    "# +----------------+------------+---------+-------------+---------+--------------------+------------+\n",
    "# |             ita|        1996|Pinocchio|Carlo Collodi|   Nuages|              107930|  8807820714|\n",
    "# +----------------+------------+---------+-------------+---------+--------------------+------------+\n",
    "# DFloans_definitive_bct:\n",
    "# +--------------------+--------------------+-------------------+-------------------+-------------------+------------+----------+-----------+\n",
    "# |manifestation_id_new|       patron_id_md5|    loan_date_begin|      loan_date_end|           due_date|from_library|to_library|end_library|\n",
    "# +--------------------+--------------------+-------------------+-------------------+-------------------+------------+----------+-----------+\n",
    "# |              220771|fa242f1458100dccc...|2012-09-05 11:27:25|2012-09-27 18:18:00|2012-10-05 11:27:21|          18|        18|         18|\n",
    "# +--------------------+--------------------+-------------------+-------------------+-------------------+------------+----------+-----------+\n",
    "print(\"BCT data have been read and filtered\")\n",
    "print(\"Prepare data for merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56bb0855-bc1c-4f24-866a-36b6169dace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data have been prepared for merge\n",
      "Merge data\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import MapType\n",
    "\n",
    "# BCT data preparation: prepare the table of loans which contains also metadata of the book\n",
    "DFloans_with_titles = DFloans_definitive_bct.join(DFmanifestations_definitive_bct, \\\n",
    "                                                  DFloans_definitive_bct.manifestation_id_new == DFmanifestations_definitive_bct.manifestation_id_new)\\\n",
    "                                             .select(DFloans_definitive_bct.manifestation_id_new, \\\n",
    "                                                     \"patron_id_md5\", \\\n",
    "                                                     \"author\", \\\n",
    "                                                     \"ISBNISSN_new\", \\\n",
    "                                                     lower(DFmanifestations_definitive_bct.title))\\\n",
    "                                             .withColumnRenamed(\"lower(title)\", \"title\")\n",
    "# DFloans_with_titles:\n",
    "# +--------------------+--------------------+-------------+------------+---------+\n",
    "# |manifestation_id_new|       patron_id_md5|       author|ISBNISSN_new|    title|\n",
    "# +--------------------+--------------------+-------------+------------+---------+\n",
    "# |              107930|b5c0986c79b1afafd...|Carlo Collodi|  8807820714|pinocchio|\n",
    "# +--------------------+--------------------+-------------+------------+---------+\n",
    "\n",
    "# BCT data preparation: split title and subtitle\n",
    "RDDloans_with_titles = DFloans_with_titles.rdd    \n",
    "RDDloans_for_merge = RDDloans_with_titles.map(title_and_subtitle)  \n",
    "# DFloans_with_titles.show(1, False)\n",
    "DFloans_to_merge = RDDloans_for_merge.toDF([\"item_id\", \\\n",
    "                                            \"person_id\", \\\n",
    "                                            \"title\", \\\n",
    "                                            \"author\", \\\n",
    "                                            \"sub_title\", \\\n",
    "                                            \"item_review\", \\\n",
    "                                            \"total_review\", \\\n",
    "                                            \"average_rating\", \\\n",
    "                                            \"total_votes\", \\\n",
    "                                            \"isbn\", \\\n",
    "                                            \"data_type\"])\n",
    "# BCT data preparation: add missing columns, which are present in DFloans_definitive_anobii_genres\n",
    "columns_to_add = ['total_wishlist',\n",
    "                  'no_of_page',\n",
    "                  'publication_date',\n",
    "                  'publisher',\n",
    "                  'binding',\n",
    "                  'edition',\n",
    "                  'product_type',\n",
    "                  'encrypt_item_id',\n",
    "                  'genre_with_votes',\n",
    "                 ]\n",
    "values_to_add = {'total_wishlist': None,\n",
    "                  'no_of_page': None,\n",
    "                  'publication_date': None, \n",
    "                  'publisher': None, \n",
    "                  'binding': \"Paperback\",\n",
    "                  'edition': None,\n",
    "                  'product_type': 1,\n",
    "                  'encrypt_item_id': None,\n",
    "                  'genre_with_votes': None,\n",
    "    \n",
    "}\n",
    "for c in columns_to_add:\n",
    "        DFloans_to_merge = DFloans_to_merge.withColumn(c, lit(values_to_add[c]))\n",
    "\n",
    "DFloans_to_merge = DFloans_to_merge.select(\"item_id\", \"title\", \"sub_title\", \"isbn\", \"average_rating\", \"total_review\", \"total_wishlist\", \"no_of_page\", \"publication_date\", \"publisher\", \"binding\", \"edition\", \"product_type\", \"total_votes\", \"encrypt_item_id\", \"genre_with_votes\", \"data_type\", \"author\", \"person_id\", \"item_review\")\n",
    "\n",
    "#DFloans_to_merge = DFloans_to_merge.withColumn(\"genre_with_votes\",DFloans_to_merge.genre_with_votes.cast(MapType(StringType(), StringType())))\n",
    "#DFloans_definitive_anobii_genres.printSchema()\n",
    "#DFloans_to_merge.printSchema()\n",
    "\n",
    "# DFloans_to_merge:\n",
    "# +-------+--------------------+---------+-----------+-------------+-----------+------------+--------------+-----------+----------+-----------+--------------+----------+----------------+---------+---------+-------+------------+---------------+-----+\n",
    "# |item_id|           person_id|    title|author_name|    sub_title|item_review|total_review|average_rating|total_votes|      isbn|  data_type|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|encrypt_item_id|genre|\n",
    "# +-------+--------------------+---------+-----------+-------------+-----------+------------+--------------+-----------+----------+-----------+--------------+----------+----------------+---------+---------+-------+------------+---------------+-----+\n",
    "# | 107930|fcc17388ef20567fe...|pinocchio|       None|Carlo Collodi|          4|        None|          None|       None|8807820714|biblioteche|          null|      null|            null|     null|Paperback|   null|           1|           null| null|\n",
    "# +-------+--------------------+---------+-----------+-------------+-----------+------------+--------------+-----------+----------+-----------+--------------+----------+----------------+---------+---------+-------+------------+---------------+-----+\n",
    "print(\"Data have been prepared for merge\")\n",
    "print(\"Merge data\")\n",
    "# union of the loans of bct data and anobii data\n",
    "DFloans_merged = DFloans_definitive_anobii_genres.union(DFloans_to_merge)\n",
    "# DFloans_merged.show(1)\n",
    "# DFloans_merged\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|    author|person_id|item_review|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Family-Sex&Relati...|   anobii|Pulsatilla|   734393|          5|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# Remove books, whose title is one word only, i.e. keep books whose title contains at least a blank space\n",
    "# DFloans_merged.filter(DFloans_merged.title == \"pinocchio\").show()\n",
    "DFloans_merged_nosinglewordtitles = DFloans_merged.filter(DFloans_merged.title.contains(\" \")) \n",
    "# DFloans_merged_nosinglewordtitles.filter(DFloans_merged_nosinglewordtitles.title == \"pinocchio\").show()\n",
    "\n",
    "# Assign to books with the same title, the same book_id\n",
    "DFloans_merged_aggregated = DFloans_merged_nosinglewordtitles.select(\"*\", \\\n",
    "                                                                     F.first(DFloans_merged_nosinglewordtitles.item_id)\\\n",
    "                                                                      .over(Window.partitionBy(DFloans_merged_nosinglewordtitles.title).orderBy(DFloans_merged_nosinglewordtitles.data_type))\\\n",
    "                                                                      .alias(\"book_id\"))\\\n",
    "                                                              .drop(\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c2dded4-31c9-4606-a5fd-f6ca5cabf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFloans_merged_aggregated.show(2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fb67af8-d8d3-4a4d-a132-8320bab1d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been merged\n",
      "Write data on csv files\n"
     ]
    }
   ],
   "source": [
    "# DFloans_merged_aggregated.show(1)\n",
    "# DFloans_merged_aggregated:\n",
    "# +--------------------+-------------------+----------+----------------+------------+--------------+----------+----------------+-------------------+---------+-------+------------+-----------+------------------+--------------------+---------+----------------+---------+-----------+-------+\n",
    "# |               title|          sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|          publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|          author|person_id|item_review|book_id|\n",
    "# +--------------------+-------------------+----------+----------------+------------+--------------+----------+----------------+-------------------+---------+-------+------------+-----------+------------------+--------------------+---------+----------------+---------+-----------+-------+\n",
    "# |come mi batte for...|storia di mio padre|8806198882|4.14122681883024|         206|           270|       302|      2009-11-03|Einaudi (Frontiere)|Hardcover|      1|           1|        705|01500978c278d06718|Crime / Teens / N...|   anobii|Benedetta Tobagi|   767565|          3|2806310|\n",
    "# +--------------------+-------------------+----------+----------------+------------+--------------+----------+----------------+-------------------+---------+-------+------------+-----------+------------------+--------------------+---------+----------------+---------+-----------+-------+\n",
    "\n",
    "# Assign to books with the same title, the same new_author\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.min(DFloans_merged_aggregated.author)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_author\"))\n",
    "\n",
    "# Assign to books with the same title, the same new_genre\n",
    "#DFloans_merged_aggregated.show(1, False)\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.first(DFloans_merged_aggregated.genre_with_votes)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id).orderBy(DFloans_merged_aggregated.data_type))\\\n",
    "                                                              .alias(\"new_genre\"))\n",
    "\n",
    "# Assign to books with the same title, the same new_encrypt\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.min(DFloans_merged_aggregated.encrypt_item_id)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_encrypt_item_id\"))\n",
    "\n",
    "# Assign to books with the same title, the same new_isbn\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.min(DFloans_merged_aggregated.isbn)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_isbn\"))\n",
    "\n",
    "# Update metrics of loans: new_total_count (to count the number of votes or loans) \n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.count(\"*\")\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_total_count\"))\n",
    "\n",
    "# Update metrics of loans: new_average_rating \n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.avg(DFloans_merged_aggregated.item_review)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_average_rating\"))\n",
    "\n",
    "# Update metrics of loans: new_total_review, to count the actual number of review for a book (only anobii provides real review!!) \n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.max(DFloans_merged_aggregated.total_review)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_total_review\"))\n",
    "\n",
    "# Remove \"old\" attributes\n",
    "columnsToDrop = ['author',\n",
    "                 'genre_with_votes',\n",
    "                 'encrypt_item_id',\n",
    "                 'isbn',\n",
    "                 'total_count',\n",
    "                 'average_rating',\n",
    "                 'total_review',]\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.drop(*columnsToDrop)\n",
    "# DFloans_merged_aggregated.show(1)\n",
    "# DFloans_merged_aggregated:\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+-------------------+----------+---------------+------------------+----------------+\n",
    "# |            title|sub_title|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|data_type|person_id|item_review|book_id|new_author|           new_genre|new_encrypt_item_id|  new_isbn|new_total_count|new_average_rating|new_total_review|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+-------------------+----------+---------------+------------------+----------------+\n",
    "# |giulietta squeenz|     null|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|   anobii|   959028|          3|1981748|Pulsatilla|Philosophy / Hist...| 014f24ec9629744c88|8845260372|            543|3.1860036832412524|             129|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+-------------------+----------+---------------+------------------+----------------+\n",
    "\n",
    "# Rename \"new\" attributes\n",
    "columnsToRenames = ['new_author',\n",
    "                    'new_genre',\n",
    "                    'new_encrypt_item_id',\n",
    "                    'new_isbn',\n",
    "                    'new_total_count',\n",
    "                    'new_average_rating',\n",
    "                    'new_total_review',]\n",
    "for c in columnsToRenames:\n",
    "    DFloans_merged_aggregated = DFloans_merged_aggregated.withColumnRenamed(c, c.replace(\"new_\", \"\"))\n",
    "# DFloans_merged_aggregated.show(1)\n",
    "# DFloans_merged_aggregated:\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+------------------+----------+-----------+------------------+------------+\n",
    "# |            title|sub_title|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|data_type|person_id|item_review|book_id|    author|               genre|   encrypt_item_id|      isbn|total_count|    average_rating|total_review|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+------------------+----------+-----------+------------------+------------+\n",
    "# |giulietta squeenz|     null|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|   anobii|   273238|          4|1981748|Pulsatilla|Humor / Philosoph...|014f24ec9629744c88|8845260372|        543|3.1860036832412524|         129|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+------------------+----------+-----------+------------------+------------+\n",
    "\n",
    "# Remove ['book_id', 'person_id'] duplicates\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.dropDuplicates(['book_id', 'person_id'])\n",
    "print(\"Data has been merged\")\n",
    "# before write data on file, create index for both users and books of the sparce matrix\n",
    "RDDmerged_almost_definitive = DFloans_merged_aggregated.rdd\n",
    "\n",
    "# functions which creates the dictionary where the key is the book_id/person_id and the value is the corresponding index\n",
    "book_dictionary = create_book_dictionary(RDDmerged_almost_definitive)\n",
    "user_dictionary = create_user_dictionary(RDDmerged_almost_definitive)\n",
    "\n",
    "# add the user_index and the book_index in the dataframe\n",
    "RDDdefinitive = RDDmerged_almost_definitive.map(addRowColumnId)\n",
    "DFloans_merged_aggregated = RDDdefinitive.toDF(['book_id',\n",
    "                                                'title',\n",
    "                                                'sub_title',\n",
    "                                                'total_wishlist',\n",
    "                                                'no_of_page',\n",
    "                                                'publication_date',\n",
    "                                                'publisher',\n",
    "                                                'binding',\n",
    "                                                'edition',\n",
    "                                                'product_type',\n",
    "                                                'total_votes',\n",
    "                                                'data_type',\n",
    "                                                'person_id',\n",
    "                                                'item_review',\n",
    "                                                'author',\n",
    "                                                'genre',\n",
    "                                                'encrypt_item_id',\n",
    "                                                'isbn',\n",
    "                                                'total_count',\n",
    "                                                'average_rating',\n",
    "                                                'total_review', \n",
    "                                                'user_index', \n",
    "                                                'book_index'], sampleRatio=0.9)\n",
    "print(\"Write data on csv files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8628b9d5-2a66-4ee4-a357-f26b4bf343b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFloans_merged_aggregated.show(2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441c8362-0f52-48ec-8d46-5654c9817979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5987892\n",
      "There are 5987892 loans\n"
     ]
    }
   ],
   "source": [
    "ratings = DFloans_merged_aggregated.filter(~(DFloans_merged_aggregated.genre.isNull()))#DFdefinitive\n",
    "print(ratings.count())\n",
    "# in the rating table, each row is given by \"person_id\", \"book_id\", \"rating\" attributes\n",
    "ratings_tocsv = ratings.select([\"person_id\", \"book_id\", \"item_review\"])\\\n",
    "                       .withColumnRenamed(\"item_review\", \"rating\")\n",
    "# ratings_tocsv.show(1)\n",
    "# ratings_tocsv:\n",
    "# +---------+-------+------+\n",
    "# |person_id|book_id|rating|\n",
    "# +---------+-------+------+\n",
    "# |   100489|1981748|     3|\n",
    "# +---------+-------+------+\n",
    "print(\"There are \"+str(ratings_tocsv.count())+\" loans\")\n",
    "ratings_tocsv.toPandas().to_csv(\"ratings2021_extended.csv\")\n",
    "\n",
    "# in the book table, the metadata about the book are reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad46c0d5-089d-4b62-b65e-72f55ab426da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9198 books\n",
      "3582\n",
      "3582\n",
      "3582\n",
      "3582\n",
      "There are 231646 users\n",
      "Csv files have been written\n"
     ]
    }
   ],
   "source": [
    "# +-------+-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+----------+--------------------+------------------+----------+-----------+------------------+------------+----------+\n",
    "# |book_id|            title|sub_title|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|data_type|    author|               genre|   encrypt_item_id|      isbn|total_count|    average_rating|total_review|book_index|\n",
    "# +-------+-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+----------+--------------------+------------------+----------+-----------+------------------+------------+----------+\n",
    "# |1981748|giulietta squeenz|     null|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|   anobii|Pulsatilla|Family-Sex&Relati...|014f24ec9629744c88|8845260372|        543|3.1860036832412524|         129|       319|\n",
    "# +-------+-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+----------+--------------------+------------------+----------+-----------+------------------+------------+----------+\n",
    "\n",
    "\n",
    "books_tocsv = ratings.dropDuplicates(['book_id'])\\\n",
    "                     .drop(\"person_id\")\\\n",
    "                     .drop(\"user_index\")\\\n",
    "                     .drop(\"item_review\")\n",
    "# books_tocsv.show(1)\n",
    "# books_tocsv:\n",
    "print(\"There are \"+str(books_tocsv.count())+\" books\")\n",
    "books_tocsv = books_tocsv.join(DFdescriptions, DFdescriptions.item_id == books_tocsv.book_id).select(\"book_id\", \"title\", \"sub_title\", \"total_wishlist\", \"no_of_page\", \"publication_date\", \"publisher\", \"binding\", \"edition\", \"product_type\", \"total_votes\", \"data_type\", \"author\", \"genre\", \"encrypt_item_id\", \"isbn\", \"total_count\", \"average_rating\", \"total_review\", \"book_index\", \"content\")\n",
    "#books_tocsv.toPandas().to_csv(\"books2021_extended.csv\")\n",
    "#books_tocsv.show(1, False)\n",
    "books_tocsv = books_tocsv.withColumn(\"content2\", books_tocsv[\"content\"])\n",
    "print(books_tocsv.select(\"book_id\").distinct().count())\n",
    "\n",
    "RDDbooks_tocsv = books_tocsv.rdd\n",
    "RDDbooks_tocsv = RDDbooks_tocsv.map(lambda x: (x.book_id, x))\n",
    "RDDreducedbooks = RDDbooks_tocsv.reduceByKey(concatenateContentsDescriptions)\n",
    "RDDreducedbooks = RDDreducedbooks.map(toSingleDescriptions)\n",
    "books_tocsv = RDDreducedbooks.toDF([])\n",
    "print(books_tocsv.count())\n",
    "\n",
    "#Joiniamo DFreviews a books_tocsv (la cardinalità aumenterà ma con una funzione di riduzione con gli RDD dovremmo riportarla alla normalità)\n",
    "books_tocsv = books_tocsv.join(DFreviews, books_tocsv.book_id == DFreviews.item_id, \"left\").select(books_tocsv.book_id, \"title\", \"sub_title\", \"total_wishlist\", \"no_of_page\", \"publication_date\", \"publisher\", \"binding\", \"edition\", \"product_type\", \"total_votes\", \"data_type\", \"author\", \"genre\", \"encrypt_item_id\", \"isbn\", \"total_count\", \"average_rating\", \"total_review\", \"book_index\", \"content\", \"comment_content\", \"content2\")\n",
    "\n",
    "#Fatto ciò,  trasformiamo books_tocsv in un rdd per poter applicare la funzione di concatenazione\n",
    "print(books_tocsv.select(\"book_id\").distinct().count())\n",
    "\n",
    "RDDbooks = books_tocsv.rdd\n",
    "RDDbooks = RDDbooks.map(lambda x: (x.book_id, x))\n",
    "RDDreducedbooks = RDDbooks.reduceByKey(concatenateContentsComments)\n",
    "RDDreducedbooks = RDDreducedbooks.map(toSingleComments)\n",
    "RDDreducedbooks = RDDreducedbooks.map(mergeDescriptionComments)\n",
    "books_tocsv = RDDreducedbooks.toDF([])\n",
    "\n",
    "print(books_tocsv.count())\n",
    "\n",
    "# in the user table, the person_id is reported for each user (in each row)\n",
    "users_tocsv = ratings.select(\"person_id\", \"user_index\").dropDuplicates()\n",
    "# users_tocsv.show(1)\n",
    "# users_tocsv:\n",
    "# +---------+----------+\n",
    "# |person_id|user_index|\n",
    "# +---------+----------+\n",
    "# |  1224803|     10623|\n",
    "# +---------+----------+\n",
    "print(\"There are \"+str(users_tocsv.count())+\" users\")\n",
    "users_tocsv.toPandas().to_csv(\"users2021_extended.csv\")\n",
    "\n",
    "print(\"Csv files have been written\")\n",
    "#books_tocsv.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "331cac95-7e05-465e-86e0-d394704ffd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ESTRAZIONE KEYWORD###\n",
    "\n",
    "#Usiamo una versione filtrata di books_tocsv dove non compaiono tuple senza descrizione per attuare la nostra\n",
    "#estrazione.\n",
    "\n",
    "books_with_content = books_tocsv.filter(books_tocsv.content!=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a5661f3-7626-4625-883b-764ce1f109a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_with_content = books_with_content.filter(~(books_with_content.content.contains(\"Scheda INCOMPLETA\"))) #Filtra i libri con le schede incomplete\n",
    "books_with_content = books_with_content.filter(~(books_with_content.content.contains(\"SCHEDA DOPPIA\")))\n",
    "books_with_content = books_with_content.filter(~(books_with_content.content.contains(\"Questa scheda è DOPPIA.\")))\n",
    "books_with_content = books_with_content.filter(~(((books_with_content.content.contains(\"scheda\")) | (books_with_content.content.contains(\"Scheda\"))) & ((books_with_content.content.contains(\"incompleta\")) | (books_with_content.content.contains(\"doppia\")))))\n",
    "#books_with_content = books_with_content.dropDuplicates(['book_id'])\n",
    "\n",
    "#books_with_content.filter(books_with_content.content.contains(\"Esistt\")).show(1, False)\n",
    "\n",
    "#Per estrarre le keyword in maniera \"TF-IDF\" dobbiamo innanzitutto implementare una funzione che pulisca il testo \n",
    "#dalle stopwords (congiunzioni, rumori, etc.)\n",
    "\n",
    "def clean_text_light(doc_collection):\n",
    "    new_corpus_doc = []\n",
    "    for description in doc_collection:\n",
    "        description = description.replace(\"agrave\", \" \")\n",
    "        description = description.replace(\"egrave\", \" \")\n",
    "        description = description.replace(\"igrave\", \" \")\n",
    "        description = description.replace(\"ograve\", \" \")\n",
    "        description = description.replace(\"ograve\", \" \")\n",
    "        description = description.replace(\"<b>\", \" \")\n",
    "        description = description.replace(\",\", \" \")\n",
    "        description = description.replace(\"'\", \" \")\n",
    "        description = description.replace('\"', \" \")\n",
    "        description = description.replace(\"<b>\", \" \")\n",
    "        description = description.replace(\"<br />\", \" \")\n",
    "        description = description.replace(\"</b>\", \" \")\n",
    "        description = description.replace(\"<br>\", \" \")\n",
    "        description = description.replace(\"</br>\", \" \")\n",
    "        description = description.replace(\"<p>\", \" \")\n",
    "        description = description.replace(\"</p>\", \" \")\n",
    "        description = description.replace(\"<P>\", \" \")\n",
    "        description = description.replace(\"</P>\", \" \")\n",
    "        description = description.replace(\"<i>\", \" \")\n",
    "        description = description.replace(\"</i>\", \" \")\n",
    "        description = description.replace(\"&quot\", \" \")\n",
    "        description = description.replace(\"<strong>\", \" \")\n",
    "        description = description.replace(\"</strong>\", \" \")\n",
    "        description = description.replace(\"&\", \" \")  \n",
    "        #KEYWORD PRESENTI MA NON MOLTO UTILI\n",
    "        pattern = re.compile(re.escape(\"leggere\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettrice\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettore\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettori\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettura\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"libro\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"libri\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mese \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mesi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"tempo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"tempi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"parte \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"parti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" cosa \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" cose \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" personaggio \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" personaggi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volume \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volumi\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" protagonista\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" protagonisti\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" testo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" testi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" pagina \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" pagine \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" secolo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" secoli \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" persona \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" persone \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"scrittore \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scrittrice \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scrittrici \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scrittori \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\" vic\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fine \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"nome \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"nomi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"inizio \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" parola \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" parole \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\" ista\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"modo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" numero \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" punto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" centro \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\"ora \"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" frattempo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mezzo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mezzi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" corso \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" situazione \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" piano \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" via \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" vie \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" forma \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" forme \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" posto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" posti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fronte \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" luogo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"autore \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" autori \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" autrici \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"autrice \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" grazie \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fatto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fatti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"edizione \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" edizioni \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" opere \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" serie \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tema \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" temi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" grado \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" gradi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" genere \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" generi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" titolo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" titoli \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"oggetto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" oggetti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tempo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tempi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" sfondo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" sfondi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" conto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" conti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volta \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volte \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\" race\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scena \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scene \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"età \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" figura \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" figure \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"epoca \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" epoche\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"none\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"libreria\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"librerie\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"biblioteca\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"biblioteche\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"generazione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"generazioni\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mano \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mani \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"aspetto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"titolo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"produzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" senso \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" trama \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" trame \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"racconto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"racconti\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"capitolo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"capitoli\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fondo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"domanda\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"risposta\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"domande\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"risposte\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"versione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"argomento\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"argomenti\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tesi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"italiano\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"effetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" carta \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"bisogno\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"bisogni\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"analisi\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"momento\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"milione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"disegno\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" dono \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"successo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"traduzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" penna \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"regola\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"regole\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"progetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"esempio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"narrazione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" data \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"pratica\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"frase\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"ultimo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"base\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"effetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"attenzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"motivo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"essere\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"linguaggio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" lingua \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lavoro\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"critica\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"aspetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"introduzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"episodio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"livello\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"pubblicazione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" pezzo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"interesse\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"consiglio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"effetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\"ore\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "       \n",
    "        new_corpus_doc.append(description)\n",
    "    return new_corpus_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a696c66-bdc4-4569-9fe5-193ede25cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFcontent_pandas = books_with_content.toPandas()\n",
    "\n",
    "corpus_doc = DFcontent_pandas['content'].to_list()\n",
    "#print(corpus_doc[0])\n",
    "cleaned_corpus_doc = clean_text_light(corpus_doc) #Pulizia rumori\n",
    "#print(cleaned_corpus_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "011ef596-4cab-431b-b89e-87fc392c0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###FILTRIAMO NOMI E AGGETTIVI DALLE DESCRIZIONI CON SPACY###\n",
    "##CELLA DI ESEMPIO PER L'UTILIZZO DI SPACY, UN MODULO PER CLASSIFICARE IL RUOLO DELLE PAROLE NELE FRASI\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\") #Caricamento file per il riconoscimento\n",
    "\n",
    "new_corpus_doc = []\n",
    "for document in cleaned_corpus_doc:\n",
    "    noun_tokens = []\n",
    "    doc = nlp(document)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" and token.is_oov==False: #Solo nomi presenti nel vocabolario italiano\n",
    "            noun_tokens.append(token.lemma_)\n",
    "    new_document = (\" \").join(noun_tokens)\n",
    "    new_corpus_doc.append(new_document)\n",
    "\n",
    "#print(new_corpus_doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab3d39e-cf20-4971-997f-66c0b3422797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programmando VERB advcl False programmare\n",
      "è AUX cop False essere\n",
      "proprio ADV advmod False proprio\n",
      "bello ADJ ROOT False bello\n"
     ]
    }
   ],
   "source": [
    "#CELLA DI PROVA PER SPACY\n",
    "\n",
    "#text = \"programmando è proprio bello\"\n",
    "#doc = nlp(text)\n",
    "#for token in doc:\n",
    "#    print(token.text, token.pos_, token.dep_, token.is_oov, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7351a15-ac55-4c05-8f9a-9d24f681ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "###FASE DI STEMMING### #SOSTITUITA A FASE DI LEMMATIZING, NON USARE\n",
    "\n",
    "#Fase usata per unire le parole chiave molto simili fra loro\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# import stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "#Crea uno stemmer italiano\n",
    "stemmer_snowball = SnowballStemmer('italian')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "stemmed_corpus_doc = []\n",
    "for descrizione in new_corpus_doc:\n",
    "    new_tokens_without_sw = []\n",
    "    text_token = word_tokenize(descrizione)\n",
    "    tokens_without_sw = [word for word in text_token if not word in stop_words]\n",
    "    \n",
    "    for word in tokens_without_sw:\n",
    "        stem_word = stemmer_snowball.stem(word)\n",
    "        new_tokens_without_sw.append(stem_word)\n",
    "    filtered_sentence = (\" \").join(new_tokens_without_sw)\n",
    "    \n",
    "    stemmed_corpus_doc.append(filtered_sentence)\n",
    "\n",
    "#print(stemmed_corpus_doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2dbdcbe-6b7e-41bc-9e09-d4f62f6525bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1448: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kimihiro Watanuki, liceale, possiede la capaci...</td>\n",
       "      <td>[negozio, desidero, strega, spirito, proprieta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dopo anni di duro apprendistato, nel 1976 Carv...</td>\n",
       "      <td>[normalità, marito, sensazione, fotografia, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Per il commissario Vittorio Spotorno, il dupli...</td>\n",
       "      <td>[commissario, gioco, vista, meta, quartiere, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parigi, giugno 1889. La città palpita, travolt...</td>\n",
       "      <td>[proprietario, onda, nipote, tecnica, città, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leona Dorn &amp;egrave; soddisfatta della sua vita...</td>\n",
       "      <td>[suicidio, casa, fiducia, incubo, giornalista,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>È il destino, ancora una volta, a dare le cart...</td>\n",
       "      <td>[notte, luce, moglie, fede, medico, matrimonio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>Lauren torna a Wisteria, Maryland, sette anni ...</td>\n",
       "      <td>[madre, incidente, zia, colpevole, morte, pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>Ciclo Dirk Pitt - Vol. 19Dirk Pitt resta quasi...</td>\n",
       "      <td>[avventura, ricerca, squadra, nave, tocco, ond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>Don Fabrizio, principe di Salina, all'arrivo d...</td>\n",
       "      <td>[felicità, pretesto, sopravvivenza, decina, af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>Passata la mezzanotte, il tempo e la realtà su...</td>\n",
       "      <td>[foto, gt, fantasma, incubo, bambino, fiaba, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3503 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     Kimihiro Watanuki, liceale, possiede la capaci...   \n",
       "1     Dopo anni di duro apprendistato, nel 1976 Carv...   \n",
       "2     Per il commissario Vittorio Spotorno, il dupli...   \n",
       "3     Parigi, giugno 1889. La città palpita, travolt...   \n",
       "4     Leona Dorn &egrave; soddisfatta della sua vita...   \n",
       "...                                                 ...   \n",
       "3527  È il destino, ancora una volta, a dare le cart...   \n",
       "3528  Lauren torna a Wisteria, Maryland, sette anni ...   \n",
       "3529  Ciclo Dirk Pitt - Vol. 19Dirk Pitt resta quasi...   \n",
       "3530  Don Fabrizio, principe di Salina, all'arrivo d...   \n",
       "3531  Passata la mezzanotte, il tempo e la realtà su...   \n",
       "\n",
       "                                           top_keywords  \n",
       "0     [negozio, desidero, strega, spirito, proprieta...  \n",
       "1     [normalità, marito, sensazione, fotografia, mo...  \n",
       "2     [commissario, gioco, vista, meta, quartiere, m...  \n",
       "3     [proprietario, onda, nipote, tecnica, città, m...  \n",
       "4     [suicidio, casa, fiducia, incubo, giornalista,...  \n",
       "...                                                 ...  \n",
       "3527  [notte, luce, moglie, fede, medico, matrimonio...  \n",
       "3528  [madre, incidente, zia, colpevole, morte, pres...  \n",
       "3529  [avventura, ricerca, squadra, nave, tocco, ond...  \n",
       "3530  [felicità, pretesto, sopravvivenza, decina, af...  \n",
       "3531  [foto, gt, fantasma, incubo, bambino, fiaba, t...  \n",
       "\n",
       "[3503 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importiamo il TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(smooth_idf=True, use_idf=True, max_df=0.35, min_df=80) #Di solito 0.60, 40\n",
    "\n",
    "vectorizer.fit_transform(new_corpus_doc) #CREAZIONE DIZIONARIO CON I VOCABOLI CONTENUTI NELLE DESCRIZIONI\n",
    "\n",
    "feature_names = vectorizer.get_feature_names() #Estrae le feature (i vocaboli) importanti\n",
    "\n",
    "def sort_coo(coo_matrix): #Funzione usata per riordinare il dizionario per score\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data) #COLONNE: FEATURES RIGHE: DESCRIZIONE\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=5): #Funzione usata per ottenere le n parole chiave\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "    \n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc): #Funzione usata per ritornare le top k parole chiave di una descrizione\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items, 20) #10 keyword per ora\n",
    "    \n",
    "    return list(keywords.keys())\n",
    "\n",
    "result = []\n",
    "for descrizione1, descrizione2 in zip(corpus_doc, new_corpus_doc):\n",
    "    df = {}\n",
    "    df['content'] = descrizione1\n",
    "    df['top_keywords'] = get_keywords(vectorizer, feature_names, descrizione2)\n",
    "    result.append(df)\n",
    "    \n",
    "DFkeywords = pd.DataFrame(result)\n",
    "DFkeywords = DFkeywords[DFkeywords['top_keywords'].str.len() > 0]\n",
    "DFkeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "59189b67-56f3-4716-a0b9-11e03e2a81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ora che abbiamo il dataframe di pandas con le keywords associate alle descrizioni, non ci resta che joinare\n",
    "#il contenuto di esso con il dataframe dei libri, integrando l'attributo keywords.\n",
    "#Riportiamo il dataframe nella forma PySpark\n",
    "\n",
    "DFkeywords = spark.createDataFrame(DFkeywords)\n",
    "\n",
    "DFkeywords = DFkeywords.dropDuplicates(['content'])\n",
    "DF_almost_final = books_with_content.join(DFkeywords, DFkeywords.content == books_with_content.content).drop(DFkeywords.content).select(\"book_id\", \"top_keywords\")\n",
    "\n",
    "#Adesso joiniamo per book_id al dataframe con più libri (compresi quelli senza descrizione)\n",
    "DF_final = books_with_content.join(DF_almost_final, DF_almost_final.book_id == books_with_content.book_id).drop(DF_almost_final.book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "211325a5-ff84-4f54-9a9c-be26d0487203",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = DF_final.filter(F.size(col(\"top_keywords\")) > 3) #Scartiamo i libri con poche info come parole chiave\n",
    "#print(DF_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8a4535b-e693-425b-b358-53a75c30212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proviamo a inserire nel dataframe il link per l'immagine della copertina seguendo lo schema del link di anobii:\n",
    "#https://media.anobii.com/covers/id/width/height/scale/treatment/format/version\n",
    "\n",
    "#Ovviamente, il link deve essere costruito solo per le tuple che hanno un \"encrypt item id\" altrimenti sarà null\n",
    "#Per ora metto il link con dei parametri \"safe\": immagine 250x250, formato png, no timestamp, scale 1, treatment = paperback\n",
    "\n",
    "#NOTA: In caso poi si possono cambiare i parametri\n",
    "\n",
    "def createImageUrl(line):\n",
    "    if line.encrypt_item_id == None: #Non c'è un item_encrypt_id\n",
    "        return line\n",
    "    else: #C'è l'id\n",
    "        image_url = f\"https://media.anobii.com/covers/{line.encrypt_item_id}/250/250/1/paperback/png/\"     \n",
    "        return Row(book_id=line.book_id, title=line.title, sub_title=line.sub_title, \n",
    "               total_wishlist=line.total_wishlist, no_of_page=line.no_of_page, publication_date=line.publication_date, \n",
    "               publisher=line.publisher, binding=line.binding, edition=line.edition, product_type=line.product_type, \n",
    "               total_votes=line.total_votes, data_type=line.data_type, author=line.author, genre=line.genre, \n",
    "               encrypt_item_id=line.encrypt_item_id, isbn=line.isbn, total_count=line.total_count, average_rating=line.average_rating, \n",
    "               total_review=line.total_review, book_index=line.book_index, content=line.content, description = line.content2, top_keywords=line.top_keywords, image_url=image_url)\n",
    "    \n",
    "#Aggiungiamo la nuova colonna al dataframe books_tocsv.\n",
    "RDDfinal = DF_final.rdd\n",
    "RDDfinal_with_urls = RDDfinal.map(createImageUrl)\n",
    "\n",
    "DF_final = RDDfinal_with_urls.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5df73b5d-9d48-4213-ad94-a1c3f15da07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(DF_final.count()) #3495 LIBRI\n",
    "#print(DF_final.select('book_id').distinct().count())\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "DF_final = DF_final.withColumn('content', regexp_replace('content', '\\,', ''))\n",
    "DF_final = DF_final.withColumn('content2', regexp_replace('content', '\\,', ''))\n",
    "DF_final = DF_final.withColumn('content', regexp_replace('content', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('content2', regexp_replace('content', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('content', regexp_replace('content', '\\\"', ''))\n",
    "DF_final = DF_final.withColumn('content2', regexp_replace('content', '\\\"', ''))\n",
    "DF_final.toPandas().to_csv('for_now_final_DF5.csv')\n",
    "\n",
    "#DF_final.filter(array_contains(col(\"top_keywords\"), \"scheda\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42fc3c8f-c457-4ad4-80f8-a172b5b78869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amore': 498, 'donna': 489, 'casa': 362, 'amico': 342, 'figlio': 342, 'famiglia': 333, 'morte': 313, 'giorno': 311, 'po': 300, 'ragazzo': 296, 'bambino': 287, 'vicenda': 284, 'guerra': 280, 'madre': 280, 'padre': 269, 'caso': 259, 'viaggio': 240, 'realtà': 228, 'paese': 224, 'città': 222, 'avventura': 220, 'moglie': 206, 'ragazza': 203, 'passato': 203, 'notte': 188, 'ricerca': 186, 'potere': 181, 'sogno': 178, 'idea': 178, 'occhio': 175, 'passione': 172, 'cuore': 171, 'marito': 169, 'saga': 162, 'stile': 160, 'strada': 160, 'fratello': 159, 'indagine': 154, 'forza': 153, 'rapporto': 153, 'terra': 152, 'sentimento': 151, 'opera': 149, 'mistero': 149, 'segreto': 148, 'corpo': 145, 'destino': 145, 'mente': 144, 'omicidio': 143, 'sangue': 143, 'pensiero': 140, 'esistenza': 139, 'dolore': 138, 'voce': 138, 'finale': 137, 'ricordo': 136, 'scrittura': 134, 'capo': 134, 'socio': 133, 'signore': 132, 'giovane': 132, 'letteratura': 132, 'paura': 132, 'film': 131, 'problema': 129, 'genitore': 127, 'scuola': 127, 'polizia': 125, 'verità': 124, 'colpo': 124, 'sorella': 124, 'giallo': 123, 'esperienza': 122, 'male': 122, 'incontro': 122, 'figlia': 120, 'mare': 119, 'delitto': 119, 'amicizia': 118, 'matrimonio': 116, 'vittima': 115, 'immagine': 114, 'futuro': 113, 're': 113, 'ora': 111, 'giornalista': 110, 'gioco': 110, 'gruppo': 110, 'commissario': 109, 'assassino': 109, 'lettera': 108, 'luce': 107, 'evento': 107, 'azione': 107, 'arte': 106, 'fumetto': 106, 'magia': 106, 'lotta': 106, 'ragazzino': 105, 'scelta': 105, 'memoria': 104, 'natura': 103, 'infanzia': 102, 'nemico': 102, 'eroe': 102, 'speranza': 102, 'vampiro': 102, 'raccolta': 101, 'saggio': 101, 'cultura': 101, 'amante': 101, 'morto': 100, 'anima': 100, 'violenza': 99, 'libertà': 99, 'nonno': 99, 'descrizione': 98, 'cadavere': 98, 'detective': 96, 'gente': 95, 'killer': 95, 'trilogia': 95, 'campo': 93, 'sesso': 92, 'creatura': 92, 'fidanzato': 92, 'dio': 91, 'thriller': 91, 'riflessione': 91, 'incubo': 91, 'testa': 91, 'professore': 90, 'villaggio': 90, 'ombra': 90, 'cane': 90, 'stato': 89, 'popolo': 89, 'fuga': 89, 'battaglia': 88, 'origine': 88, 'aiuto': 88, 'universo': 88, 'animale': 87, 'poliziotto': 87, 'orrore': 86, 'coppia': 86, 'isola': 86, 'luogo': 86, 'acqua': 86, 'estate': 85, 'spazio': 84, 'diario': 84, 'passo': 84, 'medico': 83, 'quartiere': 83, 'emozione': 83, 'incidente': 83, 'elemento': 83, 'mamma': 83, 'identità': 82, 'scoperta': 82, 'compagno': 82, 'tratto': 82, 'agente': 82, 'investigatore': 82, 'musica': 82, 'vista': 81, 'leggenda': 81, 'legge': 81, 'pericolo': 81, 'studente': 81, 'nipote': 80, 'atmosfera': 80, 'caccia': 80, 'stella': 80, 'periodo': 80, 'strega': 80, 'crimine': 80, 'conoscenza': 79, 'relazione': 79, 'regno': 79, 'ironia': 79, 'fantasy': 79, 'missione': 78, 'tragedia': 78, 'felicità': 78, 'politica': 78, 'giustizia': 78, 'sguardo': 78, 'momento': 78, 'desiderio': 78, 'avvocato': 78, 'maestro': 77, 'porta': 77, 'fantasia': 76, 'ragione': 76, 'crisi': 76, 'poesia': 76, 'ritmo': 76, 'confronto': 75, 'scomparsa': 75, 'colore': 75, 'percorso': 75, 'bene': 75, 'macchina': 74, 'milione': 74, 'abitante': 74, 'ritorno': 74, 'solitudine': 74, 'minaccia': 74, 'studio': 74, 'fantascienza': 74, 'servizio': 73, 'notizia': 73, 'soluzione': 73, 'processo': 73, 'indizio': 73, 'fantasma': 72, 'comunità': 72, 'collega': 72, 'soldo': 72, 'capello': 72, 'dubbio': 72, 'coraggio': 72, 'giro': 72, 'corte': 72, 'copertina': 72, 'civiltà': 72, 'traccia': 71, 'carriera': 71, 'maniera': 71, 'pace': 71, 'favola': 71, 'presenza': 71, 'teatro': 70, 'sensazione': 70, 'compagnia': 70, 'scienza': 70, 'seguito': 70, 'vendetta': 70, 'prova': 70, 'bellezza': 70, 'vento': 70, 'malattia': 69, 'montagna': 69, 'chiesa': 69, 'lato': 69, 'situazione': 69, 'affare': 69, 'fuoco': 69, 'ispettore': 69, 'università': 69, 'giornata': 69, 'ciclo': 69, 'legame': 69, 'poeta': 69, 'occasione': 69, 'causa': 68, 'umanità': 68, 'nave': 68, 'denaro': 68, 'sera': 68, 'chiave': 68, 'mito': 68, 'messaggio': 68, 'ruolo': 68, 'vacanza': 68, 'cittadino': 68, 'fortuna': 68, 'scienziato': 68, 'tipo': 68, 'sistema': 68, 'particolare': 68, 'rivista': 67, 'droga': 67, 'faccia': 67, 'ospedale': 67, 'valore': 67, 'religione': 67, 'ordine': 67, 'narratore': 67, 'resto': 67, 'inchiesta': 67, 'artista': 67, 'campagna': 67, 'tradizione': 67, 'vicino': 66, 'cielo': 66, 'ritratto': 66, 'bosco': 66, 'sfida': 66, 'provincia': 66, 'esercito': 66, 'impresa': 66, 'capacità': 66, 'guerriero': 66, 'gt': 66, 'dialogo': 66, 'noir': 66, 'ossessione': 65, 'attore': 65, 'spirito': 65, 'mostro': 65, 'cinema': 65, 'classe': 65, 'serial': 65, 'teoria': 65, 'colpevole': 65, 'nascita': 64, 'festa': 64, 'intrigo': 64, 'arma': 64, 'testimonianza': 64, 'mattina': 64, 'volto': 64, 'soldato': 64, 'buio': 64, 'appuntamento': 64, 'pianeta': 64, 'tentativo': 63, 'pietra': 63, 'demone': 63, 'bordo': 63, 'tradimento': 63, 'visione': 63, 'stagione': 63, 'adulto': 63, 'possibilità': 63, 'cronaca': 63, 'segno': 63, 'mago': 63, 'scritto': 63, 'scopo': 63, 'appartamento': 62, 'regina': 62, 'carcere': 62, 'fianco': 62, 'capitale': 62, 'commento': 62, 'confine': 62, 'auto': 62, 'termine': 62, 'rivoluzione': 62, 'zio': 62, 'minuto': 61, 'interno': 61, 'membro': 61, 'albero': 61, 'simbolo': 61, 'adolescenza': 61, 'tempo': 61, 'condizione': 61, 'essere': 61, 'premio': 60, 'intreccio': 60, 'rete': 60, 'colpa': 60, 'significato': 60, 'informazione': 60, 'metodo': 60, 'letto': 60, 'attività': 60, 'insegnante': 60, 'proprietario': 60, 'ambientazione': 60, 'filosofia': 59, 'adolescente': 59, 'punto': 59, 'biografia': 59, 'squadra': 59, 'giornale': 59, 'spalla': 59, 'intelligenza': 59, 'angelo': 59, 'concetto': 59, 'impero': 59, 'narrativa': 59, 'passaggio': 59, 'suicidio': 59, 'meraviglia': 59, 'ambiente': 59, 'limite': 59, 'dettaglio': 58, 'muro': 58, 'fede': 58, 'costume': 58, 'neve': 58, 'arrivo': 58, 'scontro': 58, 'zona': 58, 'giardino': 58, 'mestiere': 58, 'cura': 58, 'visita': 58, 'terrore': 58, 'eroina': 58, 'coscienza': 58, 'palazzo': 57, 'cattivo': 57, 'vecchio': 57, 'guida': 57, 'gioia': 57, 'tecnica': 57, 'inglese': 57, 'inferno': 57, 'caos': 57, 'amica': 57, 'canzone': 57, 'verso': 57, 'commedia': 57, 'zia': 57, 'stanza': 57, 'certezza': 57, 'erede': 57, 'numero': 57, 'contatto': 57, 'obiettivo': 57, 'razza': 57, 'fame': 57, 'puntata': 57, 'lezione': 57, 'strumento': 56, 'quadro': 56, 'episodio': 56, 'silenzio': 56, 'dottore': 56, 'treno': 56, 'genio': 56, 'filo': 56, 'circostanza': 56, 'carattere': 56, 'decennio': 56, 'settimana': 56, 'umano': 55, 'oro': 55, 'operazione': 55, 'vizio': 55, 'meta': 55, 'intervista': 55, 'sospetto': 55, 'tensione': 55, 'guardia': 55, 'governo': 55, 'movimento': 55, 'fiaba': 55, 'pioggia': 55, 'compagna': 55, 'errore': 55, 'luna': 55, 'attacco': 55, 'aspetto': 55, 'comportamento': 54, 'controllo': 54, 'umorismo': 54, 'comune': 54, 'talento': 54, 'inverno': 54, 'attesa': 54, 'padrone': 54, 'citazione': 54, 'solito': 54, 'conflitto': 54, 'fotografia': 54, 'appassionato': 54, 'piede': 54, 'ricchezza': 54, 'potenza': 54, 'bar': 54, 'logica': 54, 'aneddoto': 54, 'pezzo': 54, 'documento': 54, 'editore': 54, 'follia': 54, 'italiano': 53, 'abito': 53, 'pena': 53, 'intenzione': 53, 'diritto': 53, 'protagonista': 53, 'fiore': 53, 'secolo': 53, 'castello': 53, 'camera': 53, 'nord': 53, 'sofferenza': 53, 'cammino': 53, 'animo': 53, 'gesto': 53, 'fatica': 53, 'classico': 53, 'giovinezza': 52, 'miracolo': 52, 'rischio': 52, 'fascino': 52, 'cibo': 52, 'invenzione': 52, 'apparenza': 52, 'rito': 52, 'immaginazione': 52, 'battuta': 52, 'inganno': 52, 'eredità': 52, 'prezzo': 52, 'struttura': 52, 'sud': 52, 'inquietudine': 52, 'prodotto': 52, 'negozio': 52, 'radice': 52, 'materiale': 51, 'materia': 51, 'guaio': 51, 'questione': 51, 'sinistra': 51, 'territorio': 51, 'televisione': 51, 'ufficio': 51, 'senso': 51, 'rivelazione': 51, 'patria': 51, 'modello': 51, 'preda': 51, 'pregiudizio': 51, 'costruzione': 51, 'pubblico': 51, 'cena': 51, 'quotidiano': 51, 'codice': 51, 'tono': 51, 'difficoltà': 51, 'modo': 50, 'risata': 50, 'popolazione': 50, 'fase': 50, 'direzione': 50, 'organizzazione': 50, 'formazione': 50, 'piacere': 50, 'fiume': 50, 'mercato': 50, 'originale': 50, 'costo': 50, 'sole': 50, 'nota': 50, 'risultato': 50, 'cervello': 50, 'articolo': 50, 'affetto': 50, 'sorta': 50, 'divertimento': 50, 'specie': 50, 'capolavore': 50, 'politico': 50, 'semplicità': 49, 'odore': 49, 'pomeriggio': 49, 'differenza': 49, 'carta': 49, 'copia': 49, 'fallimento': 49, 'abitudine': 49, 'braccio': 49, 'cambiamento': 49, 'sorpresa': 49, 'fiducia': 49, 'cerca': 49, 'istinto': 49, 'finestra': 49, 'conseguenza': 49, 'attrazione': 49, 'difesa': 49, 'energia': 49, 'illusione': 49, 'linea': 49, 'fama': 49, 'sviluppo': 49, 'generale': 49, 'spunto': 49, 'riferimento': 48, 'compito': 48, 'personalità': 48, 'voglia': 48, 'distruzione': 48, 'immaginario': 48, 'pelle': 48, 'pugno': 48, 'mania': 48, 'perdita': 48, 'impegno': 48, 'pagina': 48, 'fan': 48, 'polvere': 48, 'odio': 48, 'regalo': 48, 'sconfitta': 48, 'onore': 48, 'prospettiva': 48, 'dramma': 48, 'nostalgia': 48, 'tv': 48, 'gusto': 48, 'moda': 47, 'ispirazione': 47, 'massa': 47, 'scambio': 47, 'promessa': 47, 'lavoro': 47, 'effetto': 47, 'ideale': 47, 'paio': 47, 'straordinario': 47, 'atto': 47, 'vite': 47, 'funzione': 47, 'innamorato': 47, 'tristezza': 47, 'vena': 47, 'malinconia': 47, 'corsa': 47, 'ricostruzione': 47, 'catena': 47, 'sopravvivenza': 47, 'trasformazione': 47, 'insieme': 47, 'attimo': 47, 'testimone': 47, 'difetto': 47, 'dovere': 46, 'migliaio': 46, 'caduta': 46, 'piega': 46, 'frutto': 46, 'miseria': 46, 'distanza': 46, 'foto': 46, 'individuo': 46, 'livello': 46, 'cavallo': 46, 'piano': 46, 'maschio': 46, 'tenerezza': 46, 'frase': 46, 'scenario': 46, 'dimensione': 46, 'portata': 46, 'ipotesi': 46, 'desidero': 46, 'impressione': 46, 'giudizio': 46, 'favore': 45, 'all': 45, 'esordio': 45, 'avviso': 45, 'grazia': 45, 'espressione': 45, 'personaggio': 45, 'riga': 45, 'clima': 45, 'parente': 45, 'interesse': 45, 'salto': 45, 'contraddizione': 45, 'specchio': 45, 'buco': 45, 'discussione': 45, 'conclusione': 45, 'ipocrisia': 45, 'interpretazione': 45, 'grande': 45, 'cambio': 45, 'suspense': 44, 'rappresentazione': 44, 'uscita': 44, 'psicologia': 44, 'prosa': 44, 'assenza': 44, 'decisione': 44, 'critico': 44, 'essenza': 44, 'curiosità': 44, 'programma': 44, 'diversità': 44, 'sorriso': 44, 'sonno': 44, 'massimo': 44, 'convinzione': 44, 'scrittore': 44, 'accordo': 44, 'creazione': 44, 'istante': 44, 'ecc': 44, 'scritta': 43, 'fonte': 43, 'nome': 43, 'altezza': 43, 'meccanismo': 43, 'crudeltà': 43, 'disposizione': 43, 'cosa': 43, 'meglio': 43, 'trattato': 43, 'peccato': 43, 'sicurezza': 43, 'disagio': 43, 'dato': 43, 'paesaggio': 43, 'eccezione': 43, 'frammento': 43, 'abbandono': 43, 'volontà': 43, 'volo': 43, 'lacrima': 43, 'fenomeno': 43, 'tentazione': 42, 'premessa': 42, 'avvenimento': 42, 'spiegazione': 42, 'media': 42, 'rabbia': 42, 'sacrificio': 42, 'soglia': 42, 'fisico': 42, 'persona': 42, 'evoluzione': 42, 'complesso': 42, 'considerazione': 42, 'innocenza': 42, 'ruota': 42, 'dose': 42, 'richiamo': 42, 'punta': 42, 'posizione': 42, 'fretta': 42, 'rispetto': 41, 'angolo': 41, 'misura': 41, 'realismo': 41, 'autore': 41, 'scaffale': 41, 'canto': 41, 'malgrado': 41, 'maturità': 41, 'parola': 41, 'entusiasmo': 41, 'importanza': 41, 'ansia': 41, 'pretesto': 41, 'principio': 41, 'lettura': 41, 'simpatia': 40, 'stellina': 40, 'centinaio': 40, 'epilogo': 40, 'svolta': 40, 'arco': 40, 'dignità': 40, 'salvezza': 40, 'soddisfazione': 40, 'delusione': 40, 'carica': 40, 'originalità': 40, 'comprensione': 40, 'parte': 40, 'motivo': 40, 'opinione': 40, 'angoscia': 40, 'aria': 40, 'profondità': 40, 'precedente': 39, 'caratteristica': 39, 'crescendo': 39, 'sforzo': 39, 'uso': 39, 'gamba': 39, 'volta': 39, 'categoria': 39, 'morale': 39, 'sorte': 39, 'fila': 39, 'partenza': 39, 'affresco': 39, 'responsabilità': 39, 'sensibilità': 39, 'onda': 39, 'normalità': 38, 'moto': 38, 'tutto': 38, 'preferito': 38, 'riguardo': 38, 'quotidianità': 38, 'fine': 38, 'stomaco': 38, 'qualità': 38, 'coinvolgimento': 38, 'voto': 38, 'presa': 38, 'genere': 38, 'amato': 38, 'carne': 38, 'atteggiamento': 38, 'tocco': 38, 'respiro': 38, 'discorso': 37, 'merito': 37, 'peso': 37, 'metafora': 37, 'perfezione': 37, 'impatto': 37, 'turno': 37, 'aspettativa': 37, 'pretesa': 37, 'banalità': 37, 'proposito': 37, 'giusto': 37, 'schema': 37, 'vuoto': 37, 'danno': 37, 'parere': 36, 'contenuto': 36, 'contorno': 36, 'trama': 36, 'risvolto': 36, 'approccio': 36, 'riscatto': 36, 'quantità': 36, 'caratterizzazione': 36, 'motivazione': 36, 'complessità': 36, 'sacco': 36, 'reazione': 36, 'decina': 36, 'noia': 36, 'disperazione': 36, 'intensità': 36, 'tanto': 36, 'piatto': 36, 'vero': 35, 'scena': 35, 'suono': 35, 'fiato': 35, 'approfondimento': 35, 'pregio': 35, 'maestria': 35, 'crescita': 35, 'contrasto': 34, 'bocca': 34, 'tematica': 34, 'contesto': 34, 'componente': 34, 'consapevolezza': 34, 'inizio': 34, 'certo': 33, 'fondo': 33, 'contrario': 33, 'debolezza': 33, 'abilità': 32, 'spessore': 32, 'mano': 32, 'bello': 32, 'recensione': 32, 'dote': 32, 'intento': 32, 'novità': 32, 'sapore': 31, 'naso': 31, 'sostanza': 31, 'buono': 31, 'amaro': 30, 'minimo': 29, 'mancanza': 29, 'sfumatura': 29, 'necessità': 27, 'pecca': 27, 'credo': 25}\n"
     ]
    }
   ],
   "source": [
    "###KEYWORD COVERAGE###\n",
    "\n",
    "##In questa sezione proverò a creare un set di N keywords (quante? Da decidere, per ora facciamo 100, anche se non\n",
    "##è detto che il problema abbia soluzione con 100) che coprano tutto il dataset dei 3500 libri.\n",
    "\n",
    "#Non avendo una chiara idea su come procedere, tecnicamente parlando, proverò a risolvere il problema in maniera\n",
    "#\"creativa\". Creiamo, innanzitutto, un dizionario contenente le parole chiave estratte dal dataframe DF_final\n",
    "#e assegniamo un \"peso\" ad ogni parola chiave. Il peso, nel nostro caso, non sarà altro che il numero di occorrenze\n",
    "#della parola chiave nella lista: infatti, se una parola compare, ad esempio, 5 volte, vuol dire che la parola chiave\n",
    "#\"copre\" 5 libri e così via. Questo vuol dire che, sapendo che il nostro dataframe ha 3554 libri, accumulare\n",
    "#3554 unità di peso dei vocaboli dovrebbe garantire una coverage completa del dataset.\n",
    "\n",
    "#Iniziamo con il contare i termini con il countvectorizer\n",
    "\n",
    "\n",
    "keywords_list = DF_final.toPandas()['top_keywords'].to_list()\n",
    "\n",
    "flat_list = [keyword for keywords_sublist in keywords_list for keyword in keywords_sublist]\n",
    "#print(len([item for item in flat_list if item==\"storia\"]))\n",
    "#print(flat_list)\n",
    "\n",
    "#Adesso che abbiamo la lista dei vari vocaboli, creiamo un dizionario che contenga elementi nella forma\n",
    "#vocabolo: occorrenza.\n",
    "\n",
    "occurrence_dict = {}\n",
    "\n",
    "def countOccurrencies(dictionary, reference_list):\n",
    "    for keyword in reference_list:\n",
    "        if keyword in dictionary:\n",
    "            dictionary[keyword] += 1\n",
    "        elif keyword not in dictionary:\n",
    "            dictionary[keyword] = 1\n",
    "    return dictionary\n",
    "\n",
    "occurrence_dict = countOccurrencies(occurrence_dict, flat_list)\n",
    "sorted_dict = {k: v for k, v in sorted(occurrence_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94be8f91-3984-4b9d-9db4-c3446ae7b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "#DF_final.show(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11b9b8af-ae32-47b0-9ee4-0c9898a8201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è amore. Ha una coverage di 498. La lunghezza rimanente del dataset è 3427. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è donna. Ha una coverage di 356. La lunghezza rimanente del dataset è 2929. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è amico. Ha una coverage di 251. La lunghezza rimanente del dataset è 2573. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è casa. Ha una coverage di 209. La lunghezza rimanente del dataset è 2322. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è guerra. Ha una coverage di 195. La lunghezza rimanente del dataset è 2322. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è caso. Ha una coverage di 180. La lunghezza rimanente del dataset è 2127. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è bambino. Ha una coverage di 162. La lunghezza rimanente del dataset è 1947. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è morte. Ha una coverage di 147. La lunghezza rimanente del dataset è 1785. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è giorno. Ha una coverage di 132. La lunghezza rimanente del dataset è 1638. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ragazzo. Ha una coverage di 127. La lunghezza rimanente del dataset è 1638. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è viaggio. Ha una coverage di 110. La lunghezza rimanente del dataset è 1511. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è vicenda. Ha una coverage di 103. La lunghezza rimanente del dataset è 1401. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è po. Ha una coverage di 96. La lunghezza rimanente del dataset è 1401. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è famiglia. Ha una coverage di 92. La lunghezza rimanente del dataset è 1401. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è realtà. Ha una coverage di 82. La lunghezza rimanente del dataset è 1309. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è avventura. Ha una coverage di 79. La lunghezza rimanente del dataset è 1227. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è paese. Ha una coverage di 70. La lunghezza rimanente del dataset è 1148. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è idea. Ha una coverage di 66. La lunghezza rimanente del dataset è 1078. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è città. Ha una coverage di 60. La lunghezza rimanente del dataset è 1012. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è padre. Ha una coverage di 53. La lunghezza rimanente del dataset è 952. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è indagine. Ha una coverage di 49. La lunghezza rimanente del dataset è 899. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è problema. Ha una coverage di 47. La lunghezza rimanente del dataset è 850. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ragazza. Ha una coverage di 42. La lunghezza rimanente del dataset è 803. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è capo. Ha una coverage di 37. La lunghezza rimanente del dataset è 761. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sogno. Ha una coverage di 35. La lunghezza rimanente del dataset è 724. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è esperienza. Ha una coverage di 34. La lunghezza rimanente del dataset è 689. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è re. Ha una coverage di 34. La lunghezza rimanente del dataset è 689. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è arte. Ha una coverage di 31. La lunghezza rimanente del dataset è 655. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è saggio. Ha una coverage di 31. La lunghezza rimanente del dataset è 624. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ricerca. Ha una coverage di 29. La lunghezza rimanente del dataset è 624. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è signore. Ha una coverage di 27. La lunghezza rimanente del dataset è 595. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è giornalista. Ha una coverage di 27. La lunghezza rimanente del dataset è 568. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sangue. Ha una coverage di 25. La lunghezza rimanente del dataset è 541. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è paura. Ha una coverage di 23. La lunghezza rimanente del dataset è 516. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è cultura. Ha una coverage di 23. La lunghezza rimanente del dataset è 493. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è occhio. Ha una coverage di 22. La lunghezza rimanente del dataset è 470. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è gioco. Ha una coverage di 21. La lunghezza rimanente del dataset è 448. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è raccolta. Ha una coverage di 21. La lunghezza rimanente del dataset è 427. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è passato. Ha una coverage di 19. La lunghezza rimanente del dataset è 406. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è tragedia. Ha una coverage di 18. La lunghezza rimanente del dataset è 387. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è giallo. Ha una coverage di 17. La lunghezza rimanente del dataset è 369. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è nemico. Ha una coverage di 17. La lunghezza rimanente del dataset è 369. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è esistenza. Ha una coverage di 16. La lunghezza rimanente del dataset è 352. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è delitto. Ha una coverage di 16. La lunghezza rimanente del dataset è 336. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sistema. Ha una coverage di 15. La lunghezza rimanente del dataset è 320. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è battaglia. Ha una coverage di 14. La lunghezza rimanente del dataset è 305. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è significato. Ha una coverage di 14. La lunghezza rimanente del dataset è 291. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è lotta. Ha una coverage di 13. La lunghezza rimanente del dataset è 291. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è genio. Ha una coverage di 13. La lunghezza rimanente del dataset è 278. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è memoria. Ha una coverage di 12. La lunghezza rimanente del dataset è 265. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è essere. Ha una coverage di 12. La lunghezza rimanente del dataset è 253. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scrittura. Ha una coverage di 11. La lunghezza rimanente del dataset è 253. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è film. Ha una coverage di 11. La lunghezza rimanente del dataset è 253. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è maestro. Ha una coverage di 11. La lunghezza rimanente del dataset è 242. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è macchina. Ha una coverage di 11. La lunghezza rimanente del dataset è 242. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scienza. Ha una coverage di 11. La lunghezza rimanente del dataset è 231. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è notte. Ha una coverage di 10. La lunghezza rimanente del dataset è 220. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ordine. Ha una coverage di 10. La lunghezza rimanente del dataset è 210. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è concetto. Ha una coverage di 10. La lunghezza rimanente del dataset è 200. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è numero. Ha una coverage di 10. La lunghezza rimanente del dataset è 200. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è assassino. Ha una coverage di 9. La lunghezza rimanente del dataset è 200. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è riflessione. Ha una coverage di 9. La lunghezza rimanente del dataset è 191. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è studio. Ha una coverage di 9. La lunghezza rimanente del dataset è 182. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è saga. Ha una coverage di 8. La lunghezza rimanente del dataset è 173. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scuola. Ha una coverage di 8. La lunghezza rimanente del dataset è 165. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è animale. Ha una coverage di 8. La lunghezza rimanente del dataset è 157. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è diritto. Ha una coverage di 8. La lunghezza rimanente del dataset è 149. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è acqua. Ha una coverage di 7. La lunghezza rimanente del dataset è 141. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è quartiere. Ha una coverage di 7. La lunghezza rimanente del dataset è 134. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scontro. Ha una coverage di 7. La lunghezza rimanente del dataset è 127. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è stile. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è evento. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fumetto. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è morto. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è poesia. Ha una coverage di 6. La lunghezza rimanente del dataset è 114. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è minuto. Ha una coverage di 6. La lunghezza rimanente del dataset è 108. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è classico. Ha una coverage di 6. La lunghezza rimanente del dataset è 108. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è male. Ha una coverage di 5. La lunghezza rimanente del dataset è 102. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ombra. Ha una coverage di 5. La lunghezza rimanente del dataset è 97. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è orrore. Ha una coverage di 5. La lunghezza rimanente del dataset è 92. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è tratto. Ha una coverage di 5. La lunghezza rimanente del dataset è 87. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è occasione. Ha una coverage di 5. La lunghezza rimanente del dataset è 87. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è artista. Ha una coverage di 5. La lunghezza rimanente del dataset è 87. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è cronaca. Ha una coverage di 5. La lunghezza rimanente del dataset è 82. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è tecnica. Ha una coverage di 5. La lunghezza rimanente del dataset è 77. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è inglese. Ha una coverage di 5. La lunghezza rimanente del dataset è 77. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è compito. Ha una coverage di 5. La lunghezza rimanente del dataset è 72. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sentimento. Ha una coverage di 4. La lunghezza rimanente del dataset è 67. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è mistero. Ha una coverage di 4. La lunghezza rimanente del dataset è 63. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è destino. Ha una coverage di 4. La lunghezza rimanente del dataset è 59. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è avvocato. Ha una coverage di 4. La lunghezza rimanente del dataset è 55. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è bar. Ha una coverage di 4. La lunghezza rimanente del dataset è 51. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è semplicità. Ha una coverage di 4. La lunghezza rimanente del dataset è 47. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è pregio. Ha una coverage di 4. La lunghezza rimanente del dataset è 43. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è figlia. Ha una coverage di 3. La lunghezza rimanente del dataset è 43. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ora. Ha una coverage di 3. La lunghezza rimanente del dataset è 40. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è stato. Ha una coverage di 3. La lunghezza rimanente del dataset è 40. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è servizio. Ha una coverage di 3. La lunghezza rimanente del dataset è 37. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è vento. Ha una coverage di 3. La lunghezza rimanente del dataset è 37. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è commedia. Ha una coverage di 3. La lunghezza rimanente del dataset è 34. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è moglie. Ha una coverage di 2. La lunghezza rimanente del dataset è 31. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è omicidio. Ha una coverage di 2. La lunghezza rimanente del dataset è 29. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è mare. Ha una coverage di 2. La lunghezza rimanente del dataset è 27. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è luogo. Ha una coverage di 2. La lunghezza rimanente del dataset è 25. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è passo. Ha una coverage di 2. La lunghezza rimanente del dataset è 25. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è studente. Ha una coverage di 2. La lunghezza rimanente del dataset è 25. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è notizia. Ha una coverage di 2. La lunghezza rimanente del dataset è 23. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è chiesa. Ha una coverage di 2. La lunghezza rimanente del dataset è 21. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ciclo. Ha una coverage di 2. La lunghezza rimanente del dataset è 19. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è narratore. Ha una coverage di 2. La lunghezza rimanente del dataset è 17. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è pezzo. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è specie. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fan. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è interpretazione. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è passione. Ha una coverage di 1. La lunghezza rimanente del dataset è 15. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è rapporto. Ha una coverage di 1. La lunghezza rimanente del dataset è 14. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è letteratura. Ha una coverage di 1. La lunghezza rimanente del dataset è 13. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è polizia. Ha una coverage di 1. La lunghezza rimanente del dataset è 13. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è amicizia. Ha una coverage di 1. La lunghezza rimanente del dataset è 12. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è immagine. Ha una coverage di 1. La lunghezza rimanente del dataset è 11. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è lettera. Ha una coverage di 1. La lunghezza rimanente del dataset è 11. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è campo. Ha una coverage di 1. La lunghezza rimanente del dataset è 10. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è popolo. Ha una coverage di 1. La lunghezza rimanente del dataset è 10. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è compagno. Ha una coverage di 1. La lunghezza rimanente del dataset è 9. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è relazione. Ha una coverage di 1. La lunghezza rimanente del dataset è 8. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è momento. Ha una coverage di 1. La lunghezza rimanente del dataset è 7. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è crisi. Ha una coverage di 1. La lunghezza rimanente del dataset è 7. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fantascienza. Ha una coverage di 1. La lunghezza rimanente del dataset è 6. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è maniera. Ha una coverage di 1. La lunghezza rimanente del dataset è 5. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è messaggio. Ha una coverage di 1. La lunghezza rimanente del dataset è 5. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è vicino. Ha una coverage di 1. La lunghezza rimanente del dataset è 5. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è cielo. Ha una coverage di 1. La lunghezza rimanente del dataset è 4. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è rete. Ha una coverage di 1. La lunghezza rimanente del dataset è 3. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è angelo. Ha una coverage di 1. La lunghezza rimanente del dataset è 3. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è gioia. Ha una coverage di 1. La lunghezza rimanente del dataset è 2. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fotografia. Ha una coverage di 1. La lunghezza rimanente del dataset è 1. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è entusiasmo. Ha una coverage di 1. La lunghezza rimanente del dataset è 1. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n",
      "['amore', 'donna', 'amico', 'guerra', 'caso', 'bambino', 'morte', 'ragazzo', 'viaggio', 'famiglia', 'realtà', 'avventura', 'paese', 'idea', 'città', 'padre', 'indagine', 'problema', 'ragazza', 'capo', 'sogno', 're', 'arte', 'ricerca', 'signore', 'giornalista', 'sangue', 'paura', 'cultura', 'occhio', 'gioco', 'raccolta', 'passato', 'tragedia', 'nemico', 'esistenza', 'delitto', 'sistema', 'battaglia', 'lotta', 'genio', 'memoria', 'film', 'macchina', 'scienza', 'notte', 'ordine', 'assassino', 'riflessione', 'studio', 'saga', 'scuola', 'animale', 'diritto', 'acqua', 'quartiere', 'scontro', 'morto', 'poesia', 'classico', 'male', 'ombra', 'orrore', 'artista', 'cronaca', 'inglese', 'compito', 'sentimento', 'mistero', 'destino', 'avvocato', 'bar', 'semplicità', 'figlia', 'stato', 'vento', 'commedia', 'moglie', 'omicidio', 'mare', 'studente', 'notizia', 'chiesa', 'ciclo', 'narratore', 'passione', 'rapporto', 'polizia', 'amicizia', 'lettera', 'popolo', 'compagno', 'relazione', 'crisi', 'fantascienza', 'vicino', 'cielo', 'angelo', 'gioia', 'entusiasmo']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Il dizionario ha adesso una forma più decente.\n",
    "\n",
    "#Implementiamo una funzione che estragga un set di keywords che coprano l'intero dataset di libri.\n",
    "\n",
    "#La funzione deve darci un set di X keywords (per ora facciamo 100) che:\n",
    "#-Copra tutto il dataset\n",
    "#-Abbia keywords il meno ridondanti possibile\n",
    "\n",
    "#Abbiamo a disposizione il dataframe (DF_final) e il dizionario (sorted_dict).\n",
    "\n",
    "\n",
    "#import random\n",
    "#DF_final.filter(array_contains(col('top_keywords'), 'entusiasmo')).show(10, False) #Possiamo usare array_contains per filtrare\n",
    "#per keyword\n",
    "def keywordCoverage(dictionary, dataframe):\n",
    "    \n",
    "    output_list = []\n",
    "    \n",
    "    temp_dictionary = dictionary.copy()\n",
    "    temp_subset = dataframe.toPandas()['top_keywords'].to_list()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        max_coverage_count = 0\n",
    "        chosen_keyword = \"\"\n",
    "        \n",
    "        for keyword in temp_dictionary:\n",
    "        \n",
    "            coverage_count = len([item for item in temp_subset if keyword in item])\n",
    "            if coverage_count > max_coverage_count:\n",
    "                max_coverage_count = coverage_count\n",
    "                chosen_keyword = keyword\n",
    "            \n",
    "        try:\n",
    "            choice = input(f\"La parola scelta è {chosen_keyword}. Ha una coverage di {max_coverage_count}. La lunghezza rimanente del dataset è {len(temp_subset)}. (y/n)\")\n",
    "\n",
    "            if choice == \"y\":\n",
    "                temp_dictionary.pop(chosen_keyword)\n",
    "                output_list.append(chosen_keyword)\n",
    "                temp_subset = [item for item in temp_subset if chosen_keyword not in item]\n",
    "                print(\"KEYWORD SCELTA\")\n",
    "                if len(temp_subset) == 0:\n",
    "                    break\n",
    "                    \n",
    "            elif choice == \"n\":\n",
    "                temp_dictionary.pop(chosen_keyword)\n",
    "                print(\"KEYWORD SCARTATA\")\n",
    "                if len(temp_subset) == 0:\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Scegliere una scelta possibile (y/n).\")\n",
    "\n",
    "    return output_list\n",
    "\n",
    "chosen_N = keywordCoverage(sorted_dict, DF_final)\n",
    "print(chosen_N)\n",
    "print(len(chosen_N))\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed5aeab-30ee-4f51-b6b4-5e4ca024faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dizionario keyword scelte : percentuale di generi\n",
    "from pyspark.sql.functions import col, array_contains, size\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#Oltre alla nostra lista delle keyword, dovremo usare il solito DF_final.\n",
    "#print(chosen_N)\n",
    "DF_final = spark.read.csv('anobii_genres/for_now_final_DF4.csv', header=True)\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', '\\[', ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', '\\]', ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\, \", ','))\n",
    "#DF_final_new = DF_final_new.withColumn('genre_vector', array(DF_final_new.genre_vector))\n",
    "\n",
    "DF_final = DF_final.withColumn('top_keywords', split(col('top_keywords'), \",\"))\n",
    "\n",
    "#DF_final.show(1, False)\n",
    "#LISTA GIà CREATA\n",
    "chosen_N = ['amore', 'donna', 'amico', 'guerra', 'caso', 'bambino', 'morte', 'ragazzo', 'viaggio', 'famiglia', 'realtà', 'avventura', 'paese', 'idea', 'città', 'padre', 'indagine', 'problema', 'ragazza', 'capo', 'sogno', 're', 'arte', 'ricerca', 'signore', 'giornalista', 'sangue', 'paura', 'cultura', 'occhio', 'gioco', 'raccolta', 'passato', 'tragedia', 'nemico', 'esistenza', 'delitto', 'sistema', 'battaglia', 'lotta', 'genio', 'memoria', 'film', 'macchina', 'scienza', 'notte', 'ordine', 'assassino', 'riflessione', 'studio', 'saga', 'scuola', 'animale', 'diritto', 'acqua', 'quartiere', 'scontro', 'morto', 'poesia', 'classico', 'male', 'ombra', 'orrore', 'artista', 'cronaca', 'inglese', 'compito', 'sentimento', 'mistero', 'destino', 'avvocato', 'bar', 'semplicità', 'figlia', 'stato', 'vento', 'commedia', 'moglie', 'omicidio', 'mare', 'studente', 'notizia', 'chiesa', 'ciclo', 'narratore', 'passione', 'rapporto', 'polizia', 'amicizia', 'lettera', 'popolo', 'compagno', 'relazione', 'crisi', 'fantascienza', 'vicino', 'cielo', 'angelo', 'gioia', 'entusiasmo']\n",
    "#chosen_N = ['amore', 'donna', 'amico', 'morte', 'bambino', 'guerra', 'caso', 'realtà', 'ragazzo', 'figlio', 'paese', 'città', 'idea', 'avventura', 'famiglia', 'viaggio', 'mistero', 'problema', 'ricerca', 'potere', 'raccolta', 'paura', 'polizia', 'padre', 'delitto', 'arte', 'saga', 'giornalista', 'passato', 'film', 'battaglia', 'gioco', 'universo', 'notte', 'capo', 'ragazza', 'pensiero', 'movimento', 'cuore', 'male', 'tragedia', 'speranza', 'morto', 'droga', 'processo', 'animale', 'poesia', 'genio', 'signore', 'mare', 'killer', 'coppia', 'riflessione', 'scoperta', 'aiuto', 'faccia', 'psicologia', 'esistenza', 'cultura', 'dio', 'studio', 'cronaca', 'voce', 'artista', 'trattato', 'assassino', 'natura', 'cane', 'caccia', 'civiltà', 'fantascienza', 'quadro', 'moglie', 'passione', 'strada', 'figlia', 'provincia', 'sangue', 'orrore', 'compagno', 'stato', 'sfida', 'ordine', 'angelo', 'voglia', 'oro', 'rischio', 'rivelazione', 'rapporto', 'sentimento', 'lettera', 'vittima', 'amante', 'relazione', 'testa', 'popolo', 'comunità', 'ciclo', 'ispirazione', 'entusiasmo']\n",
    "def genresPercentage(dataframe, _list):\n",
    "    \n",
    "    DF_final_pandas = dataframe.toPandas() #Local dataframe\n",
    "    percentage_dict = {}\n",
    "    \n",
    "    for keyword in _list:\n",
    "        masking = DF_final_pandas.top_keywords.apply(lambda x: keyword in x)\n",
    "        DF_final_keyword = DF_final_pandas[masking] #Dataframe contenente solo le tuple che contengono la keyword\n",
    "\n",
    "        genre_list = DF_final_keyword['genre'].to_list() #Estraiamo il dizionario con i generi per ogni tupla con la keyword\n",
    "\n",
    "        percentage_dict[keyword] = {'total': len(genre_list)}\n",
    "\n",
    "        for dictionary in genre_list:\n",
    "            dictionary = dictionary.replace(\"'\", '\"')\n",
    "            dictionary = json.loads(dictionary)\n",
    "            \n",
    "            for key in dictionary:\n",
    "\n",
    "                if key in percentage_dict[keyword]:\n",
    "                    percentage_dict[keyword][key] += float(dictionary[key])/sum(list(map(float, dictionary.values()))) #WEIGHTED\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    percentage_dict[keyword][key] = float(dictionary[key])/sum(list(map(float, dictionary.values())))\n",
    "\n",
    "        for key in percentage_dict[keyword]:\n",
    "\n",
    "            if key != 'total':\n",
    "                percentage_dict[keyword][key] = (percentage_dict[keyword][key] * 100) / percentage_dict[keyword]['total']\n",
    "    return percentage_dict\n",
    "        \n",
    "percentage_dict = genresPercentage(DF_final, chosen_N)\n",
    "#print(percentage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3637635-2795-4081-ae58-98c4580266d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amore</th>\n",
       "      <th>donna</th>\n",
       "      <th>amico</th>\n",
       "      <th>guerra</th>\n",
       "      <th>caso</th>\n",
       "      <th>bambino</th>\n",
       "      <th>morte</th>\n",
       "      <th>ragazzo</th>\n",
       "      <th>viaggio</th>\n",
       "      <th>famiglia</th>\n",
       "      <th>...</th>\n",
       "      <th>popolo</th>\n",
       "      <th>compagno</th>\n",
       "      <th>relazione</th>\n",
       "      <th>crisi</th>\n",
       "      <th>fantascienza</th>\n",
       "      <th>vicino</th>\n",
       "      <th>cielo</th>\n",
       "      <th>angelo</th>\n",
       "      <th>gioia</th>\n",
       "      <th>entusiasmo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>498.000000</td>\n",
       "      <td>489.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family-Sex&amp;Relationships</th>\n",
       "      <td>7.758273</td>\n",
       "      <td>7.133577</td>\n",
       "      <td>5.301741</td>\n",
       "      <td>2.326511</td>\n",
       "      <td>2.359581</td>\n",
       "      <td>3.998856</td>\n",
       "      <td>3.538663</td>\n",
       "      <td>4.220919</td>\n",
       "      <td>2.891508</td>\n",
       "      <td>6.951478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517999</td>\n",
       "      <td>4.824721</td>\n",
       "      <td>7.936964</td>\n",
       "      <td>5.496743</td>\n",
       "      <td>0.979787</td>\n",
       "      <td>6.267687</td>\n",
       "      <td>3.729795</td>\n",
       "      <td>2.423222</td>\n",
       "      <td>5.420215</td>\n",
       "      <td>4.339015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction&amp;Literature</th>\n",
       "      <td>46.598101</td>\n",
       "      <td>47.166554</td>\n",
       "      <td>37.712174</td>\n",
       "      <td>42.023418</td>\n",
       "      <td>32.562166</td>\n",
       "      <td>41.243915</td>\n",
       "      <td>40.017384</td>\n",
       "      <td>38.401293</td>\n",
       "      <td>41.063557</td>\n",
       "      <td>49.739449</td>\n",
       "      <td>...</td>\n",
       "      <td>38.565369</td>\n",
       "      <td>39.538734</td>\n",
       "      <td>43.094314</td>\n",
       "      <td>40.440970</td>\n",
       "      <td>30.012201</td>\n",
       "      <td>51.801614</td>\n",
       "      <td>42.426208</td>\n",
       "      <td>36.569484</td>\n",
       "      <td>41.567252</td>\n",
       "      <td>38.862795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humor</th>\n",
       "      <td>1.683975</td>\n",
       "      <td>2.261595</td>\n",
       "      <td>3.121255</td>\n",
       "      <td>1.015098</td>\n",
       "      <td>2.531946</td>\n",
       "      <td>3.252735</td>\n",
       "      <td>1.004468</td>\n",
       "      <td>1.507328</td>\n",
       "      <td>1.953963</td>\n",
       "      <td>1.633051</td>\n",
       "      <td>...</td>\n",
       "      <td>2.382013</td>\n",
       "      <td>2.026766</td>\n",
       "      <td>3.448042</td>\n",
       "      <td>5.670057</td>\n",
       "      <td>1.173463</td>\n",
       "      <td>2.712446</td>\n",
       "      <td>2.140362</td>\n",
       "      <td>2.473487</td>\n",
       "      <td>2.112668</td>\n",
       "      <td>2.428131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>4.160573</td>\n",
       "      <td>6.304667</td>\n",
       "      <td>2.380553</td>\n",
       "      <td>15.590335</td>\n",
       "      <td>2.919778</td>\n",
       "      <td>4.292048</td>\n",
       "      <td>5.502305</td>\n",
       "      <td>2.635927</td>\n",
       "      <td>4.481042</td>\n",
       "      <td>7.038948</td>\n",
       "      <td>...</td>\n",
       "      <td>16.719551</td>\n",
       "      <td>6.863171</td>\n",
       "      <td>3.768243</td>\n",
       "      <td>4.696467</td>\n",
       "      <td>0.568990</td>\n",
       "      <td>4.888728</td>\n",
       "      <td>4.470452</td>\n",
       "      <td>1.986855</td>\n",
       "      <td>4.735335</td>\n",
       "      <td>6.656105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceFiction&amp;Fantasy</th>\n",
       "      <td>12.291724</td>\n",
       "      <td>5.681256</td>\n",
       "      <td>12.896563</td>\n",
       "      <td>13.440439</td>\n",
       "      <td>5.663632</td>\n",
       "      <td>9.829995</td>\n",
       "      <td>13.780516</td>\n",
       "      <td>18.786913</td>\n",
       "      <td>15.399738</td>\n",
       "      <td>7.331449</td>\n",
       "      <td>...</td>\n",
       "      <td>14.831851</td>\n",
       "      <td>15.611158</td>\n",
       "      <td>8.393392</td>\n",
       "      <td>3.268926</td>\n",
       "      <td>44.355780</td>\n",
       "      <td>7.831203</td>\n",
       "      <td>14.729790</td>\n",
       "      <td>21.061625</td>\n",
       "      <td>3.074400</td>\n",
       "      <td>1.668250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td>1.042892</td>\n",
       "      <td>0.477669</td>\n",
       "      <td>1.331200</td>\n",
       "      <td>1.606718</td>\n",
       "      <td>0.508792</td>\n",
       "      <td>1.013002</td>\n",
       "      <td>0.607691</td>\n",
       "      <td>0.572132</td>\n",
       "      <td>9.378677</td>\n",
       "      <td>0.423487</td>\n",
       "      <td>...</td>\n",
       "      <td>5.377636</td>\n",
       "      <td>2.963852</td>\n",
       "      <td>1.170361</td>\n",
       "      <td>1.276208</td>\n",
       "      <td>0.080766</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>1.699078</td>\n",
       "      <td>0.039417</td>\n",
       "      <td>0.985276</td>\n",
       "      <td>1.251837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>9.485980</td>\n",
       "      <td>6.754494</td>\n",
       "      <td>4.912996</td>\n",
       "      <td>1.618118</td>\n",
       "      <td>1.670488</td>\n",
       "      <td>2.064946</td>\n",
       "      <td>2.555626</td>\n",
       "      <td>3.790989</td>\n",
       "      <td>1.437224</td>\n",
       "      <td>4.333231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692088</td>\n",
       "      <td>2.679235</td>\n",
       "      <td>6.677591</td>\n",
       "      <td>4.857566</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>2.983254</td>\n",
       "      <td>3.968895</td>\n",
       "      <td>5.380706</td>\n",
       "      <td>9.871348</td>\n",
       "      <td>6.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-fiction</th>\n",
       "      <td>0.862230</td>\n",
       "      <td>0.973327</td>\n",
       "      <td>1.112126</td>\n",
       "      <td>0.669858</td>\n",
       "      <td>1.676658</td>\n",
       "      <td>0.572411</td>\n",
       "      <td>1.327439</td>\n",
       "      <td>0.515594</td>\n",
       "      <td>1.492492</td>\n",
       "      <td>0.481186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741651</td>\n",
       "      <td>0.503049</td>\n",
       "      <td>0.309264</td>\n",
       "      <td>1.751363</td>\n",
       "      <td>1.452137</td>\n",
       "      <td>0.145903</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.500770</td>\n",
       "      <td>1.012127</td>\n",
       "      <td>2.151197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biography</th>\n",
       "      <td>1.800709</td>\n",
       "      <td>3.056844</td>\n",
       "      <td>2.215511</td>\n",
       "      <td>3.715312</td>\n",
       "      <td>1.168251</td>\n",
       "      <td>2.875914</td>\n",
       "      <td>2.784120</td>\n",
       "      <td>1.031910</td>\n",
       "      <td>2.795534</td>\n",
       "      <td>3.442409</td>\n",
       "      <td>...</td>\n",
       "      <td>4.828882</td>\n",
       "      <td>1.948300</td>\n",
       "      <td>1.785902</td>\n",
       "      <td>1.130707</td>\n",
       "      <td>0.616406</td>\n",
       "      <td>0.479798</td>\n",
       "      <td>1.415149</td>\n",
       "      <td>0.459228</td>\n",
       "      <td>2.577250</td>\n",
       "      <td>5.070610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mystery&amp;Thrillers</th>\n",
       "      <td>2.385971</td>\n",
       "      <td>11.370702</td>\n",
       "      <td>11.917312</td>\n",
       "      <td>4.828719</td>\n",
       "      <td>30.220630</td>\n",
       "      <td>9.474716</td>\n",
       "      <td>13.431266</td>\n",
       "      <td>8.544733</td>\n",
       "      <td>6.277536</td>\n",
       "      <td>8.417157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.158632</td>\n",
       "      <td>4.760259</td>\n",
       "      <td>9.266856</td>\n",
       "      <td>5.359408</td>\n",
       "      <td>7.850738</td>\n",
       "      <td>13.615812</td>\n",
       "      <td>6.792182</td>\n",
       "      <td>10.283846</td>\n",
       "      <td>3.596236</td>\n",
       "      <td>7.717618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialScience</th>\n",
       "      <td>0.473246</td>\n",
       "      <td>1.212689</td>\n",
       "      <td>0.291049</td>\n",
       "      <td>0.876018</td>\n",
       "      <td>1.000662</td>\n",
       "      <td>1.511150</td>\n",
       "      <td>0.619810</td>\n",
       "      <td>0.284034</td>\n",
       "      <td>1.542338</td>\n",
       "      <td>0.894159</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800016</td>\n",
       "      <td>0.101626</td>\n",
       "      <td>1.638494</td>\n",
       "      <td>4.677535</td>\n",
       "      <td>0.348810</td>\n",
       "      <td>0.959094</td>\n",
       "      <td>1.723113</td>\n",
       "      <td>1.301501</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>0.116144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Political</th>\n",
       "      <td>0.239310</td>\n",
       "      <td>0.267341</td>\n",
       "      <td>0.254426</td>\n",
       "      <td>2.897799</td>\n",
       "      <td>1.404965</td>\n",
       "      <td>0.814439</td>\n",
       "      <td>1.297295</td>\n",
       "      <td>0.570101</td>\n",
       "      <td>0.214395</td>\n",
       "      <td>0.754270</td>\n",
       "      <td>...</td>\n",
       "      <td>3.992367</td>\n",
       "      <td>1.008052</td>\n",
       "      <td>1.303379</td>\n",
       "      <td>3.328869</td>\n",
       "      <td>1.616735</td>\n",
       "      <td>0.240725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.850959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime</th>\n",
       "      <td>0.365526</td>\n",
       "      <td>1.661062</td>\n",
       "      <td>1.512851</td>\n",
       "      <td>0.634157</td>\n",
       "      <td>4.877255</td>\n",
       "      <td>1.720405</td>\n",
       "      <td>2.411162</td>\n",
       "      <td>1.609734</td>\n",
       "      <td>0.736752</td>\n",
       "      <td>1.094400</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.656691</td>\n",
       "      <td>1.806851</td>\n",
       "      <td>1.370367</td>\n",
       "      <td>0.938195</td>\n",
       "      <td>1.660063</td>\n",
       "      <td>1.357796</td>\n",
       "      <td>1.143525</td>\n",
       "      <td>0.492031</td>\n",
       "      <td>1.108737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horror</th>\n",
       "      <td>0.542057</td>\n",
       "      <td>0.178531</td>\n",
       "      <td>0.435382</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.503616</td>\n",
       "      <td>0.902296</td>\n",
       "      <td>1.353060</td>\n",
       "      <td>0.584837</td>\n",
       "      <td>0.293229</td>\n",
       "      <td>0.476470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397740</td>\n",
       "      <td>0.533020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.768215</td>\n",
       "      <td>0.318678</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.137741</td>\n",
       "      <td>0.092450</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.965447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FreeTime</th>\n",
       "      <td>1.436592</td>\n",
       "      <td>1.002722</td>\n",
       "      <td>1.906624</td>\n",
       "      <td>1.198160</td>\n",
       "      <td>0.457099</td>\n",
       "      <td>1.031466</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.597849</td>\n",
       "      <td>1.128122</td>\n",
       "      <td>0.556966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766794</td>\n",
       "      <td>0.919101</td>\n",
       "      <td>0.869332</td>\n",
       "      <td>1.399665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.862255</td>\n",
       "      <td>1.399589</td>\n",
       "      <td>1.051725</td>\n",
       "      <td>10.798408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Children&amp;Teens</th>\n",
       "      <td>3.092793</td>\n",
       "      <td>0.679949</td>\n",
       "      <td>5.012851</td>\n",
       "      <td>1.338786</td>\n",
       "      <td>0.796353</td>\n",
       "      <td>5.905008</td>\n",
       "      <td>1.356755</td>\n",
       "      <td>10.359567</td>\n",
       "      <td>1.925685</td>\n",
       "      <td>3.064237</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264469</td>\n",
       "      <td>6.515560</td>\n",
       "      <td>0.682641</td>\n",
       "      <td>1.795406</td>\n",
       "      <td>0.321750</td>\n",
       "      <td>1.631542</td>\n",
       "      <td>3.097130</td>\n",
       "      <td>2.578275</td>\n",
       "      <td>2.172902</td>\n",
       "      <td>1.796040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comics&amp;GraphicNovels</th>\n",
       "      <td>2.294475</td>\n",
       "      <td>1.197863</td>\n",
       "      <td>4.283858</td>\n",
       "      <td>3.330905</td>\n",
       "      <td>3.441015</td>\n",
       "      <td>4.779554</td>\n",
       "      <td>3.854951</td>\n",
       "      <td>3.003311</td>\n",
       "      <td>1.974938</td>\n",
       "      <td>1.006308</td>\n",
       "      <td>...</td>\n",
       "      <td>1.687748</td>\n",
       "      <td>6.963685</td>\n",
       "      <td>1.254654</td>\n",
       "      <td>3.201004</td>\n",
       "      <td>4.423904</td>\n",
       "      <td>1.436666</td>\n",
       "      <td>4.102356</td>\n",
       "      <td>3.998459</td>\n",
       "      <td>9.844895</td>\n",
       "      <td>1.145602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philosophy</th>\n",
       "      <td>1.595085</td>\n",
       "      <td>0.915037</td>\n",
       "      <td>1.213571</td>\n",
       "      <td>1.208962</td>\n",
       "      <td>2.069513</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.014615</td>\n",
       "      <td>0.476545</td>\n",
       "      <td>1.767604</td>\n",
       "      <td>0.601460</td>\n",
       "      <td>...</td>\n",
       "      <td>2.161756</td>\n",
       "      <td>0.426763</td>\n",
       "      <td>1.422370</td>\n",
       "      <td>2.568661</td>\n",
       "      <td>1.157956</td>\n",
       "      <td>1.616789</td>\n",
       "      <td>1.471099</td>\n",
       "      <td>2.093079</td>\n",
       "      <td>3.616881</td>\n",
       "      <td>1.588740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health-Mind&amp;Body</th>\n",
       "      <td>0.840770</td>\n",
       "      <td>1.111182</td>\n",
       "      <td>0.808223</td>\n",
       "      <td>0.387946</td>\n",
       "      <td>1.199952</td>\n",
       "      <td>1.193608</td>\n",
       "      <td>1.811452</td>\n",
       "      <td>0.759322</td>\n",
       "      <td>1.484601</td>\n",
       "      <td>0.448532</td>\n",
       "      <td>...</td>\n",
       "      <td>1.325110</td>\n",
       "      <td>0.152439</td>\n",
       "      <td>1.037478</td>\n",
       "      <td>0.805829</td>\n",
       "      <td>0.719663</td>\n",
       "      <td>0.409812</td>\n",
       "      <td>2.403571</td>\n",
       "      <td>4.908939</td>\n",
       "      <td>4.054835</td>\n",
       "      <td>1.602787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Professional&amp;Technical</th>\n",
       "      <td>0.707131</td>\n",
       "      <td>0.437578</td>\n",
       "      <td>0.751153</td>\n",
       "      <td>0.606860</td>\n",
       "      <td>1.446676</td>\n",
       "      <td>2.227537</td>\n",
       "      <td>0.631628</td>\n",
       "      <td>1.367269</td>\n",
       "      <td>1.044094</td>\n",
       "      <td>0.845954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287863</td>\n",
       "      <td>0.611320</td>\n",
       "      <td>2.842477</td>\n",
       "      <td>3.827005</td>\n",
       "      <td>1.288979</td>\n",
       "      <td>0.072150</td>\n",
       "      <td>1.150915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>1.329849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science&amp;Nature</th>\n",
       "      <td>0.342586</td>\n",
       "      <td>0.155363</td>\n",
       "      <td>0.628578</td>\n",
       "      <td>0.482757</td>\n",
       "      <td>1.520974</td>\n",
       "      <td>0.815999</td>\n",
       "      <td>0.194485</td>\n",
       "      <td>0.379692</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>0.465401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500463</td>\n",
       "      <td>0.392499</td>\n",
       "      <td>1.291395</td>\n",
       "      <td>2.309029</td>\n",
       "      <td>1.430235</td>\n",
       "      <td>0.168350</td>\n",
       "      <td>1.528300</td>\n",
       "      <td>1.305543</td>\n",
       "      <td>1.482953</td>\n",
       "      <td>1.518638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               amore       donna       amico      guerra  \\\n",
       "total                     498.000000  489.000000  342.000000  280.000000   \n",
       "Family-Sex&Relationships    7.758273    7.133577    5.301741    2.326511   \n",
       "Fiction&Literature         46.598101   47.166554   37.712174   42.023418   \n",
       "Humor                       1.683975    2.261595    3.121255    1.015098   \n",
       "History                     4.160573    6.304667    2.380553   15.590335   \n",
       "ScienceFiction&Fantasy     12.291724    5.681256   12.896563   13.440439   \n",
       "Travel                      1.042892    0.477669    1.331200    1.606718   \n",
       "Romance                     9.485980    6.754494    4.912996    1.618118   \n",
       "Non-fiction                 0.862230    0.973327    1.112126    0.669858   \n",
       "Biography                   1.800709    3.056844    2.215511    3.715312   \n",
       "Mystery&Thrillers           2.385971   11.370702   11.917312    4.828719   \n",
       "SocialScience               0.473246    1.212689    0.291049    0.876018   \n",
       "Political                   0.239310    0.267341    0.254426    2.897799   \n",
       "Crime                       0.365526    1.661062    1.512851    0.634157   \n",
       "Horror                      0.542057    0.178531    0.435382    0.203125   \n",
       "FreeTime                    1.436592    1.002722    1.906624    1.198160   \n",
       "Children&Teens              3.092793    0.679949    5.012851    1.338786   \n",
       "Comics&GraphicNovels        2.294475    1.197863    4.283858    3.330905   \n",
       "Philosophy                  1.595085    0.915037    1.213571    1.208962   \n",
       "Health-Mind&Body            0.840770    1.111182    0.808223    0.387946   \n",
       "Professional&Technical      0.707131    0.437578    0.751153    0.606860   \n",
       "Science&Nature              0.342586    0.155363    0.628578    0.482757   \n",
       "\n",
       "                                caso     bambino       morte     ragazzo  \\\n",
       "total                     259.000000  287.000000  313.000000  296.000000   \n",
       "Family-Sex&Relationships    2.359581    3.998856    3.538663    4.220919   \n",
       "Fiction&Literature         32.562166   41.243915   40.017384   38.401293   \n",
       "Humor                       2.531946    3.252735    1.004468    1.507328   \n",
       "History                     2.919778    4.292048    5.502305    2.635927   \n",
       "ScienceFiction&Fantasy      5.663632    9.829995   13.780516   18.786913   \n",
       "Travel                      0.508792    1.013002    0.607691    0.572132   \n",
       "Romance                     1.670488    2.064946    2.555626    3.790989   \n",
       "Non-fiction                 1.676658    0.572411    1.327439    0.515594   \n",
       "Biography                   1.168251    2.875914    2.784120    1.031910   \n",
       "Mystery&Thrillers          30.220630    9.474716   13.431266    8.544733   \n",
       "SocialScience               1.000662    1.511150    0.619810    0.284034   \n",
       "Political                   1.404965    0.814439    1.297295    0.570101   \n",
       "Crime                       4.877255    1.720405    2.411162    1.609734   \n",
       "Horror                      0.503616    0.902296    1.353060    0.584837   \n",
       "FreeTime                    0.457099    1.031466    0.905312    0.597849   \n",
       "Children&Teens              0.796353    5.905008    1.356755   10.359567   \n",
       "Comics&GraphicNovels        3.441015    4.779554    3.854951    3.003311   \n",
       "Philosophy                  2.069513    0.480000    1.014615    0.476545   \n",
       "Health-Mind&Body            1.199952    1.193608    1.811452    0.759322   \n",
       "Professional&Technical      1.446676    2.227537    0.631628    1.367269   \n",
       "Science&Nature              1.520974    0.815999    0.194485    0.379692   \n",
       "\n",
       "                             viaggio    famiglia  ...     popolo   compagno  \\\n",
       "total                     240.000000  333.000000  ...  89.000000  82.000000   \n",
       "Family-Sex&Relationships    2.891508    6.951478  ...   0.517999   4.824721   \n",
       "Fiction&Literature         41.063557   49.739449  ...  38.565369  39.538734   \n",
       "Humor                       1.953963    1.633051  ...   2.382013   2.026766   \n",
       "History                     4.481042    7.038948  ...  16.719551   6.863171   \n",
       "ScienceFiction&Fantasy     15.399738    7.331449  ...  14.831851  15.611158   \n",
       "Travel                      9.378677    0.423487  ...   5.377636   2.963852   \n",
       "Romance                     1.437224    4.333231  ...   0.692088   2.679235   \n",
       "Non-fiction                 1.492492    0.481186  ...   0.741651   0.503049   \n",
       "Biography                   2.795534    3.442409  ...   4.828882   1.948300   \n",
       "Mystery&Thrillers           6.277536    8.417157  ...   1.158632   4.760259   \n",
       "SocialScience               1.542338    0.894159  ...   1.800016   0.101626   \n",
       "Political                   0.214395    0.754270  ...   3.992367   1.008052   \n",
       "Crime                       0.736752    1.094400  ...        NaN   0.656691   \n",
       "Horror                      0.293229    0.476470  ...   0.397740   0.533020   \n",
       "FreeTime                    1.128122    0.556966  ...   0.766794   0.919101   \n",
       "Children&Teens              1.925685    3.064237  ...   1.264469   6.515560   \n",
       "Comics&GraphicNovels        1.974938    1.006308  ...   1.687748   6.963685   \n",
       "Philosophy                  1.767604    0.601460  ...   2.161756   0.426763   \n",
       "Health-Mind&Body            1.484601    0.448532  ...   1.325110   0.152439   \n",
       "Professional&Technical      1.044094    0.845954  ...   0.287863   0.611320   \n",
       "Science&Nature              0.716969    0.465401  ...   0.500463   0.392499   \n",
       "\n",
       "                          relazione      crisi  fantascienza     vicino  \\\n",
       "total                     79.000000  76.000000     74.000000  66.000000   \n",
       "Family-Sex&Relationships   7.936964   5.496743      0.979787   6.267687   \n",
       "Fiction&Literature        43.094314  40.440970     30.012201  51.801614   \n",
       "Humor                      3.448042   5.670057      1.173463   2.712446   \n",
       "History                    3.768243   4.696467      0.568990   4.888728   \n",
       "ScienceFiction&Fantasy     8.393392   3.268926     44.355780   7.831203   \n",
       "Travel                     1.170361   1.276208      0.080766   0.151515   \n",
       "Romance                    6.677591   4.857566      0.344828   2.983254   \n",
       "Non-fiction                0.309264   1.751363      1.452137   0.145903   \n",
       "Biography                  1.785902   1.130707      0.616406   0.479798   \n",
       "Mystery&Thrillers          9.266856   5.359408      7.850738  13.615812   \n",
       "SocialScience              1.638494   4.677535      0.348810   0.959094   \n",
       "Political                  1.303379   3.328869      1.616735   0.240725   \n",
       "Crime                      1.806851   1.370367      0.938195   1.660063   \n",
       "Horror                          NaN   0.768215      0.318678   0.585938   \n",
       "FreeTime                   0.869332   1.399665           NaN   0.340909   \n",
       "Children&Teens             0.682641   1.795406      0.321750   1.631542   \n",
       "Comics&GraphicNovels       1.254654   3.201004      4.423904   1.436666   \n",
       "Philosophy                 1.422370   2.568661      1.157956   1.616789   \n",
       "Health-Mind&Body           1.037478   0.805829      0.719663   0.409812   \n",
       "Professional&Technical     2.842477   3.827005      1.288979   0.072150   \n",
       "Science&Nature             1.291395   2.309029      1.430235   0.168350   \n",
       "\n",
       "                              cielo     angelo      gioia  entusiasmo  \n",
       "total                     66.000000  59.000000  57.000000   41.000000  \n",
       "Family-Sex&Relationships   3.729795   2.423222   5.420215    4.339015  \n",
       "Fiction&Literature        42.426208  36.569484  41.567252   38.862795  \n",
       "Humor                      2.140362   2.473487   2.112668    2.428131  \n",
       "History                    4.470452   1.986855   4.735335    6.656105  \n",
       "ScienceFiction&Fantasy    14.729790  21.061625   3.074400    1.668250  \n",
       "Travel                     1.699078   0.039417   0.985276    1.251837  \n",
       "Romance                    3.968895   5.380706   9.871348    6.033089  \n",
       "Non-fiction                0.793814   0.500770   1.012127    2.151197  \n",
       "Biography                  1.415149   0.459228   2.577250    5.070610  \n",
       "Mystery&Thrillers          6.792182  10.283846   3.596236    7.717618  \n",
       "SocialScience              1.723113   1.301501   0.696777    0.116144  \n",
       "Political                       NaN        NaN        NaN    1.850959  \n",
       "Crime                      1.357796   1.143525   0.492031    1.108737  \n",
       "Horror                     0.137741   0.092450   0.724638    0.965447  \n",
       "FreeTime                   0.862255   1.399589   1.051725   10.798408  \n",
       "Children&Teens             3.097130   2.578275   2.172902    1.796040  \n",
       "Comics&GraphicNovels       4.102356   3.998459   9.844895    1.145602  \n",
       "Philosophy                 1.471099   2.093079   3.616881    1.588740  \n",
       "Health-Mind&Body           2.403571   4.908939   4.054835    1.602787  \n",
       "Professional&Technical     1.150915        NaN   0.910256    1.329849  \n",
       "Science&Nature             1.528300   1.305543   1.482953    1.518638  \n",
       "\n",
       "[22 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(percentage_dict)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c403877-0d9a-4c0e-9a7e-6bb8f11ecd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nota: eliminare tutti i libri con genere Comics&GraphicNovels ? Da valutare\n",
    "\n",
    "#Adesso che abbiamo il dataframe del peso delle varie keyword, possiamo creare il vettore \"generi\" dei libri sui\n",
    "#quali calcolare la similarità.\n",
    "\n",
    "#VETTORI GENERI = [0,0,....,0]\n",
    "#DFgenres.show(200, False)\n",
    "#Il vettore genere è lungo 21 elementi e contiene, nella i-esima posizione, un valore che indica quanto il libro\n",
    "#appartiene a quel genere.\n",
    "\n",
    "mapping = {'Fiction&Literature': 0,\n",
    "              'Family-Sex&Relationships': 1,\n",
    "              'Humor': 2,\n",
    "              'History': 3,\n",
    "              'ScienceFiction&Fantasy': 4,\n",
    "              'Romance': 5,\n",
    "              'Travel': 6,\n",
    "              'Mystery&Thrillers': 7,\n",
    "              'FreeTime': 8,\n",
    "              'Non-fiction': 9,\n",
    "              'Biography': 10,\n",
    "              'SocialScience': 11,\n",
    "              'Political': 12,\n",
    "              'Crime': 13,\n",
    "              'Children&Teens': 14,\n",
    "              'Philosophy': 15,\n",
    "              'Horror': 16,\n",
    "              'Health-Mind&Body': 17,\n",
    "              'Professional&Technical': 18,\n",
    "              'Science&Nature': 19,\n",
    "              'Comics&GraphicNovels': 20}\n",
    "\n",
    "DF_final_pandas = DF_final.toPandas()\n",
    "\n",
    "DF_mapped = pd.Series(DF_final_pandas['genre'].values)\n",
    "\n",
    "def mapSeries(line):\n",
    "    \n",
    "    line = line.replace(\"'\", '\"')\n",
    "    line = json.loads(line)\n",
    "    for key in line:\n",
    "        line[key] = float(line[key])\n",
    "        \n",
    "    total = sum(line.values())\n",
    "    genre_vector = [0 for i in range(21)]\n",
    "    \n",
    "    for key in line:\n",
    "        value_genre = line[key] / total\n",
    "        genre_vector[mapping[key]] = value_genre\n",
    "    \n",
    "    return genre_vector\n",
    "\n",
    "DF_transformed = DF_mapped.map(mapSeries)\n",
    "\n",
    "DF_final_pandas_new = DF_final_pandas.assign(genre_vector = DF_transformed)\n",
    "DF_final_pandas_new['genre_vector'] = DF_final_pandas_new['genre_vector'].astype(str)\n",
    "\n",
    "DF_final_new = spark.createDataFrame(DF_final_pandas_new)\n",
    "\n",
    "DF_final_new = DF_final_new.withColumn('genre_vector', regexp_replace('genre_vector', '\\[', ''))\n",
    "DF_final_new = DF_final_new.withColumn('genre_vector', regexp_replace('genre_vector', '\\]', ''))\n",
    "#DF_final_new = DF_final_new.withColumn('genre_vector', array(DF_final_new.genre_vector))\n",
    "\n",
    "DF_final_new = DF_final_new.withColumn('genre_vector', split(col('genre_vector'), \",\"))\n",
    "\n",
    "#NOTA: eliminare comics? Come migliorare le keywords?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3810bb-0d7f-4ac5-b6ba-dacaac14ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_final_new = DF_final_new.drop('_c0').drop('content2')\n",
    "\n",
    "DF_final_new.toPandas().to_csv('for_now_final_DF_with_genre_vector.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952f9d7b-e299-4b79-90fc-d093c790f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_final = spark.read.csv('for_now_final_DF_with_genre_vector.csv', header=True)\n",
    "#DF_final.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "544f68dc-9f17-42a2-b060-a09af70609a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = spark.read.csv(\"for_now_final_DF_with_genre_vector.csv\", header=True)\n",
    "#DF_final.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c149648-1e02-4c93-8e52-557a99f0dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAZIONE FILE JSON SECONDO I CANONI DEL TEAM VR\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DF_final_pandas = DF_final.toPandas()\n",
    "#print(type(DF_final_pandas['genre_vector']))\n",
    "#DF_final_pandas.groupby(['book_id']).apply(lambda x: x[['genre_vector']].to_dict()).reset_index().to_json(orient='records')\n",
    "\n",
    "#DF_final_pandas.to_json('books.json')\n",
    "#JSON_ = DF_final_pandas.to_dict('records')\n",
    "#print(JSON_[0:5])\n",
    "column_list = []\n",
    "column_keywords_list =  []\n",
    "column_dict_out = []\n",
    "\n",
    "for genre_string in DF_final_pandas['genre_vector']:\n",
    "    \n",
    "    genre_string = genre_string.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "    genre_list = [float(element.replace(\"  \", \"\")) for element in genre_string.split(\",\")]\n",
    "    column_list.append(genre_list)  \n",
    "    \n",
    "for keywords_string in DF_final_pandas['top_keywords']:\n",
    "    keywords_string = keywords_string.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "    keywords_list = [str(element.replace(\" \", \"\")) for element in keywords_string.split(\",\")]\n",
    "    column_keywords_list.append(keywords_list)\n",
    "    \n",
    "for genre_string_dict in DF_final_pandas['genre']: \n",
    "    column_dict = []\n",
    "    genre_string_dict = genre_string_dict.replace(\"'\", '\"')\n",
    "    genre_dict = json.loads(genre_string_dict)\n",
    "    for key in genre_dict:\n",
    "        genre_string = key + \":\" + genre_dict[key]\n",
    "        column_dict.append(genre_string)\n",
    "    column_dict_out.append(column_dict)\n",
    "\n",
    "#print(column_dict_out)\n",
    "DF_final_pandas['genre_vector'] = pd.Series(column_list).values\n",
    "DF_final_pandas['top_keywords'] = pd.Series(column_keywords_list).values\n",
    "DF_final_pandas['genre'] = pd.Series(column_dict_out).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45f9ce01-784d-41d7-b408-9c670ee52762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DF_final_pandas.groupby(['book_id']).apply(lambda x: x[['genre_vector']].to_dict('records')).reset_index().to_json(orient='records')\n",
    "#DF_final_pandas.to_json('books.json')\n",
    "\n",
    "dict_ = DF_final_pandas.to_dict()\n",
    "#3426 books\n",
    "\n",
    "#Rearrange the json\n",
    "output_dict = [] #Containing elements with the structure \"book_id\" : \"metadata\"\n",
    "#print(output_dict)\n",
    "with open('books3.json', 'w') as f:\n",
    "    for _id in range(3426):\n",
    "        metadata_dict = {}\n",
    "        for key in dict_:\n",
    "            metadata_dict[key] = dict_[key][_id]\n",
    "            #print(metadata_dict)\n",
    "        #print(metadata_dict['genre'])\n",
    "        output_dict.append(metadata_dict)\n",
    "    json.dump([output_dict], f)\n",
    "        \n",
    "    \n",
    "        \n",
    "#print(output_dict[1])\n",
    "#with open('books.json', 'w') as f:\n",
    "#    json.dump([], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f8dc9-557c-44d2-bea8-be001b3c796b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f8f71-e626-4766-ba40-fa952eb7f046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
