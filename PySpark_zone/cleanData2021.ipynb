{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341f9886-9da6-4867-b8a9-31db685a5b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Anobii data\n",
      "Raws data have been read\n",
      "Filter Anobii data (select books in italian, drop comics and books with a low number of ratings)\n"
     ]
    }
   ],
   "source": [
    "##Python .py code from .jpynb:\n",
    "from pyspark.sql.functions import rand, col\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.types import IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import row_number, lit, dense_rank, concat_ws\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "#############################################\n",
    "#            read data                      # \n",
    "#############################################\n",
    "print(\"Read Anobii data\")\n",
    "\n",
    "# Read table which contains for each item_id (id of each book), the id of the author (author_id) who wrote that book\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/author_item.csv\"\n",
    "DFauthorbooks = spark.read.csv(file,header=True) #MOSTRA I LIBRI RELATIVI AGLI AUTORI\n",
    "\n",
    "# Read table which contains for each author_id (id of each auhtor), the info of that author\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/author_display.csv\"\n",
    "DFdisplay = spark.read.csv(file,header=True) #MOSTRA GLI AUTORI DEL DATABASE\n",
    "\n",
    "# Read table which contains the mapping of the language (language=11 means italian)\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/language_mapping.csv\"\n",
    "DFlanguages = spark.read.csv(file,header=True)\n",
    "\n",
    "# Read table which contains the info about items, i.e. books\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/item.csv\"\n",
    "DFitems_anobii = spark.read.csv(file,header=True) \n",
    "# DFitems_anobii.show(1, False)\n",
    "\n",
    "# Read table which contains the rate given by a person to a book\n",
    "file = \"/data/SMARTDATA/books/anobii_2021/sql/link_person_item.csv\" #item_review ha il voto dato dall'utente a quel libro\n",
    "DFinfo = spark.read.csv(file,header=True)\n",
    "# DFinfo.show(1, False)\n",
    "\n",
    "# Read table which contains the genre of each book\n",
    "file = \"anobii_genres/new_genres5.csv\"\n",
    "DFgenres = spark.read.csv(file, header=True)\n",
    "#DFgenres.filter(DFgenres.itemid == \"1981748\").show(10, False)\n",
    "\n",
    "file = \"anobii_genres/content.csv\"\n",
    "DFdescriptions = spark.read.csv(file, header=True)\n",
    "#DFdescriptions.show(1, False)\n",
    "\n",
    "#DFdescriptions.filter(DFdescriptions.item_id==\"2015441\").show(2, False)\n",
    "#FUNZIONI\n",
    "file = \"/data/SMARTDATA/books/anobii/link_person_item_comment.csv\"\n",
    "DFreviews = spark.read.csv(file, header=True)\n",
    "#DFreviews.show(1, False)\n",
    "\n",
    "def title_and_subtitle(line): #Usata per splittare titolo e sottotitolo nel database delle biblioteche e renderli più simili ad anobii\n",
    "    if len(line.title.split(\" : \")) > 1: #Il libro possiede un sottotitolo\n",
    "        title = line.title.split(\" : \")[0]\n",
    "        sub_title = line.title.split(\" : \")[1]\n",
    "        return Row(line.manifestation_id_new, line.patron_id_md5, title, sub_title, line.author, 4, \"None\", 4, \"None\", line.ISBNISSN_new, \"bct\") #NE APPROFITTO ANCHE PER INSERIRE VOTO E CAMPI PER IL MERGE (PER ORA NULL)\n",
    "    else: #Il libro non possiede un sottotitolo\n",
    "        return Row(line.manifestation_id_new, line.patron_id_md5, line.title, \"None\", line.author, 4, \"None\", 4, \"None\", line.ISBNISSN_new, \"bct\")\n",
    "\n",
    "def create_user_dictionary(rdd): #Used to assign an integer id to each user of the rdd (to get the rows of the CSR)\n",
    "    rdd = rdd.map(lambda x: (str(x.person_id), list(x))).sortByKey()\n",
    "    user_dictionary = rdd.countByKey()\n",
    "    i = 0\n",
    "    for key in user_dictionary.keys():\n",
    "        user_dictionary[key] = i\n",
    "        i += 1\n",
    "    return user_dictionary\n",
    "\n",
    "def create_book_dictionary(rdd): #Used to assign an integer id to each book of the rdd (to get the column of the CSR)\n",
    "    rdd = rdd.map(lambda x: (x.book_id, list(x))).sortByKey()\n",
    "    book_dictionary = rdd.countByKey()\n",
    "    i = 0\n",
    "    for key in book_dictionary.keys():\n",
    "        book_dictionary[key] = i\n",
    "        i += 1\n",
    "    return book_dictionary\n",
    "\n",
    "def addDataType1(line):\n",
    "    return Row(line.item_id, line.person_id, line.title, str(line.sub_title), line.item_review, line.total_review, line.average_rating, str(line.total_votes), line.isbn, \"anobii\")\n",
    "\n",
    "\n",
    "def addRowColumnId(line): #Funzione usata per aggiungere gli indici per ogni libro e utente\n",
    "    book_number = book_dictionary[line.book_id] \n",
    "    user_number = user_dictionary[line.person_id]\n",
    "    return Row(line.book_id,\n",
    "               line.title,\n",
    "               line.sub_title,\n",
    "               line.total_wishlist,\n",
    "               line.no_of_page,\n",
    "               line.publication_date,\n",
    "               line.publisher,\n",
    "               line.binding,\n",
    "               line.edition,\n",
    "               line.product_type,\n",
    "               line.total_votes,\n",
    "               line.data_type,\n",
    "               line.person_id,\n",
    "               line.item_review,\n",
    "               line.author,\n",
    "               line.genre,\n",
    "               line.encrypt_item_id,\n",
    "               line.isbn,\n",
    "               line.total_count,\n",
    "               line.average_rating,\n",
    "               line.total_review,\n",
    "               str(user_number), \n",
    "               str(book_number))\n",
    "    return Row(line.person_id, line.title, str(line.sub_title), line.item_review, line.data_type, line.book_id, line.author, line.genre, line.encrypt_item_id, line.total_votes_or_loans, line.average_rating, line.total_review, line.isbn, str(user_number), str(book_number))\n",
    "\n",
    "def title_and_subtitle_books(line): #FUNZIONE USATA PER DIVIDERE TITOLO E SOTTOTITOLO NEL DATASET DEI LIBRI DELLE BIBLIOTECHE E PER ELIMINARE EDITION_DATE ED EDITION_LANGUAGE\n",
    "    if len(line.title.split(\" : \")) > 1:\n",
    "        title = line.title.split(\" : \")[0]\n",
    "        sub_title = line.title.split(\" : \")[1]\n",
    "    else:\n",
    "        title = line.title\n",
    "        sub_title = \"None\"\n",
    "    #\"title\", \"sub_title\", \"author\", \"publisher\", \"book_id\"\n",
    "    return Row(title, sub_title, line.author, line.publisher, line.manifestation_id_new, line.ISBNISSN_new)\n",
    "\n",
    "def concatenateContentsDescriptions(line1, line2):\n",
    "    #print(line1)\n",
    "    #Caso in cui vi è più di una descrizione presente per uno stesso libro\n",
    "    return Row(book_id=line1.book_id, title=line1.title, sub_title=line1.sub_title, \n",
    "               total_wishlist=line1.total_wishlist, no_of_page=line1.no_of_page, publication_date=line1.publication_date, \n",
    "               publisher=line1.publisher, binding=line1.binding, edition=line1.edition, product_type=line1.product_type, \n",
    "               total_votes=line1.total_votes, data_type=line1.data_type, author=line1.author, genre=line1.genre, \n",
    "               encrypt_item_id=line1.encrypt_item_id, isbn=line1.isbn, total_count=line1.total_count, average_rating=line1.average_rating, \n",
    "               total_review=line1.total_review, book_index=line1.book_index, content=line1.content + \" \" + line2.content, content2=line1.content2)\n",
    "\n",
    "def toSingleDescriptions(line1):\n",
    "    #print(line1)\n",
    "    return Row(book_id=line1[1].book_id, title=line1[1].title, sub_title=line1[1].sub_title, \n",
    "               total_wishlist=line1[1].total_wishlist, no_of_page=line1[1].no_of_page, publication_date=line1[1].publication_date, \n",
    "               publisher=line1[1].publisher, binding=line1[1].binding, edition=line1[1].edition, product_type=line1[1].product_type, \n",
    "               total_votes=line1[1].total_votes, data_type=line1[1].data_type, author=line1[1].author, genre=line1[1].genre, \n",
    "               encrypt_item_id=line1[1].encrypt_item_id, isbn=line1[1].isbn, total_count=line1[1].total_count, average_rating=line1[1].average_rating, \n",
    "               total_review=line1[1].total_review, book_index=line1[1].book_index, content=line1[1].content, content2=line1[1].content2)\n",
    "\n",
    "def concatenateContentsComments(line1, line2):\n",
    "    #print(line1)\n",
    "    #Caso in cui dobbiamo concatenare i commenti di un libro alla sua descrizione\n",
    "    return Row(book_id=line1.book_id, title=line1.title, sub_title=line1.sub_title, \n",
    "               total_wishlist=line1.total_wishlist, no_of_page=line1.no_of_page, publication_date=line1.publication_date, \n",
    "               publisher=line1.publisher, binding=line1.binding, edition=line1.edition, product_type=line1.product_type, \n",
    "               total_votes=line1.total_votes, data_type=line1.data_type, author=line1.author, genre=line1.genre, \n",
    "               encrypt_item_id=line1.encrypt_item_id, isbn=line1.isbn, total_count=line1.total_count, average_rating=line1.average_rating, \n",
    "               total_review=line1.total_review, book_index=line1.book_index, content=line1.content, content2=line1.content2, comment_content = str(line1.comment_content) + \" \" + str(line2.comment_content))\n",
    "\n",
    "def mergeDescriptionComments(line1):\n",
    "    return Row(book_id=line1.book_id, title=line1.title, sub_title=line1.sub_title, \n",
    "               total_wishlist=line1.total_wishlist, no_of_page=line1.no_of_page, publication_date=line1.publication_date, \n",
    "               publisher=line1.publisher, binding=line1.binding, edition=line1.edition, product_type=line1.product_type, \n",
    "               total_votes=line1.total_votes, data_type=line1.data_type, author=line1.author, genre=line1.genre, \n",
    "               encrypt_item_id=line1.encrypt_item_id, isbn=line1.isbn, total_count=line1.total_count, average_rating=line1.average_rating, \n",
    "               total_review=line1.total_review, book_index=line1.book_index, content2=line1.content2, content=str(line1.content) + \" \" + str(line1.comment_content))\n",
    "\n",
    "def toSingleComments(line1):\n",
    "    #print(line1)\n",
    "    return Row(book_id=line1[1].book_id, title=line1[1].title, sub_title=line1[1].sub_title, \n",
    "               total_wishlist=line1[1].total_wishlist, no_of_page=line1[1].no_of_page, publication_date=line1[1].publication_date, \n",
    "               publisher=line1[1].publisher, binding=line1[1].binding, edition=line1[1].edition, product_type=line1[1].product_type, \n",
    "               total_votes=line1[1].total_votes, data_type=line1[1].data_type, author=line1[1].author, genre=line1[1].genre, \n",
    "               encrypt_item_id=line1[1].encrypt_item_id, isbn=line1[1].isbn, total_count=line1[1].total_count, average_rating=line1[1].average_rating, \n",
    "               total_review=line1[1].total_review, book_index=line1[1].book_index, content2=line1[1].content2, content=line1[1].content, comment_content = line1[1].comment_content)\n",
    "print(\"Raws data have been read\")\n",
    "#these are the columns which are needed for our work, for the item table. The others can be dropped\n",
    "DFitems_anobii_cols_to_keep = [\n",
    "                        \"item_id\",\\\n",
    "                        \"isbn\",\\\n",
    "#                         \"family_id\",\\\n",
    "                        \"title\",\\\n",
    "                        \"sub_title\",\\\n",
    "                        # \"barcode\",\\\n",
    "                        # \"image_source\",\\\n",
    "                        # \"image_width\",\\\n",
    "                        # \"image_height\",\\\n",
    "                        \"no_of_page\",\\\n",
    "                        \"publication_date\",\\\n",
    "                        \"publisher\",\\\n",
    "                        \"binding\",\\\n",
    "                        \"edition\",\\\n",
    "                        # \"reading_level\",\\\n",
    "                        # \"height\",\\\n",
    "                        # \"height_unit\",\\\n",
    "                        # \"length\",\\\n",
    "                        # \"length_unit\",\\\n",
    "                        # \"width\",\\\n",
    "                        # \"width_unit\",\\\n",
    "                        # \"weight\",\\\n",
    "                        # \"weight_unit\",\\\n",
    "                        # \"salesrank\",\\\n",
    "                        # \"item_popularity\",\\\n",
    "                        # \"check_same_family\",\\\n",
    "                        # \"check_internal_family\",\\\n",
    "                        # \"last_update\",\\\n",
    "                        \"average_rating\",\\\n",
    "                        \"total_review\",\\\n",
    "                        \"product_type\",\\\n",
    "                        # \"title_foreign\",\\\n",
    "                        # \"image_process\",\\\n",
    "                        # \"amazon\",\\\n",
    "                        # \"google\",\\\n",
    "                        \"encrypt_item_id\",\\\n",
    "                        # \"pid\",\\\n",
    "                        # \"binding_id\",\\\n",
    "                        # \"problem\",\\\n",
    "                        \"language\",\\\n",
    "                        # \"language_correct\",\\\n",
    "                        # \"ean\",\\\n",
    "                        # \"family_head\",\\\n",
    "                        # \"google_time\",\\\n",
    "                        # \"total_world\",\\\n",
    "                        # \"lock\",\\\n",
    "                        # \"volumes\",\\\n",
    "                        # \"publication_country_id\",\\\n",
    "                        # \"added_date\",\\\n",
    "                        # \"publishing_status\",\\\n",
    "                        # \"total_libraries\",\\\n",
    "                        # \"unavailable_date\",\\\n",
    "                        # \"table_of_contents_html\",\\\n",
    "                        # \"product_form_detail\",\\\n",
    "                        # \"total_edition_ratings\",\\\n",
    "                        # \"epub_url\",\\\n",
    "                        # \"imprint_name\",\\\n",
    "                        # \"is_sellable\",\\\n",
    "                        # \"total_topics\",\\\n",
    "                        # \"publisher_id\",\\\n",
    "                        \"total_wishlist\",\\\n",
    "                        # \"embargo_date\",\\\n",
    "                        # \"data_source\",\\\n",
    "                        # \"ebook_type\",\\\n",
    "                        # \"has_ebook\",\\\n",
    "                        # \"fulfillment_book_id\",\\\n",
    "                        # \"ebook_filesize\",\\\n",
    "                        # \"sample_status\",\\\n",
    "                        # \"sample_url\",\\\n",
    "                        # \"sample_filesize\",\\\n",
    "                      ]\n",
    "DFitems_anobii = DFitems_anobii.select(DFitems_anobii_cols_to_keep)\n",
    "print(\"Filter Anobii data (select books in italian, drop comics and books with a low number of ratings)\")\n",
    "#FILTRAGGIO 1: linguaggio italiano\n",
    "#Mi basterà filtrare da item.csv tutte le tuple con language diverso da 11.\n",
    "\n",
    "DFfilteredlanguageitems = DFitems_anobii.filter(DFitems_anobii.language == \"11\")\n",
    "DFitems_anobii = DFitems_anobii.select(DFitems_anobii_cols_to_keep)\n",
    "#DFfilteredlanguageitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 2: solo \"hardcover\" e \"paperback\"\n",
    "\n",
    "DFfilteredbindingitems = DFfilteredlanguageitems.filter((DFfilteredlanguageitems.binding == \"Paperback\") | (DFfilteredlanguageitems.binding == \"Hardcover\"))\n",
    "#DFfilteredbindingitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 3: eliminare i fumetti (per quanto possibile)\n",
    "#Userò come parole chiave personaggi famosi Disney quali Topolino e Paperino e i fumetti Bonelli (che fra gli italiani\n",
    "#sono probabilmente i più gettonati)\n",
    "\n",
    "DFfilteredmagazineitems = DFfilteredbindingitems.filter(~(DFfilteredbindingitems.title.contains(\"Topolino\")) | (DFfilteredbindingitems.title.contains(\"Paperino\"))|(DFfilteredbindingitems.title.contains(\"Tex\"))|(DFfilteredbindingitems.title.contains(\"Dylan Dog\"))|(DFfilteredbindingitems.title.contains(\"Nathan Never\"))|(DFfilteredbindingitems.title.contains(\"Zagor\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cbad8b-6ad5-47e9-b96f-e9a110184389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFgenres.filter(DFgenres.votes.isNull()).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e51d1714-dab7-43e3-8ba0-c433d7a4c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESSING DATAFRAME GENRES#\n",
    "\n",
    "#Fase 1: eliminare le tuple con SelfHelp, Textbook e Reference come genere\n",
    "#DFgenres.filter(DFgenres.name == \"Textbook\").show(1, False)\n",
    "DFgenres = DFgenres.filter((DFgenres.name != \"SelfHelp\") & (DFgenres.name != \"Reference\") & (DFgenres.name != \"Textbook\"))\n",
    "\n",
    "#Fase 1.2: eliminare le tuple con votes = null (esistono)\n",
    "\n",
    "DFgenres = DFgenres.filter(~(DFgenres.votes.isNull()))\n",
    "#DFgenres.filter(DFgenres.votes.isNull()).show(10, False)\n",
    "#Fase 2: mappare i vari generi alla corrispondente aggregazione scelta da Greta e aggregare eventuali duplicati\n",
    "#NOTA: usiamo un RDD\n",
    "\n",
    "RDDgenres = DFgenres.rdd\n",
    "\n",
    "def mapGenres(line):\n",
    "    transformed_line = line.name\n",
    "    if transformed_line == \"Cooking-Food&Wine\" or transformed_line == \"Games\" or transformed_line == \"Crafts&Hobbies\" or transformed_line == \"Home&Gardening\" or transformed_line == \"Music\" or transformed_line == \"Art-Architecture&Photography\" or transformed_line == \"Sports-Outdoors&Adventure\" or transformed_line == \"Entertainment\":\n",
    "        transformed_line = \"FreeTime\"\n",
    "    elif transformed_line == \"Professional&Technical\" or transformed_line == \"Computer&Technology\" or transformed_line == \"Law\" or transformed_line == \"Medicine\" or transformed_line == \"Business&Economics\" or transformed_line == \"ForeignLanguageStudy\" or transformed_line == \"Education&Teaching\":\n",
    "        transformed_line = \"Professional&Technical\"\n",
    "    elif transformed_line == \"Health-Mind&Body\" or transformed_line == \"Religion&Spirituality\":\n",
    "        transformed_line = \"Health-Mind&Body\"\n",
    "    elif transformed_line == \"Family-Sex&Relationships\" or transformed_line == \"Gay&Lesbian\":\n",
    "        transformed_line = \"Family-Sex&Relationships\"\n",
    "    elif transformed_line == \"Science&Nature\" or transformed_line == \"Pets\":\n",
    "        transformed_line = \"Science&Nature\"\n",
    "    elif transformed_line == \"Children\" or transformed_line == \"Teens\":\n",
    "        transformed_line = \"Children&Teens\"\n",
    "        \n",
    "    return Row(id=line.id, familyid=line.familyid, itemid=line.itemid, categoryid=line.categoryid, slug=line.slug, name=transformed_line, languageid=line.languageid, votes = line.votes)\n",
    "\n",
    "#NOTA: slug è rimasto invariato nel caso servisse il genere originale da cui è stata fatta l'aggregazione (nel dubbio)\n",
    "\n",
    "RDDgenres_mapped = RDDgenres.map(mapGenres)\n",
    "DFgenres = RDDgenres_mapped.toDF([])\n",
    "\n",
    "#Fase 2.2: aggreghiamo i nuovi generi duplicati sommando i voti (partitionBy)\n",
    "\n",
    "DFgenres = DFgenres.select(\"*\", F.sum(DFgenres.votes).over(Window.partitionBy(DFgenres.itemid, DFgenres.name)).alias(\"new_votes\")).drop(DFgenres.votes)\n",
    "DFgenres = DFgenres.withColumnRenamed(\"new_votes\", \"votes\").dropDuplicates([\"itemid\", \"name\"])\n",
    "\n",
    "#DFgenres.filter(DFgenres.itemid == \"10000108\").show(3, False)\n",
    "\n",
    "#Fase 3: nel caso di libri con più di 3 generi, tagliare facendo rimanere solo i primi 3 generi per voti\n",
    "DFgenres = DFgenres.withColumn(\"rank\", row_number().over(Window.partitionBy(DFgenres.itemid).orderBy(col(\"votes\").desc()))).filter(col(\"rank\") <= 3)\n",
    "#DFgenres.show(20, False)\n",
    "\n",
    "#Fase 4: aggregazione dei libri con più genere sotto un'unica tupla contenente la lista dei suoi generi \n",
    "#(segnarsi i voti di ogni genere se servisse). Torniamo agli RDD\n",
    "\n",
    "DFgenres = DFgenres.select(concat_ws(\"/\", DFgenres.name, DFgenres.votes).alias(\"genre_with_votes\"), \"id\", \"familyid\", \"itemid\", \"categoryid\", \"languageid\", \"slug\")\n",
    "#DFgenres.show(5000, False)\n",
    "\n",
    "RDDgenres = DFgenres.rdd\n",
    "RDDgenres_pair = RDDgenres.map(lambda x: (x.itemid, x))\n",
    "\n",
    "def aggregateGenres(line1, line2):\n",
    "    return Row(id=line1.id, familyid=line1.familyid, itemid=line1.itemid, categoryid=line1.categoryid, slug=line1.slug, languageid=line1.languageid, genre_with_votes = line1.genre_with_votes + \" \" + line2.genre_with_votes)\n",
    "\n",
    "RDDgenres_reduced = RDDgenres_pair.reduceByKey(aggregateGenres)\n",
    "\n",
    "def toSingleGenres(line1):\n",
    "    return Row(id=line1[1].id, familyid=line1[1].familyid, itemid=line1[1].itemid, categoryid=line1[1].categoryid, slug=line1[1].slug, languageid=line1[1].languageid, genre_with_votes = line1[1].genre_with_votes)\n",
    "\n",
    "RDDgenres_single = RDDgenres_reduced.map(toSingleGenres)\n",
    "\n",
    "def toDictGenre(line1):\n",
    "    dict_genres = {}\n",
    "    genre_vote_list = line1.genre_with_votes.split(\" \")\n",
    "    for genre_vote in genre_vote_list:\n",
    "        genre = genre_vote.split(\"/\")[0]\n",
    "        vote = genre_vote.split(\"/\")[1]\n",
    "        dict_genres[genre] = vote\n",
    "\n",
    "    return Row(id=line1.id, familyid=line1.familyid, itemid=line1.itemid, categoryid=line1.categoryid, slug=line1.slug, languageid=line1.languageid, genre_with_votes = dict_genres)\n",
    "\n",
    "RDDgenres_single = RDDgenres_single.map(toDictGenre)\n",
    "DFgenres = RDDgenres_single.toDF([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c6d667e-e4b6-4c77-a8f7-498ba15e57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Concateniamo i contenuti delle descrizioni con quelli dei commenti##\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Filtriamo le recensioni con almeno... 100 parole? Da valutare\n",
    "DFreviews = DFreviews.filter(F.size(F.split('comment_content', ' ')) >= 120)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c7698a2-093c-45f2-beb3-801eed6ba353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFfilteredmagazineitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 4: libri con più di X votes. Per vedere i voti di ogni libro dovrei usare il dataframe link_person_item.csv.\n",
    "#Come possiamo notare, alcune recensione hanno valore 0: questo non vuol dire che il libro è stato valutato\n",
    "#atrocemente ma che l'utente ha letto il libro senza inserire voti.\n",
    "\n",
    "#Dobbiamo dunque scremare tali voti da questo dataframe.\n",
    "\n",
    "# file = \"/data/SMARTDATA/books/anobii/link_person_item.csv\" #MOSTRA IL VOTO DI UN UTENTE A UN LIBRO\n",
    "# DFstars = spark.read.csv(file,header=True)\n",
    "#DFstars.show(50, False)\n",
    "\n",
    "DFstarsfilteredno0 = DFinfo.filter(DFinfo.item_review > 0)\n",
    "#DFstarsfilteredno0.show(20,False)\n",
    "\n",
    "#Contiamo i libri con più voti e visualizziamoli in ordine discendente\n",
    "\n",
    "DFgrouped = DFstarsfilteredno0.groupby(\"item_id\").count().withColumnRenamed(\"count\", \"total_votes\")\\\n",
    "                              .sort(\"total_votes\", ascending=False)\n",
    "#I voti dei libri più votati si aggirano intorno ai 10^4 come ordine di grandezza per cui per ora scelgo 300 come\n",
    "#soglia di voti necessaria a rimanere nel dataframe (soggetto a cambiamenti)\n",
    "DFgroupedfiltered = DFgrouped.filter(DFgrouped['total_votes'] > 100) #NUOVA SOGLIA: 150 PER 2500 LIBRI ANOBII\n",
    "DFjoinbookstars = DFfilteredmagazineitems.join(DFgroupedfiltered, DFfilteredmagazineitems.item_id == DFgroupedfiltered.item_id).drop(DFgroupedfiltered.item_id)\n",
    "# DFjoinbookstars.show(1, False)\n",
    "# DFjoinbookstars:\n",
    "# +-------+----------+-----------------+---------+----------+----------------+---------+---------+-------+----------------+------------+------------+------------------+--------+--------------+-----------+\n",
    "# |item_id|isbn      |title            |sub_title|no_of_page|publication_date|publisher|binding  |edition|average_rating  |total_review|product_type|encrypt_item_id   |language|total_wishlist|total_votes|\n",
    "# +-------+----------+-----------------+---------+----------+----------------+---------+---------+-------+----------------+------------+------------+------------------+--------+--------------+-----------+\n",
    "# |1981748|8845260372|Giulietta squeenz|null     |209       |2008-05-01      |Bompiani |Paperback|null   |3.18773234200743|129         |1           |014f24ec9629744c88|11      |110           |543        |\n",
    "# +-------+----------+-----------------+---------+----------+----------------+---------+---------+-------+----------------+------------+------------+------------------+--------+--------------+-----------+\n",
    "\n",
    "#FILTRAGGIO 5: Pulizie varie ed eventuali\n",
    "#DFfilteredlessthan10items.filter(DFfilteredlessthan10items.total_review > 100).show()\n",
    "\n",
    "#NOTA: prendo i titoli con le minuscole per facilità di merging con le biblioteche\n",
    "DFmanifestations_definitive_anobii = DFjoinbookstars.select(\"item_id\", \\\n",
    "                                                            lower(DFjoinbookstars.title), \\\n",
    "                                                            lower(DFjoinbookstars.sub_title), \\\n",
    "                                                            \"isbn\",\\\n",
    "                                                            \"average_rating\", \\\n",
    "                                                            \"total_review\", \\\n",
    "                                                            \"total_wishlist\", \\\n",
    "                                                            \"no_of_page\",\n",
    "                                                            \"publication_date\",\n",
    "                                                            \"publisher\",\n",
    "                                                            \"binding\",\n",
    "                                                            \"edition\",\n",
    "                                                            \"product_type\",\n",
    "                                                            \"total_votes\",\n",
    "                                                            \"encrypt_item_id\",)\\\n",
    "                                                    .withColumnRenamed(\"lower(title)\", \"title\")\\\n",
    "                                                    .withColumnRenamed(\"lower(sub_title)\", \"sub_title\")\n",
    "# DFmanifestations_definitive_anobii.show(1, False)\n",
    "# DFmanifestations_definitive_anobii: (uguale a DFjoinbookstars ma senza language)\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+\n",
    "# |item_id|title            |sub_title|isbn      |average_rating  |total_review|total_wishlist|no_of_page|publication_date|publisher|binding  |edition|product_type|total_votes|encrypt_item_id   |\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+\n",
    "# |1981748|giulietta squeenz|null     |8845260372|3.18773234200743|129         |110           |209       |2008-05-01      |Bompiani |Paperback|null   |1           |543        |014f24ec9629744c88|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+\n",
    "\n",
    "print(\"Anobii data have been filtered (select books in italian, drop comics and books with a low number of ratings)\")\n",
    "print(\"Integrete with genre and author attributes Anobii data\")\n",
    "#add genres to the item table: this is contained in DFgenres:\n",
    "# +---+--------+-------+----------+------------------+------------------+----------+-----+\n",
    "# |id |familyid|itemid |categoryid|slug              |name              |languageid|votes|\n",
    "# +---+--------+-------+----------+------------------+------------------+----------+-----+\n",
    "# |1  |32423317|2823074|3         |business-economics|Business&Economics|3         |2.0  |\n",
    "# +---+--------+-------+----------+------------------+------------------+----------+-----+\n",
    "# Notice that if a book has more than 1 genre, DFgenres contained more than one row with that item_id, one for genre.\n",
    "# Aggregation of rows with same itemid is needed \n",
    "#DFgenres_aggregated = DFgenres.groupBy('itemid')\\\n",
    "#                              .agg(F.concat_ws(\" / \", F.collect_list('name'))\\\n",
    "#                                    .alias('genres'))\n",
    "# DFgenres_aggregated:\n",
    "# +-------+----------------------------------------------------------------------------------------------+\n",
    "# |itemid |genres                                                                                        |\n",
    "# +-------+----------------------------------------------------------------------------------------------+\n",
    "# |1981748|Humor / Fiction&Literature / Romance / Family-Sex&Relationships / Teens / Philosophy / History|\n",
    "# +-------+----------------------------------------------------------------------------------------------+\n",
    "\n",
    "# join between items and DFgenres_aggregated to have, for each item, its genres\n",
    "DFmanifestations_definitive_anobii_genres = DFmanifestations_definitive_anobii.join(DFgenres,\\\n",
    "                                                                                    DFgenres.itemid == DFmanifestations_definitive_anobii.item_id)\\\n",
    "                                                                              .select(DFmanifestations_definitive_anobii[\"*\"],\\\n",
    "                                                                                      DFgenres.genre_with_votes) \n",
    "\n",
    "# DFmanifestations_definitive_anobii_genres:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+\n",
    "# |item_id|title            |sub_title|isbn      |average_rating  |total_review|total_wishlist|no_of_page|publication_date|publisher|binding  |edition|product_type|total_votes|encrypt_item_id   |genres                                                                                        |\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+\n",
    "# |1981748|giulietta squeenz|null     |8845260372|3.18773234200743|129         |110           |209       |2008-05-01      |Bompiani |Paperback|null   |1           |543        |014f24ec9629744c88|History / Fiction&Literature / Family-Sex&Relationships / Teens / Humor / Romance / Philosophy|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+\n",
    "# add a column which explain the origin of the data (anobii or BCT)\n",
    "DFmanifestations_definitive_anobii_genres = DFmanifestations_definitive_anobii_genres.withColumn('data_type', lit(\"anobii\"))\n",
    "# DFmanifestations_definitive_anobii_genres.show(1, False)\n",
    "# DFmanifestations_definitive_anobii_genres:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+---------+\n",
    "# |item_id|title            |sub_title|isbn      |average_rating  |total_review|total_wishlist|no_of_page|publication_date|publisher|binding  |edition|product_type|total_votes|encrypt_item_id   |genres                                                                                        |data_type|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+---------+\n",
    "# |1981748|giulietta squeenz|null     |8845260372|3.18773234200743|129         |110           |209       |2008-05-01      |Bompiani |Paperback|null   |1           |543        |014f24ec9629744c88|Family-Sex&Relationships / Teens / Humor / Romance / Philosophy / History / Fiction&Literature|anobii   |\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+----------------------------------------------------------------------------------------------+---------+\n",
    "# add author in the DFmanifestations_definitive_anobii_genres table\n",
    "# find author_id for each book\n",
    "DFmanifestations_anobii_genres_author = DFmanifestations_definitive_anobii_genres.join(DFauthorbooks, \\\n",
    "                                                                                       DFauthorbooks.item_id == DFmanifestations_definitive_anobii_genres.item_id)\\\n",
    "                                                                                 .select(DFmanifestations_definitive_anobii_genres[\"*\"], \\\n",
    "                                                                                         DFauthorbooks[\"author_id\"])\n",
    "# DFmanifestations_anobii_genres_author.show(1, False)\n",
    "# DFmanifestations_anobii_genres_author:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+---------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|author_id|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+---------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Romance / Humor /...|   anobii|   354644|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+---------+\n",
    "# find author (name) for each book (and remove books which has same title but different auhtor?)\n",
    "DFmanifestations_anobii_genres_author = DFmanifestations_anobii_genres_author.join(DFdisplay,\\\n",
    "                                                                                   DFmanifestations_anobii_genres_author.author_id == DFdisplay.author_id)\\\n",
    "                                                                             .select(DFmanifestations_anobii_genres_author[\"*\"], \\\n",
    "                                                                                     DFdisplay[\"author_name\"])\\\n",
    "                                                                             .drop(\"author_id\")\n",
    "\n",
    "# remove books which has same title but different auhtor\n",
    "DFmanifestations_anobii_genres_author = DFmanifestations_anobii_genres_author.select(\"*\", F.min(DFmanifestations_anobii_genres_author.author_name)\\\n",
    "                                                                                           .over(Window.partitionBy(DFmanifestations_anobii_genres_author.item_id))\\\n",
    "                                                                                           .alias(\"author\"))\\\n",
    "                                                                             .drop(\"author_name\")\\\n",
    "                                                                             .dropDuplicates(['item_id', 'author'])\n",
    "# DFmanifestations_anobii_genres_author.show(1, False)\n",
    "# DFmanifestations_anobii_genres_author:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|    author|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Philosophy / Hist...|   anobii|Pulsatilla|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+\n",
    "#join of item and rating tables to obtain person_id, item_id, rating and all attributes of the item in the same row\n",
    "#df1.join(df2, df1.id == df2.id).select(df1[\"*\"],df2[\"other\"])\n",
    "DFloans_definitive_anobii_genres = DFstarsfilteredno0.join(DFmanifestations_anobii_genres_author, \\\n",
    "                                                           DFmanifestations_anobii_genres_author.item_id == DFstarsfilteredno0.item_id)\\\n",
    "                                                     .select(DFmanifestations_anobii_genres_author[\"*\"], \\\n",
    "                                                             DFstarsfilteredno0[\"person_id\"], \\\n",
    "                                                             DFstarsfilteredno0[\"item_review\"])\n",
    "# DFloans_definitive_anobii_genres.show(1, False)\n",
    "# DFloans_definitive_anobii_genres:\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|    author|person_id|item_review|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Philosophy / Fict...|   anobii|Pulsatilla|  1244593|          3|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "\n",
    "print(\"Anobii data have been integreted with genre and author attributes\")\n",
    "print(\"Read and filter BCT data\")\n",
    "%run cleanDataBCTwithIBSN.ipynb #Così a quanto pare\n",
    "DFmanifestations_definitive_bct = DFmanifestations_definitive\n",
    "DFloans_definitive_bct = DFloans_definitive\n",
    "# DFmanifestations_definitive_bct.show(1, False)\n",
    "# DFmanifestations_definitive_bct:\n",
    "# +----------------+------------+---------+-------------+---------+--------------------+------------+\n",
    "# |edition_language|edition_date|    title|       author|publisher|manifestation_id_new|ISBNISSN_new|\n",
    "# +----------------+------------+---------+-------------+---------+--------------------+------------+\n",
    "# |             ita|        1996|Pinocchio|Carlo Collodi|   Nuages|              107930|  8807820714|\n",
    "# +----------------+------------+---------+-------------+---------+--------------------+------------+\n",
    "# DFloans_definitive_bct:\n",
    "# +--------------------+--------------------+-------------------+-------------------+-------------------+------------+----------+-----------+\n",
    "# |manifestation_id_new|       patron_id_md5|    loan_date_begin|      loan_date_end|           due_date|from_library|to_library|end_library|\n",
    "# +--------------------+--------------------+-------------------+-------------------+-------------------+------------+----------+-----------+\n",
    "# |              220771|fa242f1458100dccc...|2012-09-05 11:27:25|2012-09-27 18:18:00|2012-10-05 11:27:21|          18|        18|         18|\n",
    "# +--------------------+--------------------+-------------------+-------------------+-------------------+------------+----------+-----------+\n",
    "print(\"BCT data have been read and filtered\")\n",
    "print(\"Prepare data for merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56bb0855-bc1c-4f24-866a-36b6169dace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data have been prepared for merge\n",
      "Merge data\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import MapType\n",
    "\n",
    "# BCT data preparation: prepare the table of loans which contains also metadata of the book\n",
    "DFloans_with_titles = DFloans_definitive_bct.join(DFmanifestations_definitive_bct, \\\n",
    "                                                  DFloans_definitive_bct.manifestation_id_new == DFmanifestations_definitive_bct.manifestation_id_new)\\\n",
    "                                             .select(DFloans_definitive_bct.manifestation_id_new, \\\n",
    "                                                     \"patron_id_md5\", \\\n",
    "                                                     \"author\", \\\n",
    "                                                     \"ISBNISSN_new\", \\\n",
    "                                                     lower(DFmanifestations_definitive_bct.title))\\\n",
    "                                             .withColumnRenamed(\"lower(title)\", \"title\")\n",
    "# DFloans_with_titles:\n",
    "# +--------------------+--------------------+-------------+------------+---------+\n",
    "# |manifestation_id_new|       patron_id_md5|       author|ISBNISSN_new|    title|\n",
    "# +--------------------+--------------------+-------------+------------+---------+\n",
    "# |              107930|b5c0986c79b1afafd...|Carlo Collodi|  8807820714|pinocchio|\n",
    "# +--------------------+--------------------+-------------+------------+---------+\n",
    "\n",
    "# BCT data preparation: split title and subtitle\n",
    "RDDloans_with_titles = DFloans_with_titles.rdd    \n",
    "RDDloans_for_merge = RDDloans_with_titles.map(title_and_subtitle)  \n",
    "# DFloans_with_titles.show(1, False)\n",
    "DFloans_to_merge = RDDloans_for_merge.toDF([\"item_id\", \\\n",
    "                                            \"person_id\", \\\n",
    "                                            \"title\", \\\n",
    "                                            \"author\", \\\n",
    "                                            \"sub_title\", \\\n",
    "                                            \"item_review\", \\\n",
    "                                            \"total_review\", \\\n",
    "                                            \"average_rating\", \\\n",
    "                                            \"total_votes\", \\\n",
    "                                            \"isbn\", \\\n",
    "                                            \"data_type\"])\n",
    "# BCT data preparation: add missing columns, which are present in DFloans_definitive_anobii_genres\n",
    "columns_to_add = ['total_wishlist',\n",
    "                  'no_of_page',\n",
    "                  'publication_date',\n",
    "                  'publisher',\n",
    "                  'binding',\n",
    "                  'edition',\n",
    "                  'product_type',\n",
    "                  'encrypt_item_id',\n",
    "                  'genre_with_votes',\n",
    "                 ]\n",
    "values_to_add = {'total_wishlist': None,\n",
    "                  'no_of_page': None,\n",
    "                  'publication_date': None, \n",
    "                  'publisher': None, \n",
    "                  'binding': \"Paperback\",\n",
    "                  'edition': None,\n",
    "                  'product_type': 1,\n",
    "                  'encrypt_item_id': None,\n",
    "                  'genre_with_votes': None,\n",
    "    \n",
    "}\n",
    "for c in columns_to_add:\n",
    "        DFloans_to_merge = DFloans_to_merge.withColumn(c, lit(values_to_add[c]))\n",
    "\n",
    "DFloans_to_merge = DFloans_to_merge.select(\"item_id\", \"title\", \"sub_title\", \"isbn\", \"average_rating\", \"total_review\", \"total_wishlist\", \"no_of_page\", \"publication_date\", \"publisher\", \"binding\", \"edition\", \"product_type\", \"total_votes\", \"encrypt_item_id\", \"genre_with_votes\", \"data_type\", \"author\", \"person_id\", \"item_review\")\n",
    "\n",
    "#DFloans_to_merge = DFloans_to_merge.withColumn(\"genre_with_votes\",DFloans_to_merge.genre_with_votes.cast(MapType(StringType(), StringType())))\n",
    "#DFloans_definitive_anobii_genres.printSchema()\n",
    "#DFloans_to_merge.printSchema()\n",
    "\n",
    "# DFloans_to_merge:\n",
    "# +-------+--------------------+---------+-----------+-------------+-----------+------------+--------------+-----------+----------+-----------+--------------+----------+----------------+---------+---------+-------+------------+---------------+-----+\n",
    "# |item_id|           person_id|    title|author_name|    sub_title|item_review|total_review|average_rating|total_votes|      isbn|  data_type|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|encrypt_item_id|genre|\n",
    "# +-------+--------------------+---------+-----------+-------------+-----------+------------+--------------+-----------+----------+-----------+--------------+----------+----------------+---------+---------+-------+------------+---------------+-----+\n",
    "# | 107930|fcc17388ef20567fe...|pinocchio|       None|Carlo Collodi|          4|        None|          None|       None|8807820714|biblioteche|          null|      null|            null|     null|Paperback|   null|           1|           null| null|\n",
    "# +-------+--------------------+---------+-----------+-------------+-----------+------------+--------------+-----------+----------+-----------+--------------+----------+----------------+---------+---------+-------+------------+---------------+-----+\n",
    "print(\"Data have been prepared for merge\")\n",
    "print(\"Merge data\")\n",
    "# union of the loans of bct data and anobii data\n",
    "DFloans_merged = DFloans_definitive_anobii_genres.union(DFloans_to_merge)\n",
    "# DFloans_merged.show(1)\n",
    "# DFloans_merged\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |item_id|            title|sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|    author|person_id|item_review|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# |1981748|giulietta squeenz|     null|8845260372|3.18773234200743|         129|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|014f24ec9629744c88|Family-Sex&Relati...|   anobii|Pulsatilla|   734393|          5|\n",
    "# +-------+-----------------+---------+----------+----------------+------------+--------------+----------+----------------+---------+---------+-------+------------+-----------+------------------+--------------------+---------+----------+---------+-----------+\n",
    "# Remove books, whose title is one word only, i.e. keep books whose title contains at least a blank space\n",
    "# DFloans_merged.filter(DFloans_merged.title == \"pinocchio\").show()\n",
    "DFloans_merged_nosinglewordtitles = DFloans_merged.filter(DFloans_merged.title.contains(\" \")) \n",
    "# DFloans_merged_nosinglewordtitles.filter(DFloans_merged_nosinglewordtitles.title == \"pinocchio\").show()\n",
    "\n",
    "# Assign to books with the same title, the same book_id\n",
    "DFloans_merged_aggregated = DFloans_merged_nosinglewordtitles.select(\"*\", \\\n",
    "                                                                     F.first(DFloans_merged_nosinglewordtitles.item_id)\\\n",
    "                                                                      .over(Window.partitionBy(DFloans_merged_nosinglewordtitles.title).orderBy(DFloans_merged_nosinglewordtitles.data_type))\\\n",
    "                                                                      .alias(\"book_id\"))\\\n",
    "                                                              .drop(\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c2dded4-31c9-4606-a5fd-f6ca5cabf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFloans_merged_aggregated.show(2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fb67af8-d8d3-4a4d-a132-8320bab1d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been merged\n",
      "Write data on csv files\n"
     ]
    }
   ],
   "source": [
    "# DFloans_merged_aggregated.show(1)\n",
    "# DFloans_merged_aggregated:\n",
    "# +--------------------+-------------------+----------+----------------+------------+--------------+----------+----------------+-------------------+---------+-------+------------+-----------+------------------+--------------------+---------+----------------+---------+-----------+-------+\n",
    "# |               title|          sub_title|      isbn|  average_rating|total_review|total_wishlist|no_of_page|publication_date|          publisher|  binding|edition|product_type|total_votes|   encrypt_item_id|              genres|data_type|          author|person_id|item_review|book_id|\n",
    "# +--------------------+-------------------+----------+----------------+------------+--------------+----------+----------------+-------------------+---------+-------+------------+-----------+------------------+--------------------+---------+----------------+---------+-----------+-------+\n",
    "# |come mi batte for...|storia di mio padre|8806198882|4.14122681883024|         206|           270|       302|      2009-11-03|Einaudi (Frontiere)|Hardcover|      1|           1|        705|01500978c278d06718|Crime / Teens / N...|   anobii|Benedetta Tobagi|   767565|          3|2806310|\n",
    "# +--------------------+-------------------+----------+----------------+------------+--------------+----------+----------------+-------------------+---------+-------+------------+-----------+------------------+--------------------+---------+----------------+---------+-----------+-------+\n",
    "\n",
    "# Assign to books with the same title, the same new_author\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.min(DFloans_merged_aggregated.author)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_author\"))\n",
    "\n",
    "# Assign to books with the same title, the same new_genre\n",
    "#DFloans_merged_aggregated.show(1, False)\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.first(DFloans_merged_aggregated.genre_with_votes)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id).orderBy(DFloans_merged_aggregated.data_type))\\\n",
    "                                                              .alias(\"new_genre\"))\n",
    "\n",
    "# Assign to books with the same title, the same new_encrypt\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.min(DFloans_merged_aggregated.encrypt_item_id)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_encrypt_item_id\"))\n",
    "\n",
    "# Assign to books with the same title, the same new_isbn\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.min(DFloans_merged_aggregated.isbn)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_isbn\"))\n",
    "\n",
    "# Update metrics of loans: new_total_count (to count the number of votes or loans) \n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.count(\"*\")\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_total_count\"))\n",
    "\n",
    "# Update metrics of loans: new_average_rating \n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.avg(DFloans_merged_aggregated.item_review)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_average_rating\"))\n",
    "\n",
    "# Update metrics of loans: new_total_review, to count the actual number of review for a book (only anobii provides real review!!) \n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.select(\"*\", \\\n",
    "                                                             F.max(DFloans_merged_aggregated.total_review)\\\n",
    "                                                              .over(Window.partitionBy(DFloans_merged_aggregated.book_id))\\\n",
    "                                                              .alias(\"new_total_review\"))\n",
    "\n",
    "# Remove \"old\" attributes\n",
    "columnsToDrop = ['author',\n",
    "                 'genre_with_votes',\n",
    "                 'encrypt_item_id',\n",
    "                 'isbn',\n",
    "                 'total_count',\n",
    "                 'average_rating',\n",
    "                 'total_review',]\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.drop(*columnsToDrop)\n",
    "# DFloans_merged_aggregated.show(1)\n",
    "# DFloans_merged_aggregated:\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+-------------------+----------+---------------+------------------+----------------+\n",
    "# |            title|sub_title|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|data_type|person_id|item_review|book_id|new_author|           new_genre|new_encrypt_item_id|  new_isbn|new_total_count|new_average_rating|new_total_review|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+-------------------+----------+---------------+------------------+----------------+\n",
    "# |giulietta squeenz|     null|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|   anobii|   959028|          3|1981748|Pulsatilla|Philosophy / Hist...| 014f24ec9629744c88|8845260372|            543|3.1860036832412524|             129|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+-------------------+----------+---------------+------------------+----------------+\n",
    "\n",
    "# Rename \"new\" attributes\n",
    "columnsToRenames = ['new_author',\n",
    "                    'new_genre',\n",
    "                    'new_encrypt_item_id',\n",
    "                    'new_isbn',\n",
    "                    'new_total_count',\n",
    "                    'new_average_rating',\n",
    "                    'new_total_review',]\n",
    "for c in columnsToRenames:\n",
    "    DFloans_merged_aggregated = DFloans_merged_aggregated.withColumnRenamed(c, c.replace(\"new_\", \"\"))\n",
    "# DFloans_merged_aggregated.show(1)\n",
    "# DFloans_merged_aggregated:\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+------------------+----------+-----------+------------------+------------+\n",
    "# |            title|sub_title|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|data_type|person_id|item_review|book_id|    author|               genre|   encrypt_item_id|      isbn|total_count|    average_rating|total_review|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+------------------+----------+-----------+------------------+------------+\n",
    "# |giulietta squeenz|     null|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|   anobii|   273238|          4|1981748|Pulsatilla|Humor / Philosoph...|014f24ec9629744c88|8845260372|        543|3.1860036832412524|         129|\n",
    "# +-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+---------+-----------+-------+----------+--------------------+------------------+----------+-----------+------------------+------------+\n",
    "\n",
    "# Remove ['book_id', 'person_id'] duplicates\n",
    "DFloans_merged_aggregated = DFloans_merged_aggregated.dropDuplicates(['book_id', 'person_id'])\n",
    "print(\"Data has been merged\")\n",
    "# before write data on file, create index for both users and books of the sparce matrix\n",
    "RDDmerged_almost_definitive = DFloans_merged_aggregated.rdd\n",
    "\n",
    "# functions which creates the dictionary where the key is the book_id/person_id and the value is the corresponding index\n",
    "book_dictionary = create_book_dictionary(RDDmerged_almost_definitive)\n",
    "user_dictionary = create_user_dictionary(RDDmerged_almost_definitive)\n",
    "\n",
    "# add the user_index and the book_index in the dataframe\n",
    "RDDdefinitive = RDDmerged_almost_definitive.map(addRowColumnId)\n",
    "DFloans_merged_aggregated = RDDdefinitive.toDF(['book_id',\n",
    "                                                'title',\n",
    "                                                'sub_title',\n",
    "                                                'total_wishlist',\n",
    "                                                'no_of_page',\n",
    "                                                'publication_date',\n",
    "                                                'publisher',\n",
    "                                                'binding',\n",
    "                                                'edition',\n",
    "                                                'product_type',\n",
    "                                                'total_votes',\n",
    "                                                'data_type',\n",
    "                                                'person_id',\n",
    "                                                'item_review',\n",
    "                                                'author',\n",
    "                                                'genre',\n",
    "                                                'encrypt_item_id',\n",
    "                                                'isbn',\n",
    "                                                'total_count',\n",
    "                                                'average_rating',\n",
    "                                                'total_review', \n",
    "                                                'user_index', \n",
    "                                                'book_index'], sampleRatio=0.9)\n",
    "print(\"Write data on csv files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8628b9d5-2a66-4ee4-a357-f26b4bf343b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFloans_merged_aggregated.show(2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441c8362-0f52-48ec-8d46-5654c9817979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5987892\n",
      "There are 5987892 loans\n"
     ]
    }
   ],
   "source": [
    "ratings = DFloans_merged_aggregated.filter(~(DFloans_merged_aggregated.genre.isNull()))#DFdefinitive\n",
    "print(ratings.count())\n",
    "# in the rating table, each row is given by \"person_id\", \"book_id\", \"rating\" attributes\n",
    "ratings_tocsv = ratings.select([\"person_id\", \"book_id\", \"item_review\"])\\\n",
    "                       .withColumnRenamed(\"item_review\", \"rating\")\n",
    "# ratings_tocsv.show(1)\n",
    "# ratings_tocsv:\n",
    "# +---------+-------+------+\n",
    "# |person_id|book_id|rating|\n",
    "# +---------+-------+------+\n",
    "# |   100489|1981748|     3|\n",
    "# +---------+-------+------+\n",
    "print(\"There are \"+str(ratings_tocsv.count())+\" loans\")\n",
    "ratings_tocsv.toPandas().to_csv(\"ratings2021_extended.csv\")\n",
    "\n",
    "# in the book table, the metadata about the book are reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad46c0d5-089d-4b62-b65e-72f55ab426da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9198 books\n",
      "3582\n",
      "3582\n",
      "3582\n",
      "3582\n",
      "There are 231646 users\n",
      "Csv files have been written\n"
     ]
    }
   ],
   "source": [
    "# +-------+-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+----------+--------------------+------------------+----------+-----------+------------------+------------+----------+\n",
    "# |book_id|            title|sub_title|total_wishlist|no_of_page|publication_date|publisher|  binding|edition|product_type|total_votes|data_type|    author|               genre|   encrypt_item_id|      isbn|total_count|    average_rating|total_review|book_index|\n",
    "# +-------+-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+----------+--------------------+------------------+----------+-----------+------------------+------------+----------+\n",
    "# |1981748|giulietta squeenz|     null|           110|       209|      2008-05-01| Bompiani|Paperback|   null|           1|        543|   anobii|Pulsatilla|Family-Sex&Relati...|014f24ec9629744c88|8845260372|        543|3.1860036832412524|         129|       319|\n",
    "# +-------+-----------------+---------+--------------+----------+----------------+---------+---------+-------+------------+-----------+---------+----------+--------------------+------------------+----------+-----------+------------------+------------+----------+\n",
    "\n",
    "\n",
    "books_tocsv = ratings.dropDuplicates(['book_id'])\\\n",
    "                     .drop(\"person_id\")\\\n",
    "                     .drop(\"user_index\")\\\n",
    "                     .drop(\"item_review\")\n",
    "# books_tocsv.show(1)\n",
    "# books_tocsv:\n",
    "print(\"There are \"+str(books_tocsv.count())+\" books\")\n",
    "books_tocsv = books_tocsv.join(DFdescriptions, DFdescriptions.item_id == books_tocsv.book_id).select(\"book_id\", \"title\", \"sub_title\", \"total_wishlist\", \"no_of_page\", \"publication_date\", \"publisher\", \"binding\", \"edition\", \"product_type\", \"total_votes\", \"data_type\", \"author\", \"genre\", \"encrypt_item_id\", \"isbn\", \"total_count\", \"average_rating\", \"total_review\", \"book_index\", \"content\")\n",
    "#books_tocsv.toPandas().to_csv(\"books2021_extended.csv\")\n",
    "#books_tocsv.show(1, False)\n",
    "books_tocsv = books_tocsv.withColumn(\"content2\", books_tocsv[\"content\"])\n",
    "print(books_tocsv.select(\"book_id\").distinct().count())\n",
    "\n",
    "RDDbooks_tocsv = books_tocsv.rdd\n",
    "RDDbooks_tocsv = RDDbooks_tocsv.map(lambda x: (x.book_id, x))\n",
    "RDDreducedbooks = RDDbooks_tocsv.reduceByKey(concatenateContentsDescriptions)\n",
    "RDDreducedbooks = RDDreducedbooks.map(toSingleDescriptions)\n",
    "books_tocsv = RDDreducedbooks.toDF([])\n",
    "print(books_tocsv.count())\n",
    "\n",
    "#Joiniamo DFreviews a books_tocsv (la cardinalità aumenterà ma con una funzione di riduzione con gli RDD dovremmo riportarla alla normalità)\n",
    "books_tocsv = books_tocsv.join(DFreviews, books_tocsv.book_id == DFreviews.item_id, \"left\").select(books_tocsv.book_id, \"title\", \"sub_title\", \"total_wishlist\", \"no_of_page\", \"publication_date\", \"publisher\", \"binding\", \"edition\", \"product_type\", \"total_votes\", \"data_type\", \"author\", \"genre\", \"encrypt_item_id\", \"isbn\", \"total_count\", \"average_rating\", \"total_review\", \"book_index\", \"content\", \"comment_content\", \"content2\")\n",
    "\n",
    "#Fatto ciò,  trasformiamo books_tocsv in un rdd per poter applicare la funzione di concatenazione\n",
    "print(books_tocsv.select(\"book_id\").distinct().count())\n",
    "\n",
    "RDDbooks = books_tocsv.rdd\n",
    "RDDbooks = RDDbooks.map(lambda x: (x.book_id, x))\n",
    "RDDreducedbooks = RDDbooks.reduceByKey(concatenateContentsComments)\n",
    "RDDreducedbooks = RDDreducedbooks.map(toSingleComments)\n",
    "RDDreducedbooks = RDDreducedbooks.map(mergeDescriptionComments)\n",
    "books_tocsv = RDDreducedbooks.toDF([])\n",
    "\n",
    "print(books_tocsv.count())\n",
    "\n",
    "# in the user table, the person_id is reported for each user (in each row)\n",
    "users_tocsv = ratings.select(\"person_id\", \"user_index\").dropDuplicates()\n",
    "# users_tocsv.show(1)\n",
    "# users_tocsv:\n",
    "# +---------+----------+\n",
    "# |person_id|user_index|\n",
    "# +---------+----------+\n",
    "# |  1224803|     10623|\n",
    "# +---------+----------+\n",
    "print(\"There are \"+str(users_tocsv.count())+\" users\")\n",
    "users_tocsv.toPandas().to_csv(\"users2021_extended.csv\")\n",
    "\n",
    "print(\"Csv files have been written\")\n",
    "#books_tocsv.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "331cac95-7e05-465e-86e0-d394704ffd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ESTRAZIONE KEYWORD###\n",
    "\n",
    "#Usiamo una versione filtrata di books_tocsv dove non compaiono tuple senza descrizione per attuare la nostra\n",
    "#estrazione.\n",
    "\n",
    "books_with_content = books_tocsv.filter(books_tocsv.content!=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a5661f3-7626-4625-883b-764ce1f109a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_with_content = books_with_content.filter(~(books_with_content.content.contains(\"Scheda INCOMPLETA\"))) #Filtra i libri con le schede incomplete\n",
    "books_with_content = books_with_content.filter(~(books_with_content.content.contains(\"SCHEDA DOPPIA\")))\n",
    "books_with_content = books_with_content.filter(~(books_with_content.content.contains(\"Questa scheda è DOPPIA.\")))\n",
    "books_with_content = books_with_content.filter(~(((books_with_content.content.contains(\"scheda\")) | (books_with_content.content.contains(\"Scheda\"))) & ((books_with_content.content.contains(\"incompleta\")) | (books_with_content.content.contains(\"doppia\")))))\n",
    "#books_with_content = books_with_content.dropDuplicates(['book_id'])\n",
    "\n",
    "#books_with_content.filter(books_with_content.content.contains(\"Esistt\")).show(1, False)\n",
    "\n",
    "#Per estrarre le keyword in maniera \"TF-IDF\" dobbiamo innanzitutto implementare una funzione che pulisca il testo \n",
    "#dalle stopwords (congiunzioni, rumori, etc.)\n",
    "\n",
    "def clean_text_light(doc_collection):\n",
    "    new_corpus_doc = []\n",
    "    for description in doc_collection:\n",
    "        description = description.replace(\"agrave\", \" \")\n",
    "        description = description.replace(\"egrave\", \" \")\n",
    "        description = description.replace(\"igrave\", \" \")\n",
    "        description = description.replace(\"ograve\", \" \")\n",
    "        description = description.replace(\"ograve\", \" \")\n",
    "        description = description.replace(\"<b>\", \" \")\n",
    "        description = description.replace(\",\", \" \")\n",
    "        description = description.replace(\"'\", \" \")\n",
    "        description = description.replace('\"', \" \")\n",
    "        description = description.replace(\"<b>\", \" \")\n",
    "        description = description.replace(\"<br />\", \" \")\n",
    "        description = description.replace(\"</b>\", \" \")\n",
    "        description = description.replace(\"<br>\", \" \")\n",
    "        description = description.replace(\"</br>\", \" \")\n",
    "        description = description.replace(\"<p>\", \" \")\n",
    "        description = description.replace(\"</p>\", \" \")\n",
    "        description = description.replace(\"<P>\", \" \")\n",
    "        description = description.replace(\"</P>\", \" \")\n",
    "        description = description.replace(\"<i>\", \" \")\n",
    "        description = description.replace(\"</i>\", \" \")\n",
    "        description = description.replace(\"&quot\", \" \")\n",
    "        description = description.replace(\"<strong>\", \" \")\n",
    "        description = description.replace(\"</strong>\", \" \")\n",
    "        description = description.replace(\"&\", \" \")  \n",
    "        #KEYWORD PRESENTI MA NON MOLTO UTILI\n",
    "        pattern = re.compile(re.escape(\"leggere\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettrice\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettore\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettori\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lettura\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"libro\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"libri\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mese \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mesi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"tempo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"tempi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"parte \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"parti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" cosa \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" cose \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" personaggio \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" personaggi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volume \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volumi\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" protagonista\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" protagonisti\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" testo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" testi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" pagina \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" pagine \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" secolo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" secoli \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" persona \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" persone \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"scrittore \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scrittrice \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scrittrici \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scrittori \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\" vic\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fine \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"nome \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"nomi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"inizio \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" parola \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" parole \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\" ista\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"modo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" numero \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" punto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" centro \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\"ora \"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" frattempo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mezzo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mezzi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" corso \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" situazione \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" piano \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" via \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" vie \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" forma \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" forme \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" posto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" posti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fronte \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" luogo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"autore \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" autori \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" autrici \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"autrice \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" grazie \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fatto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fatti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"edizione \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" edizioni \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" opere \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" serie \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tema \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" temi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" grado \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" gradi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" genere \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" generi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" titolo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" titoli \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"oggetto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" oggetti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tempo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tempi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" sfondo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" sfondi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" conto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" conti \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volta \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" volte \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\" race\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scena \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" scene \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"età \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" figura \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" figure \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"epoca \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" epoche\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"none\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"libreria\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"librerie\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"biblioteca\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"biblioteche\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"generazione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"generazioni\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mano \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" mani \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"aspetto \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"titolo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"produzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" senso \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" trama \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" trame \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"racconto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"racconti\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"capitolo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"capitoli\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" fondo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"domanda\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"risposta\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"domande\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"risposte\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"versione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"argomento\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"argomenti\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" tesi \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"italiano\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"effetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" carta \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"bisogno\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"bisogni\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"analisi\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"momento\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"milione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"disegno\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" dono \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"successo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"traduzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" penna \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"regola\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"regole\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"progetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"esempio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"narrazione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" data \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"pratica\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"frase\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"ultimo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"base\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"effetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"attenzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"motivo\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"essere\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"linguaggio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" lingua \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"lavoro\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"critica\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"aspetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"introduzione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"episodio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"livello\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"pubblicazione\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\" pezzo \"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"interesse\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"consiglio\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        pattern = re.compile(re.escape(\"effetto\"), re.IGNORECASE)\n",
    "        description = pattern.sub(' ', description)\n",
    "        #pattern = re.compile(re.escape(\"ore\"), re.IGNORECASE)\n",
    "        #description = pattern.sub(' ', description)\n",
    "       \n",
    "        new_corpus_doc.append(description)\n",
    "    return new_corpus_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a696c66-bdc4-4569-9fe5-193ede25cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFcontent_pandas = books_with_content.toPandas()\n",
    "\n",
    "corpus_doc = DFcontent_pandas['content'].to_list()\n",
    "#print(corpus_doc[0])\n",
    "cleaned_corpus_doc = clean_text_light(corpus_doc) #Pulizia rumori\n",
    "#print(cleaned_corpus_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "011ef596-4cab-431b-b89e-87fc392c0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###FILTRIAMO NOMI E AGGETTIVI DALLE DESCRIZIONI CON SPACY###\n",
    "##CELLA DI ESEMPIO PER L'UTILIZZO DI SPACY, UN MODULO PER CLASSIFICARE IL RUOLO DELLE PAROLE NELE FRASI\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\") #Caricamento file per il riconoscimento\n",
    "\n",
    "new_corpus_doc = []\n",
    "for document in cleaned_corpus_doc:\n",
    "    noun_tokens = []\n",
    "    doc = nlp(document)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" and token.is_oov==False: #Solo nomi presenti nel vocabolario italiano\n",
    "            noun_tokens.append(token.lemma_)\n",
    "    new_document = (\" \").join(noun_tokens)\n",
    "    new_corpus_doc.append(new_document)\n",
    "\n",
    "#print(new_corpus_doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab3d39e-cf20-4971-997f-66c0b3422797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programmando VERB advcl False programmare\n",
      "è AUX cop False essere\n",
      "proprio ADV advmod False proprio\n",
      "bello ADJ ROOT False bello\n"
     ]
    }
   ],
   "source": [
    "#CELLA DI PROVA PER SPACY\n",
    "\n",
    "#text = \"programmando è proprio bello\"\n",
    "#doc = nlp(text)\n",
    "#for token in doc:\n",
    "#    print(token.text, token.pos_, token.dep_, token.is_oov, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7351a15-ac55-4c05-8f9a-9d24f681ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "###FASE DI STEMMING### #SOSTITUITA A FASE DI LEMMATIZING, NON USARE\n",
    "\n",
    "#Fase usata per unire le parole chiave molto simili fra loro\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# import stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "#Crea uno stemmer italiano\n",
    "stemmer_snowball = SnowballStemmer('italian')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "stemmed_corpus_doc = []\n",
    "for descrizione in new_corpus_doc:\n",
    "    new_tokens_without_sw = []\n",
    "    text_token = word_tokenize(descrizione)\n",
    "    tokens_without_sw = [word for word in text_token if not word in stop_words]\n",
    "    \n",
    "    for word in tokens_without_sw:\n",
    "        stem_word = stemmer_snowball.stem(word)\n",
    "        new_tokens_without_sw.append(stem_word)\n",
    "    filtered_sentence = (\" \").join(new_tokens_without_sw)\n",
    "    \n",
    "    stemmed_corpus_doc.append(filtered_sentence)\n",
    "\n",
    "#print(stemmed_corpus_doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2dbdcbe-6b7e-41bc-9e09-d4f62f6525bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1448: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kimihiro Watanuki, liceale, possiede la capaci...</td>\n",
       "      <td>[negozio, desidero, strega, spirito, proprieta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dopo anni di duro apprendistato, nel 1976 Carv...</td>\n",
       "      <td>[normalità, marito, sensazione, fotografia, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Per il commissario Vittorio Spotorno, il dupli...</td>\n",
       "      <td>[commissario, gioco, vista, meta, quartiere, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parigi, giugno 1889. La città palpita, travolt...</td>\n",
       "      <td>[proprietario, onda, nipote, tecnica, città, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leona Dorn &amp;egrave; soddisfatta della sua vita...</td>\n",
       "      <td>[suicidio, casa, fiducia, incubo, giornalista,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>È il destino, ancora una volta, a dare le cart...</td>\n",
       "      <td>[notte, luce, moglie, fede, medico, matrimonio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>Lauren torna a Wisteria, Maryland, sette anni ...</td>\n",
       "      <td>[madre, incidente, zia, colpevole, morte, pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>Ciclo Dirk Pitt - Vol. 19Dirk Pitt resta quasi...</td>\n",
       "      <td>[avventura, ricerca, squadra, nave, tocco, ond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>Don Fabrizio, principe di Salina, all'arrivo d...</td>\n",
       "      <td>[felicità, pretesto, sopravvivenza, decina, af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>Passata la mezzanotte, il tempo e la realtà su...</td>\n",
       "      <td>[foto, gt, fantasma, incubo, bambino, fiaba, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3503 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     Kimihiro Watanuki, liceale, possiede la capaci...   \n",
       "1     Dopo anni di duro apprendistato, nel 1976 Carv...   \n",
       "2     Per il commissario Vittorio Spotorno, il dupli...   \n",
       "3     Parigi, giugno 1889. La città palpita, travolt...   \n",
       "4     Leona Dorn &egrave; soddisfatta della sua vita...   \n",
       "...                                                 ...   \n",
       "3527  È il destino, ancora una volta, a dare le cart...   \n",
       "3528  Lauren torna a Wisteria, Maryland, sette anni ...   \n",
       "3529  Ciclo Dirk Pitt - Vol. 19Dirk Pitt resta quasi...   \n",
       "3530  Don Fabrizio, principe di Salina, all'arrivo d...   \n",
       "3531  Passata la mezzanotte, il tempo e la realtà su...   \n",
       "\n",
       "                                           top_keywords  \n",
       "0     [negozio, desidero, strega, spirito, proprieta...  \n",
       "1     [normalità, marito, sensazione, fotografia, mo...  \n",
       "2     [commissario, gioco, vista, meta, quartiere, m...  \n",
       "3     [proprietario, onda, nipote, tecnica, città, m...  \n",
       "4     [suicidio, casa, fiducia, incubo, giornalista,...  \n",
       "...                                                 ...  \n",
       "3527  [notte, luce, moglie, fede, medico, matrimonio...  \n",
       "3528  [madre, incidente, zia, colpevole, morte, pres...  \n",
       "3529  [avventura, ricerca, squadra, nave, tocco, ond...  \n",
       "3530  [felicità, pretesto, sopravvivenza, decina, af...  \n",
       "3531  [foto, gt, fantasma, incubo, bambino, fiaba, t...  \n",
       "\n",
       "[3503 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importiamo il TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(smooth_idf=True, use_idf=True, max_df=0.35, min_df=80) #Di solito 0.60, 40\n",
    "\n",
    "vectorizer.fit_transform(new_corpus_doc) #CREAZIONE DIZIONARIO CON I VOCABOLI CONTENUTI NELLE DESCRIZIONI\n",
    "\n",
    "feature_names = vectorizer.get_feature_names() #Estrae le feature (i vocaboli) importanti\n",
    "\n",
    "def sort_coo(coo_matrix): #Funzione usata per riordinare il dizionario per score\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data) #COLONNE: FEATURES RIGHE: DESCRIZIONE\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=5): #Funzione usata per ottenere le n parole chiave\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "    \n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc): #Funzione usata per ritornare le top k parole chiave di una descrizione\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items, 20) #10 keyword per ora\n",
    "    \n",
    "    return list(keywords.keys())\n",
    "\n",
    "result = []\n",
    "for descrizione1, descrizione2 in zip(corpus_doc, new_corpus_doc):\n",
    "    df = {}\n",
    "    df['content'] = descrizione1\n",
    "    df['top_keywords'] = get_keywords(vectorizer, feature_names, descrizione2)\n",
    "    result.append(df)\n",
    "    \n",
    "DFkeywords = pd.DataFrame(result)\n",
    "DFkeywords = DFkeywords[DFkeywords['top_keywords'].str.len() > 0]\n",
    "DFkeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "59189b67-56f3-4716-a0b9-11e03e2a81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ora che abbiamo il dataframe di pandas con le keywords associate alle descrizioni, non ci resta che joinare\n",
    "#il contenuto di esso con il dataframe dei libri, integrando l'attributo keywords.\n",
    "#Riportiamo il dataframe nella forma PySpark\n",
    "\n",
    "DFkeywords = spark.createDataFrame(DFkeywords)\n",
    "\n",
    "DFkeywords = DFkeywords.dropDuplicates(['content'])\n",
    "DF_almost_final = books_with_content.join(DFkeywords, DFkeywords.content == books_with_content.content).drop(DFkeywords.content).select(\"book_id\", \"top_keywords\")\n",
    "\n",
    "#Adesso joiniamo per book_id al dataframe con più libri (compresi quelli senza descrizione)\n",
    "DF_final = books_with_content.join(DF_almost_final, DF_almost_final.book_id == books_with_content.book_id).drop(DF_almost_final.book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "211325a5-ff84-4f54-9a9c-be26d0487203",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = DF_final.filter(F.size(col(\"top_keywords\")) > 3) #Scartiamo i libri con poche info come parole chiave\n",
    "#print(DF_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8a4535b-e693-425b-b358-53a75c30212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proviamo a inserire nel dataframe il link per l'immagine della copertina seguendo lo schema del link di anobii:\n",
    "#https://media.anobii.com/covers/id/width/height/scale/treatment/format/version\n",
    "\n",
    "#Ovviamente, il link deve essere costruito solo per le tuple che hanno un \"encrypt item id\" altrimenti sarà null\n",
    "#Per ora metto il link con dei parametri \"safe\": immagine 250x250, formato png, no timestamp, scale 1, treatment = paperback\n",
    "\n",
    "#NOTA: In caso poi si possono cambiare i parametri\n",
    "\n",
    "def createImageUrl(line):\n",
    "    if line.encrypt_item_id == None: #Non c'è un item_encrypt_id\n",
    "        return line\n",
    "    else: #C'è l'id\n",
    "        image_url = f\"https://media.anobii.com/covers/{line.encrypt_item_id}/250/250/1/paperback/png/\"     \n",
    "        return Row(book_id=line.book_id, title=line.title, sub_title=line.sub_title, \n",
    "               total_wishlist=line.total_wishlist, no_of_page=line.no_of_page, publication_date=line.publication_date, \n",
    "               publisher=line.publisher, binding=line.binding, edition=line.edition, product_type=line.product_type, \n",
    "               total_votes=line.total_votes, data_type=line.data_type, author=line.author, genre=line.genre, \n",
    "               encrypt_item_id=line.encrypt_item_id, isbn=line.isbn, total_count=line.total_count, average_rating=line.average_rating, \n",
    "               total_review=line.total_review, book_index=line.book_index, content=line.content, description = line.content2, top_keywords=line.top_keywords, image_url=image_url)\n",
    "    \n",
    "#Aggiungiamo la nuova colonna al dataframe books_tocsv.\n",
    "RDDfinal = DF_final.rdd\n",
    "RDDfinal_with_urls = RDDfinal.map(createImageUrl)\n",
    "\n",
    "DF_final = RDDfinal_with_urls.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5df73b5d-9d48-4213-ad94-a1c3f15da07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(DF_final.count()) #3495 LIBRI\n",
    "#print(DF_final.select('book_id').distinct().count())\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "DF_final = DF_final.withColumn('content', regexp_replace('content', '\\,', ''))\n",
    "DF_final = DF_final.withColumn('content2', regexp_replace('content', '\\,', ''))\n",
    "DF_final = DF_final.withColumn('content', regexp_replace('content', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('content2', regexp_replace('content', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('content', regexp_replace('content', '\\\"', ''))\n",
    "DF_final = DF_final.withColumn('content2', regexp_replace('content', '\\\"', ''))\n",
    "DF_final.toPandas().to_csv('for_now_final_DF5.csv')\n",
    "\n",
    "#DF_final.filter(array_contains(col(\"top_keywords\"), \"scheda\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42fc3c8f-c457-4ad4-80f8-a172b5b78869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amore': 498, 'donna': 489, 'casa': 362, 'amico': 342, 'figlio': 342, 'famiglia': 333, 'morte': 313, 'giorno': 311, 'po': 300, 'ragazzo': 296, 'bambino': 287, 'vicenda': 284, 'guerra': 280, 'madre': 280, 'padre': 269, 'caso': 259, 'viaggio': 240, 'realtà': 228, 'paese': 224, 'città': 222, 'avventura': 220, 'moglie': 206, 'ragazza': 203, 'passato': 203, 'notte': 188, 'ricerca': 186, 'potere': 181, 'sogno': 178, 'idea': 178, 'occhio': 175, 'passione': 172, 'cuore': 171, 'marito': 169, 'saga': 162, 'stile': 160, 'strada': 160, 'fratello': 159, 'indagine': 154, 'forza': 153, 'rapporto': 153, 'terra': 152, 'sentimento': 151, 'opera': 149, 'mistero': 149, 'segreto': 148, 'corpo': 145, 'destino': 145, 'mente': 144, 'omicidio': 143, 'sangue': 143, 'pensiero': 140, 'esistenza': 139, 'dolore': 138, 'voce': 138, 'finale': 137, 'ricordo': 136, 'scrittura': 134, 'capo': 134, 'socio': 133, 'signore': 132, 'giovane': 132, 'letteratura': 132, 'paura': 132, 'film': 131, 'problema': 129, 'genitore': 127, 'scuola': 127, 'polizia': 125, 'verità': 124, 'colpo': 124, 'sorella': 124, 'giallo': 123, 'esperienza': 122, 'male': 122, 'incontro': 122, 'figlia': 120, 'mare': 119, 'delitto': 119, 'amicizia': 118, 'matrimonio': 116, 'vittima': 115, 'immagine': 114, 'futuro': 113, 're': 113, 'ora': 111, 'giornalista': 110, 'gioco': 110, 'gruppo': 110, 'commissario': 109, 'assassino': 109, 'lettera': 108, 'luce': 107, 'evento': 107, 'azione': 107, 'arte': 106, 'fumetto': 106, 'magia': 106, 'lotta': 106, 'ragazzino': 105, 'scelta': 105, 'memoria': 104, 'natura': 103, 'infanzia': 102, 'nemico': 102, 'eroe': 102, 'speranza': 102, 'vampiro': 102, 'raccolta': 101, 'saggio': 101, 'cultura': 101, 'amante': 101, 'morto': 100, 'anima': 100, 'violenza': 99, 'libertà': 99, 'nonno': 99, 'descrizione': 98, 'cadavere': 98, 'detective': 96, 'gente': 95, 'killer': 95, 'trilogia': 95, 'campo': 93, 'sesso': 92, 'creatura': 92, 'fidanzato': 92, 'dio': 91, 'thriller': 91, 'riflessione': 91, 'incubo': 91, 'testa': 91, 'professore': 90, 'villaggio': 90, 'ombra': 90, 'cane': 90, 'stato': 89, 'popolo': 89, 'fuga': 89, 'battaglia': 88, 'origine': 88, 'aiuto': 88, 'universo': 88, 'animale': 87, 'poliziotto': 87, 'orrore': 86, 'coppia': 86, 'isola': 86, 'luogo': 86, 'acqua': 86, 'estate': 85, 'spazio': 84, 'diario': 84, 'passo': 84, 'medico': 83, 'quartiere': 83, 'emozione': 83, 'incidente': 83, 'elemento': 83, 'mamma': 83, 'identità': 82, 'scoperta': 82, 'compagno': 82, 'tratto': 82, 'agente': 82, 'investigatore': 82, 'musica': 82, 'vista': 81, 'leggenda': 81, 'legge': 81, 'pericolo': 81, 'studente': 81, 'nipote': 80, 'atmosfera': 80, 'caccia': 80, 'stella': 80, 'periodo': 80, 'strega': 80, 'crimine': 80, 'conoscenza': 79, 'relazione': 79, 'regno': 79, 'ironia': 79, 'fantasy': 79, 'missione': 78, 'tragedia': 78, 'felicità': 78, 'politica': 78, 'giustizia': 78, 'sguardo': 78, 'momento': 78, 'desiderio': 78, 'avvocato': 78, 'maestro': 77, 'porta': 77, 'fantasia': 76, 'ragione': 76, 'crisi': 76, 'poesia': 76, 'ritmo': 76, 'confronto': 75, 'scomparsa': 75, 'colore': 75, 'percorso': 75, 'bene': 75, 'macchina': 74, 'milione': 74, 'abitante': 74, 'ritorno': 74, 'solitudine': 74, 'minaccia': 74, 'studio': 74, 'fantascienza': 74, 'servizio': 73, 'notizia': 73, 'soluzione': 73, 'processo': 73, 'indizio': 73, 'fantasma': 72, 'comunità': 72, 'collega': 72, 'soldo': 72, 'capello': 72, 'dubbio': 72, 'coraggio': 72, 'giro': 72, 'corte': 72, 'copertina': 72, 'civiltà': 72, 'traccia': 71, 'carriera': 71, 'maniera': 71, 'pace': 71, 'favola': 71, 'presenza': 71, 'teatro': 70, 'sensazione': 70, 'compagnia': 70, 'scienza': 70, 'seguito': 70, 'vendetta': 70, 'prova': 70, 'bellezza': 70, 'vento': 70, 'malattia': 69, 'montagna': 69, 'chiesa': 69, 'lato': 69, 'situazione': 69, 'affare': 69, 'fuoco': 69, 'ispettore': 69, 'università': 69, 'giornata': 69, 'ciclo': 69, 'legame': 69, 'poeta': 69, 'occasione': 69, 'causa': 68, 'umanità': 68, 'nave': 68, 'denaro': 68, 'sera': 68, 'chiave': 68, 'mito': 68, 'messaggio': 68, 'ruolo': 68, 'vacanza': 68, 'cittadino': 68, 'fortuna': 68, 'scienziato': 68, 'tipo': 68, 'sistema': 68, 'particolare': 68, 'rivista': 67, 'droga': 67, 'faccia': 67, 'ospedale': 67, 'valore': 67, 'religione': 67, 'ordine': 67, 'narratore': 67, 'resto': 67, 'inchiesta': 67, 'artista': 67, 'campagna': 67, 'tradizione': 67, 'vicino': 66, 'cielo': 66, 'ritratto': 66, 'bosco': 66, 'sfida': 66, 'provincia': 66, 'esercito': 66, 'impresa': 66, 'capacità': 66, 'guerriero': 66, 'gt': 66, 'dialogo': 66, 'noir': 66, 'ossessione': 65, 'attore': 65, 'spirito': 65, 'mostro': 65, 'cinema': 65, 'classe': 65, 'serial': 65, 'teoria': 65, 'colpevole': 65, 'nascita': 64, 'festa': 64, 'intrigo': 64, 'arma': 64, 'testimonianza': 64, 'mattina': 64, 'volto': 64, 'soldato': 64, 'buio': 64, 'appuntamento': 64, 'pianeta': 64, 'tentativo': 63, 'pietra': 63, 'demone': 63, 'bordo': 63, 'tradimento': 63, 'visione': 63, 'stagione': 63, 'adulto': 63, 'possibilità': 63, 'cronaca': 63, 'segno': 63, 'mago': 63, 'scritto': 63, 'scopo': 63, 'appartamento': 62, 'regina': 62, 'carcere': 62, 'fianco': 62, 'capitale': 62, 'commento': 62, 'confine': 62, 'auto': 62, 'termine': 62, 'rivoluzione': 62, 'zio': 62, 'minuto': 61, 'interno': 61, 'membro': 61, 'albero': 61, 'simbolo': 61, 'adolescenza': 61, 'tempo': 61, 'condizione': 61, 'essere': 61, 'premio': 60, 'intreccio': 60, 'rete': 60, 'colpa': 60, 'significato': 60, 'informazione': 60, 'metodo': 60, 'letto': 60, 'attività': 60, 'insegnante': 60, 'proprietario': 60, 'ambientazione': 60, 'filosofia': 59, 'adolescente': 59, 'punto': 59, 'biografia': 59, 'squadra': 59, 'giornale': 59, 'spalla': 59, 'intelligenza': 59, 'angelo': 59, 'concetto': 59, 'impero': 59, 'narrativa': 59, 'passaggio': 59, 'suicidio': 59, 'meraviglia': 59, 'ambiente': 59, 'limite': 59, 'dettaglio': 58, 'muro': 58, 'fede': 58, 'costume': 58, 'neve': 58, 'arrivo': 58, 'scontro': 58, 'zona': 58, 'giardino': 58, 'mestiere': 58, 'cura': 58, 'visita': 58, 'terrore': 58, 'eroina': 58, 'coscienza': 58, 'palazzo': 57, 'cattivo': 57, 'vecchio': 57, 'guida': 57, 'gioia': 57, 'tecnica': 57, 'inglese': 57, 'inferno': 57, 'caos': 57, 'amica': 57, 'canzone': 57, 'verso': 57, 'commedia': 57, 'zia': 57, 'stanza': 57, 'certezza': 57, 'erede': 57, 'numero': 57, 'contatto': 57, 'obiettivo': 57, 'razza': 57, 'fame': 57, 'puntata': 57, 'lezione': 57, 'strumento': 56, 'quadro': 56, 'episodio': 56, 'silenzio': 56, 'dottore': 56, 'treno': 56, 'genio': 56, 'filo': 56, 'circostanza': 56, 'carattere': 56, 'decennio': 56, 'settimana': 56, 'umano': 55, 'oro': 55, 'operazione': 55, 'vizio': 55, 'meta': 55, 'intervista': 55, 'sospetto': 55, 'tensione': 55, 'guardia': 55, 'governo': 55, 'movimento': 55, 'fiaba': 55, 'pioggia': 55, 'compagna': 55, 'errore': 55, 'luna': 55, 'attacco': 55, 'aspetto': 55, 'comportamento': 54, 'controllo': 54, 'umorismo': 54, 'comune': 54, 'talento': 54, 'inverno': 54, 'attesa': 54, 'padrone': 54, 'citazione': 54, 'solito': 54, 'conflitto': 54, 'fotografia': 54, 'appassionato': 54, 'piede': 54, 'ricchezza': 54, 'potenza': 54, 'bar': 54, 'logica': 54, 'aneddoto': 54, 'pezzo': 54, 'documento': 54, 'editore': 54, 'follia': 54, 'italiano': 53, 'abito': 53, 'pena': 53, 'intenzione': 53, 'diritto': 53, 'protagonista': 53, 'fiore': 53, 'secolo': 53, 'castello': 53, 'camera': 53, 'nord': 53, 'sofferenza': 53, 'cammino': 53, 'animo': 53, 'gesto': 53, 'fatica': 53, 'classico': 53, 'giovinezza': 52, 'miracolo': 52, 'rischio': 52, 'fascino': 52, 'cibo': 52, 'invenzione': 52, 'apparenza': 52, 'rito': 52, 'immaginazione': 52, 'battuta': 52, 'inganno': 52, 'eredità': 52, 'prezzo': 52, 'struttura': 52, 'sud': 52, 'inquietudine': 52, 'prodotto': 52, 'negozio': 52, 'radice': 52, 'materiale': 51, 'materia': 51, 'guaio': 51, 'questione': 51, 'sinistra': 51, 'territorio': 51, 'televisione': 51, 'ufficio': 51, 'senso': 51, 'rivelazione': 51, 'patria': 51, 'modello': 51, 'preda': 51, 'pregiudizio': 51, 'costruzione': 51, 'pubblico': 51, 'cena': 51, 'quotidiano': 51, 'codice': 51, 'tono': 51, 'difficoltà': 51, 'modo': 50, 'risata': 50, 'popolazione': 50, 'fase': 50, 'direzione': 50, 'organizzazione': 50, 'formazione': 50, 'piacere': 50, 'fiume': 50, 'mercato': 50, 'originale': 50, 'costo': 50, 'sole': 50, 'nota': 50, 'risultato': 50, 'cervello': 50, 'articolo': 50, 'affetto': 50, 'sorta': 50, 'divertimento': 50, 'specie': 50, 'capolavore': 50, 'politico': 50, 'semplicità': 49, 'odore': 49, 'pomeriggio': 49, 'differenza': 49, 'carta': 49, 'copia': 49, 'fallimento': 49, 'abitudine': 49, 'braccio': 49, 'cambiamento': 49, 'sorpresa': 49, 'fiducia': 49, 'cerca': 49, 'istinto': 49, 'finestra': 49, 'conseguenza': 49, 'attrazione': 49, 'difesa': 49, 'energia': 49, 'illusione': 49, 'linea': 49, 'fama': 49, 'sviluppo': 49, 'generale': 49, 'spunto': 49, 'riferimento': 48, 'compito': 48, 'personalità': 48, 'voglia': 48, 'distruzione': 48, 'immaginario': 48, 'pelle': 48, 'pugno': 48, 'mania': 48, 'perdita': 48, 'impegno': 48, 'pagina': 48, 'fan': 48, 'polvere': 48, 'odio': 48, 'regalo': 48, 'sconfitta': 48, 'onore': 48, 'prospettiva': 48, 'dramma': 48, 'nostalgia': 48, 'tv': 48, 'gusto': 48, 'moda': 47, 'ispirazione': 47, 'massa': 47, 'scambio': 47, 'promessa': 47, 'lavoro': 47, 'effetto': 47, 'ideale': 47, 'paio': 47, 'straordinario': 47, 'atto': 47, 'vite': 47, 'funzione': 47, 'innamorato': 47, 'tristezza': 47, 'vena': 47, 'malinconia': 47, 'corsa': 47, 'ricostruzione': 47, 'catena': 47, 'sopravvivenza': 47, 'trasformazione': 47, 'insieme': 47, 'attimo': 47, 'testimone': 47, 'difetto': 47, 'dovere': 46, 'migliaio': 46, 'caduta': 46, 'piega': 46, 'frutto': 46, 'miseria': 46, 'distanza': 46, 'foto': 46, 'individuo': 46, 'livello': 46, 'cavallo': 46, 'piano': 46, 'maschio': 46, 'tenerezza': 46, 'frase': 46, 'scenario': 46, 'dimensione': 46, 'portata': 46, 'ipotesi': 46, 'desidero': 46, 'impressione': 46, 'giudizio': 46, 'favore': 45, 'all': 45, 'esordio': 45, 'avviso': 45, 'grazia': 45, 'espressione': 45, 'personaggio': 45, 'riga': 45, 'clima': 45, 'parente': 45, 'interesse': 45, 'salto': 45, 'contraddizione': 45, 'specchio': 45, 'buco': 45, 'discussione': 45, 'conclusione': 45, 'ipocrisia': 45, 'interpretazione': 45, 'grande': 45, 'cambio': 45, 'suspense': 44, 'rappresentazione': 44, 'uscita': 44, 'psicologia': 44, 'prosa': 44, 'assenza': 44, 'decisione': 44, 'critico': 44, 'essenza': 44, 'curiosità': 44, 'programma': 44, 'diversità': 44, 'sorriso': 44, 'sonno': 44, 'massimo': 44, 'convinzione': 44, 'scrittore': 44, 'accordo': 44, 'creazione': 44, 'istante': 44, 'ecc': 44, 'scritta': 43, 'fonte': 43, 'nome': 43, 'altezza': 43, 'meccanismo': 43, 'crudeltà': 43, 'disposizione': 43, 'cosa': 43, 'meglio': 43, 'trattato': 43, 'peccato': 43, 'sicurezza': 43, 'disagio': 43, 'dato': 43, 'paesaggio': 43, 'eccezione': 43, 'frammento': 43, 'abbandono': 43, 'volontà': 43, 'volo': 43, 'lacrima': 43, 'fenomeno': 43, 'tentazione': 42, 'premessa': 42, 'avvenimento': 42, 'spiegazione': 42, 'media': 42, 'rabbia': 42, 'sacrificio': 42, 'soglia': 42, 'fisico': 42, 'persona': 42, 'evoluzione': 42, 'complesso': 42, 'considerazione': 42, 'innocenza': 42, 'ruota': 42, 'dose': 42, 'richiamo': 42, 'punta': 42, 'posizione': 42, 'fretta': 42, 'rispetto': 41, 'angolo': 41, 'misura': 41, 'realismo': 41, 'autore': 41, 'scaffale': 41, 'canto': 41, 'malgrado': 41, 'maturità': 41, 'parola': 41, 'entusiasmo': 41, 'importanza': 41, 'ansia': 41, 'pretesto': 41, 'principio': 41, 'lettura': 41, 'simpatia': 40, 'stellina': 40, 'centinaio': 40, 'epilogo': 40, 'svolta': 40, 'arco': 40, 'dignità': 40, 'salvezza': 40, 'soddisfazione': 40, 'delusione': 40, 'carica': 40, 'originalità': 40, 'comprensione': 40, 'parte': 40, 'motivo': 40, 'opinione': 40, 'angoscia': 40, 'aria': 40, 'profondità': 40, 'precedente': 39, 'caratteristica': 39, 'crescendo': 39, 'sforzo': 39, 'uso': 39, 'gamba': 39, 'volta': 39, 'categoria': 39, 'morale': 39, 'sorte': 39, 'fila': 39, 'partenza': 39, 'affresco': 39, 'responsabilità': 39, 'sensibilità': 39, 'onda': 39, 'normalità': 38, 'moto': 38, 'tutto': 38, 'preferito': 38, 'riguardo': 38, 'quotidianità': 38, 'fine': 38, 'stomaco': 38, 'qualità': 38, 'coinvolgimento': 38, 'voto': 38, 'presa': 38, 'genere': 38, 'amato': 38, 'carne': 38, 'atteggiamento': 38, 'tocco': 38, 'respiro': 38, 'discorso': 37, 'merito': 37, 'peso': 37, 'metafora': 37, 'perfezione': 37, 'impatto': 37, 'turno': 37, 'aspettativa': 37, 'pretesa': 37, 'banalità': 37, 'proposito': 37, 'giusto': 37, 'schema': 37, 'vuoto': 37, 'danno': 37, 'parere': 36, 'contenuto': 36, 'contorno': 36, 'trama': 36, 'risvolto': 36, 'approccio': 36, 'riscatto': 36, 'quantità': 36, 'caratterizzazione': 36, 'motivazione': 36, 'complessità': 36, 'sacco': 36, 'reazione': 36, 'decina': 36, 'noia': 36, 'disperazione': 36, 'intensità': 36, 'tanto': 36, 'piatto': 36, 'vero': 35, 'scena': 35, 'suono': 35, 'fiato': 35, 'approfondimento': 35, 'pregio': 35, 'maestria': 35, 'crescita': 35, 'contrasto': 34, 'bocca': 34, 'tematica': 34, 'contesto': 34, 'componente': 34, 'consapevolezza': 34, 'inizio': 34, 'certo': 33, 'fondo': 33, 'contrario': 33, 'debolezza': 33, 'abilità': 32, 'spessore': 32, 'mano': 32, 'bello': 32, 'recensione': 32, 'dote': 32, 'intento': 32, 'novità': 32, 'sapore': 31, 'naso': 31, 'sostanza': 31, 'buono': 31, 'amaro': 30, 'minimo': 29, 'mancanza': 29, 'sfumatura': 29, 'necessità': 27, 'pecca': 27, 'credo': 25}\n"
     ]
    }
   ],
   "source": [
    "###KEYWORD COVERAGE###\n",
    "\n",
    "##In questa sezione proverò a creare un set di N keywords (quante? Da decidere, per ora facciamo 100, anche se non\n",
    "##è detto che il problema abbia soluzione con 100) che coprano tutto il dataset dei 3500 libri.\n",
    "\n",
    "#Non avendo una chiara idea su come procedere, tecnicamente parlando, proverò a risolvere il problema in maniera\n",
    "#\"creativa\". Creiamo, innanzitutto, un dizionario contenente le parole chiave estratte dal dataframe DF_final\n",
    "#e assegniamo un \"peso\" ad ogni parola chiave. Il peso, nel nostro caso, non sarà altro che il numero di occorrenze\n",
    "#della parola chiave nella lista: infatti, se una parola compare, ad esempio, 5 volte, vuol dire che la parola chiave\n",
    "#\"copre\" 5 libri e così via. Questo vuol dire che, sapendo che il nostro dataframe ha 3554 libri, accumulare\n",
    "#3554 unità di peso dei vocaboli dovrebbe garantire una coverage completa del dataset.\n",
    "\n",
    "#Iniziamo con il contare i termini con il countvectorizer\n",
    "\n",
    "\n",
    "keywords_list = DF_final.toPandas()['top_keywords'].to_list()\n",
    "\n",
    "flat_list = [keyword for keywords_sublist in keywords_list for keyword in keywords_sublist]\n",
    "#print(len([item for item in flat_list if item==\"storia\"]))\n",
    "#print(flat_list)\n",
    "\n",
    "#Adesso che abbiamo la lista dei vari vocaboli, creiamo un dizionario che contenga elementi nella forma\n",
    "#vocabolo: occorrenza.\n",
    "\n",
    "occurrence_dict = {}\n",
    "\n",
    "def countOccurrencies(dictionary, reference_list):\n",
    "    for keyword in reference_list:\n",
    "        if keyword in dictionary:\n",
    "            dictionary[keyword] += 1\n",
    "        elif keyword not in dictionary:\n",
    "            dictionary[keyword] = 1\n",
    "    return dictionary\n",
    "\n",
    "occurrence_dict = countOccurrencies(occurrence_dict, flat_list)\n",
    "sorted_dict = {k: v for k, v in sorted(occurrence_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94be8f91-3984-4b9d-9db4-c3446ae7b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "#DF_final.show(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11b9b8af-ae32-47b0-9ee4-0c9898a8201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è amore. Ha una coverage di 498. La lunghezza rimanente del dataset è 3427. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è donna. Ha una coverage di 356. La lunghezza rimanente del dataset è 2929. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è amico. Ha una coverage di 251. La lunghezza rimanente del dataset è 2573. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è casa. Ha una coverage di 209. La lunghezza rimanente del dataset è 2322. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è guerra. Ha una coverage di 195. La lunghezza rimanente del dataset è 2322. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è caso. Ha una coverage di 180. La lunghezza rimanente del dataset è 2127. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è bambino. Ha una coverage di 162. La lunghezza rimanente del dataset è 1947. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è morte. Ha una coverage di 147. La lunghezza rimanente del dataset è 1785. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è giorno. Ha una coverage di 132. La lunghezza rimanente del dataset è 1638. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ragazzo. Ha una coverage di 127. La lunghezza rimanente del dataset è 1638. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è viaggio. Ha una coverage di 110. La lunghezza rimanente del dataset è 1511. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è vicenda. Ha una coverage di 103. La lunghezza rimanente del dataset è 1401. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è po. Ha una coverage di 96. La lunghezza rimanente del dataset è 1401. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è famiglia. Ha una coverage di 92. La lunghezza rimanente del dataset è 1401. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è realtà. Ha una coverage di 82. La lunghezza rimanente del dataset è 1309. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è avventura. Ha una coverage di 79. La lunghezza rimanente del dataset è 1227. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è paese. Ha una coverage di 70. La lunghezza rimanente del dataset è 1148. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è idea. Ha una coverage di 66. La lunghezza rimanente del dataset è 1078. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è città. Ha una coverage di 60. La lunghezza rimanente del dataset è 1012. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è padre. Ha una coverage di 53. La lunghezza rimanente del dataset è 952. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è indagine. Ha una coverage di 49. La lunghezza rimanente del dataset è 899. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è problema. Ha una coverage di 47. La lunghezza rimanente del dataset è 850. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ragazza. Ha una coverage di 42. La lunghezza rimanente del dataset è 803. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è capo. Ha una coverage di 37. La lunghezza rimanente del dataset è 761. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sogno. Ha una coverage di 35. La lunghezza rimanente del dataset è 724. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è esperienza. Ha una coverage di 34. La lunghezza rimanente del dataset è 689. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è re. Ha una coverage di 34. La lunghezza rimanente del dataset è 689. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è arte. Ha una coverage di 31. La lunghezza rimanente del dataset è 655. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è saggio. Ha una coverage di 31. La lunghezza rimanente del dataset è 624. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ricerca. Ha una coverage di 29. La lunghezza rimanente del dataset è 624. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è signore. Ha una coverage di 27. La lunghezza rimanente del dataset è 595. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è giornalista. Ha una coverage di 27. La lunghezza rimanente del dataset è 568. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sangue. Ha una coverage di 25. La lunghezza rimanente del dataset è 541. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è paura. Ha una coverage di 23. La lunghezza rimanente del dataset è 516. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è cultura. Ha una coverage di 23. La lunghezza rimanente del dataset è 493. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è occhio. Ha una coverage di 22. La lunghezza rimanente del dataset è 470. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è gioco. Ha una coverage di 21. La lunghezza rimanente del dataset è 448. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è raccolta. Ha una coverage di 21. La lunghezza rimanente del dataset è 427. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è passato. Ha una coverage di 19. La lunghezza rimanente del dataset è 406. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è tragedia. Ha una coverage di 18. La lunghezza rimanente del dataset è 387. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è giallo. Ha una coverage di 17. La lunghezza rimanente del dataset è 369. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è nemico. Ha una coverage di 17. La lunghezza rimanente del dataset è 369. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è esistenza. Ha una coverage di 16. La lunghezza rimanente del dataset è 352. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è delitto. Ha una coverage di 16. La lunghezza rimanente del dataset è 336. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sistema. Ha una coverage di 15. La lunghezza rimanente del dataset è 320. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è battaglia. Ha una coverage di 14. La lunghezza rimanente del dataset è 305. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è significato. Ha una coverage di 14. La lunghezza rimanente del dataset è 291. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è lotta. Ha una coverage di 13. La lunghezza rimanente del dataset è 291. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è genio. Ha una coverage di 13. La lunghezza rimanente del dataset è 278. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è memoria. Ha una coverage di 12. La lunghezza rimanente del dataset è 265. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è essere. Ha una coverage di 12. La lunghezza rimanente del dataset è 253. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scrittura. Ha una coverage di 11. La lunghezza rimanente del dataset è 253. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è film. Ha una coverage di 11. La lunghezza rimanente del dataset è 253. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è maestro. Ha una coverage di 11. La lunghezza rimanente del dataset è 242. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è macchina. Ha una coverage di 11. La lunghezza rimanente del dataset è 242. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scienza. Ha una coverage di 11. La lunghezza rimanente del dataset è 231. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è notte. Ha una coverage di 10. La lunghezza rimanente del dataset è 220. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ordine. Ha una coverage di 10. La lunghezza rimanente del dataset è 210. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è concetto. Ha una coverage di 10. La lunghezza rimanente del dataset è 200. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è numero. Ha una coverage di 10. La lunghezza rimanente del dataset è 200. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è assassino. Ha una coverage di 9. La lunghezza rimanente del dataset è 200. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è riflessione. Ha una coverage di 9. La lunghezza rimanente del dataset è 191. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è studio. Ha una coverage di 9. La lunghezza rimanente del dataset è 182. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è saga. Ha una coverage di 8. La lunghezza rimanente del dataset è 173. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scuola. Ha una coverage di 8. La lunghezza rimanente del dataset è 165. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è animale. Ha una coverage di 8. La lunghezza rimanente del dataset è 157. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è diritto. Ha una coverage di 8. La lunghezza rimanente del dataset è 149. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è acqua. Ha una coverage di 7. La lunghezza rimanente del dataset è 141. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è quartiere. Ha una coverage di 7. La lunghezza rimanente del dataset è 134. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è scontro. Ha una coverage di 7. La lunghezza rimanente del dataset è 127. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è stile. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è evento. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fumetto. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è morto. Ha una coverage di 6. La lunghezza rimanente del dataset è 120. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è poesia. Ha una coverage di 6. La lunghezza rimanente del dataset è 114. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è minuto. Ha una coverage di 6. La lunghezza rimanente del dataset è 108. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è classico. Ha una coverage di 6. La lunghezza rimanente del dataset è 108. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è male. Ha una coverage di 5. La lunghezza rimanente del dataset è 102. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ombra. Ha una coverage di 5. La lunghezza rimanente del dataset è 97. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è orrore. Ha una coverage di 5. La lunghezza rimanente del dataset è 92. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è tratto. Ha una coverage di 5. La lunghezza rimanente del dataset è 87. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è occasione. Ha una coverage di 5. La lunghezza rimanente del dataset è 87. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è artista. Ha una coverage di 5. La lunghezza rimanente del dataset è 87. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è cronaca. Ha una coverage di 5. La lunghezza rimanente del dataset è 82. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è tecnica. Ha una coverage di 5. La lunghezza rimanente del dataset è 77. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è inglese. Ha una coverage di 5. La lunghezza rimanente del dataset è 77. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è compito. Ha una coverage di 5. La lunghezza rimanente del dataset è 72. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è sentimento. Ha una coverage di 4. La lunghezza rimanente del dataset è 67. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è mistero. Ha una coverage di 4. La lunghezza rimanente del dataset è 63. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è destino. Ha una coverage di 4. La lunghezza rimanente del dataset è 59. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è avvocato. Ha una coverage di 4. La lunghezza rimanente del dataset è 55. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è bar. Ha una coverage di 4. La lunghezza rimanente del dataset è 51. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è semplicità. Ha una coverage di 4. La lunghezza rimanente del dataset è 47. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è pregio. Ha una coverage di 4. La lunghezza rimanente del dataset è 43. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è figlia. Ha una coverage di 3. La lunghezza rimanente del dataset è 43. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ora. Ha una coverage di 3. La lunghezza rimanente del dataset è 40. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è stato. Ha una coverage di 3. La lunghezza rimanente del dataset è 40. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è servizio. Ha una coverage di 3. La lunghezza rimanente del dataset è 37. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è vento. Ha una coverage di 3. La lunghezza rimanente del dataset è 37. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è commedia. Ha una coverage di 3. La lunghezza rimanente del dataset è 34. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è moglie. Ha una coverage di 2. La lunghezza rimanente del dataset è 31. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è omicidio. Ha una coverage di 2. La lunghezza rimanente del dataset è 29. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è mare. Ha una coverage di 2. La lunghezza rimanente del dataset è 27. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è luogo. Ha una coverage di 2. La lunghezza rimanente del dataset è 25. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è passo. Ha una coverage di 2. La lunghezza rimanente del dataset è 25. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è studente. Ha una coverage di 2. La lunghezza rimanente del dataset è 25. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è notizia. Ha una coverage di 2. La lunghezza rimanente del dataset è 23. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è chiesa. Ha una coverage di 2. La lunghezza rimanente del dataset è 21. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è ciclo. Ha una coverage di 2. La lunghezza rimanente del dataset è 19. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è narratore. Ha una coverage di 2. La lunghezza rimanente del dataset è 17. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è pezzo. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è specie. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fan. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è interpretazione. Ha una coverage di 2. La lunghezza rimanente del dataset è 15. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è passione. Ha una coverage di 1. La lunghezza rimanente del dataset è 15. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è rapporto. Ha una coverage di 1. La lunghezza rimanente del dataset è 14. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è letteratura. Ha una coverage di 1. La lunghezza rimanente del dataset è 13. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è polizia. Ha una coverage di 1. La lunghezza rimanente del dataset è 13. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è amicizia. Ha una coverage di 1. La lunghezza rimanente del dataset è 12. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è immagine. Ha una coverage di 1. La lunghezza rimanente del dataset è 11. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è lettera. Ha una coverage di 1. La lunghezza rimanente del dataset è 11. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è campo. Ha una coverage di 1. La lunghezza rimanente del dataset è 10. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è popolo. Ha una coverage di 1. La lunghezza rimanente del dataset è 10. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è compagno. Ha una coverage di 1. La lunghezza rimanente del dataset è 9. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è relazione. Ha una coverage di 1. La lunghezza rimanente del dataset è 8. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è momento. Ha una coverage di 1. La lunghezza rimanente del dataset è 7. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è crisi. Ha una coverage di 1. La lunghezza rimanente del dataset è 7. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fantascienza. Ha una coverage di 1. La lunghezza rimanente del dataset è 6. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è maniera. Ha una coverage di 1. La lunghezza rimanente del dataset è 5. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è messaggio. Ha una coverage di 1. La lunghezza rimanente del dataset è 5. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è vicino. Ha una coverage di 1. La lunghezza rimanente del dataset è 5. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è cielo. Ha una coverage di 1. La lunghezza rimanente del dataset è 4. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è rete. Ha una coverage di 1. La lunghezza rimanente del dataset è 3. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è angelo. Ha una coverage di 1. La lunghezza rimanente del dataset è 3. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è gioia. Ha una coverage di 1. La lunghezza rimanente del dataset è 2. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è fotografia. Ha una coverage di 1. La lunghezza rimanente del dataset è 1. (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCARTATA\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "La parola scelta è entusiasmo. Ha una coverage di 1. La lunghezza rimanente del dataset è 1. (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD SCELTA\n",
      "['amore', 'donna', 'amico', 'guerra', 'caso', 'bambino', 'morte', 'ragazzo', 'viaggio', 'famiglia', 'realtà', 'avventura', 'paese', 'idea', 'città', 'padre', 'indagine', 'problema', 'ragazza', 'capo', 'sogno', 're', 'arte', 'ricerca', 'signore', 'giornalista', 'sangue', 'paura', 'cultura', 'occhio', 'gioco', 'raccolta', 'passato', 'tragedia', 'nemico', 'esistenza', 'delitto', 'sistema', 'battaglia', 'lotta', 'genio', 'memoria', 'film', 'macchina', 'scienza', 'notte', 'ordine', 'assassino', 'riflessione', 'studio', 'saga', 'scuola', 'animale', 'diritto', 'acqua', 'quartiere', 'scontro', 'morto', 'poesia', 'classico', 'male', 'ombra', 'orrore', 'artista', 'cronaca', 'inglese', 'compito', 'sentimento', 'mistero', 'destino', 'avvocato', 'bar', 'semplicità', 'figlia', 'stato', 'vento', 'commedia', 'moglie', 'omicidio', 'mare', 'studente', 'notizia', 'chiesa', 'ciclo', 'narratore', 'passione', 'rapporto', 'polizia', 'amicizia', 'lettera', 'popolo', 'compagno', 'relazione', 'crisi', 'fantascienza', 'vicino', 'cielo', 'angelo', 'gioia', 'entusiasmo']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Il dizionario ha adesso una forma più decente.\n",
    "\n",
    "#Implementiamo una funzione che estragga un set di keywords che coprano l'intero dataset di libri.\n",
    "\n",
    "#La funzione deve darci un set di X keywords (per ora facciamo 100) che:\n",
    "#-Copra tutto il dataset\n",
    "#-Abbia keywords il meno ridondanti possibile\n",
    "\n",
    "#Abbiamo a disposizione il dataframe (DF_final) e il dizionario (sorted_dict).\n",
    "\n",
    "\n",
    "#import random\n",
    "#DF_final.filter(array_contains(col('top_keywords'), 'entusiasmo')).show(10, False) #Possiamo usare array_contains per filtrare\n",
    "#per keyword\n",
    "def keywordCoverage(dictionary, dataframe):\n",
    "    \n",
    "    output_list = []\n",
    "    \n",
    "    temp_dictionary = dictionary.copy()\n",
    "    temp_subset = dataframe.toPandas()['top_keywords'].to_list()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        max_coverage_count = 0\n",
    "        chosen_keyword = \"\"\n",
    "        \n",
    "        for keyword in temp_dictionary:\n",
    "        \n",
    "            coverage_count = len([item for item in temp_subset if keyword in item])\n",
    "            if coverage_count > max_coverage_count:\n",
    "                max_coverage_count = coverage_count\n",
    "                chosen_keyword = keyword\n",
    "            \n",
    "        try:\n",
    "            choice = input(f\"La parola scelta è {chosen_keyword}. Ha una coverage di {max_coverage_count}. La lunghezza rimanente del dataset è {len(temp_subset)}. (y/n)\")\n",
    "\n",
    "            if choice == \"y\":\n",
    "                temp_dictionary.pop(chosen_keyword)\n",
    "                output_list.append(chosen_keyword)\n",
    "                temp_subset = [item for item in temp_subset if chosen_keyword not in item]\n",
    "                print(\"KEYWORD SCELTA\")\n",
    "                if len(temp_subset) == 0:\n",
    "                    break\n",
    "                    \n",
    "            elif choice == \"n\":\n",
    "                temp_dictionary.pop(chosen_keyword)\n",
    "                print(\"KEYWORD SCARTATA\")\n",
    "                if len(temp_subset) == 0:\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Scegliere una scelta possibile (y/n).\")\n",
    "\n",
    "    return output_list\n",
    "\n",
    "chosen_N = keywordCoverage(sorted_dict, DF_final)\n",
    "print(chosen_N)\n",
    "print(len(chosen_N))\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed5aeab-30ee-4f51-b6b4-5e4ca024faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dizionario keyword scelte : percentuale di generi\n",
    "from pyspark.sql.functions import col, array_contains, size\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#Oltre alla nostra lista delle keyword, dovremo usare il solito DF_final.\n",
    "#print(chosen_N)\n",
    "DF_final = spark.read.csv('anobii_genres/for_now_final_DF4.csv', header=True)\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', '\\[', ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', '\\]', ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\, \", ','))\n",
    "#DF_final_new = DF_final_new.withColumn('genre_vector', array(DF_final_new.genre_vector))\n",
    "\n",
    "DF_final = DF_final.withColumn('top_keywords', split(col('top_keywords'), \",\"))\n",
    "\n",
    "#DF_final.show(1, False)\n",
    "#LISTA GIà CREATA\n",
    "chosen_N = ['amore', 'donna', 'amico', 'guerra', 'caso', 'bambino', 'morte', 'ragazzo', 'viaggio', 'famiglia', 'realtà', 'avventura', 'paese', 'idea', 'città', 'padre', 'indagine', 'problema', 'ragazza', 'capo', 'sogno', 're', 'arte', 'ricerca', 'signore', 'giornalista', 'sangue', 'paura', 'cultura', 'occhio', 'gioco', 'raccolta', 'passato', 'tragedia', 'nemico', 'esistenza', 'delitto', 'sistema', 'battaglia', 'lotta', 'genio', 'memoria', 'film', 'macchina', 'scienza', 'notte', 'ordine', 'assassino', 'riflessione', 'studio', 'saga', 'scuola', 'animale', 'diritto', 'acqua', 'quartiere', 'scontro', 'morto', 'poesia', 'classico', 'male', 'ombra', 'orrore', 'artista', 'cronaca', 'inglese', 'compito', 'sentimento', 'mistero', 'destino', 'avvocato', 'bar', 'semplicità', 'figlia', 'stato', 'vento', 'commedia', 'moglie', 'omicidio', 'mare', 'studente', 'notizia', 'chiesa', 'ciclo', 'narratore', 'passione', 'rapporto', 'polizia', 'amicizia', 'lettera', 'popolo', 'compagno', 'relazione', 'crisi', 'fantascienza', 'vicino', 'cielo', 'angelo', 'gioia', 'entusiasmo']\n",
    "#chosen_N = ['amore', 'donna', 'amico', 'morte', 'bambino', 'guerra', 'caso', 'realtà', 'ragazzo', 'figlio', 'paese', 'città', 'idea', 'avventura', 'famiglia', 'viaggio', 'mistero', 'problema', 'ricerca', 'potere', 'raccolta', 'paura', 'polizia', 'padre', 'delitto', 'arte', 'saga', 'giornalista', 'passato', 'film', 'battaglia', 'gioco', 'universo', 'notte', 'capo', 'ragazza', 'pensiero', 'movimento', 'cuore', 'male', 'tragedia', 'speranza', 'morto', 'droga', 'processo', 'animale', 'poesia', 'genio', 'signore', 'mare', 'killer', 'coppia', 'riflessione', 'scoperta', 'aiuto', 'faccia', 'psicologia', 'esistenza', 'cultura', 'dio', 'studio', 'cronaca', 'voce', 'artista', 'trattato', 'assassino', 'natura', 'cane', 'caccia', 'civiltà', 'fantascienza', 'quadro', 'moglie', 'passione', 'strada', 'figlia', 'provincia', 'sangue', 'orrore', 'compagno', 'stato', 'sfida', 'ordine', 'angelo', 'voglia', 'oro', 'rischio', 'rivelazione', 'rapporto', 'sentimento', 'lettera', 'vittima', 'amante', 'relazione', 'testa', 'popolo', 'comunità', 'ciclo', 'ispirazione', 'entusiasmo']\n",
    "def genresPercentage(dataframe, _list):\n",
    "    \n",
    "    DF_final_pandas = dataframe.toPandas() #Local dataframe\n",
    "    percentage_dict = {}\n",
    "    \n",
    "    for keyword in _list:\n",
    "        masking = DF_final_pandas.top_keywords.apply(lambda x: keyword in x)\n",
    "        DF_final_keyword = DF_final_pandas[masking] #Dataframe contenente solo le tuple che contengono la keyword\n",
    "\n",
    "        genre_list = DF_final_keyword['genre'].to_list() #Estraiamo il dizionario con i generi per ogni tupla con la keyword\n",
    "\n",
    "        percentage_dict[keyword] = {'total': len(genre_list)}\n",
    "\n",
    "        for dictionary in genre_list:\n",
    "            dictionary = dictionary.replace(\"'\", '\"')\n",
    "            dictionary = json.loads(dictionary)\n",
    "            \n",
    "            for key in dictionary:\n",
    "\n",
    "                if key in percentage_dict[keyword]:\n",
    "                    percentage_dict[keyword][key] += float(dictionary[key])/sum(list(map(float, dictionary.values()))) #WEIGHTED\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    percentage_dict[keyword][key] = float(dictionary[key])/sum(list(map(float, dictionary.values())))\n",
    "\n",
    "        for key in percentage_dict[keyword]:\n",
    "\n",
    "            if key != 'total':\n",
    "                percentage_dict[keyword][key] = (percentage_dict[keyword][key]) / percentage_dict[keyword]['total']\n",
    "    return percentage_dict\n",
    "        \n",
    "percentage_dict = genresPercentage(DF_final, chosen_N)\n",
    "#print(percentage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3637635-2795-4081-ae58-98c4580266d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amore</th>\n",
       "      <th>donna</th>\n",
       "      <th>amico</th>\n",
       "      <th>guerra</th>\n",
       "      <th>caso</th>\n",
       "      <th>bambino</th>\n",
       "      <th>morte</th>\n",
       "      <th>ragazzo</th>\n",
       "      <th>viaggio</th>\n",
       "      <th>famiglia</th>\n",
       "      <th>...</th>\n",
       "      <th>popolo</th>\n",
       "      <th>compagno</th>\n",
       "      <th>relazione</th>\n",
       "      <th>crisi</th>\n",
       "      <th>fantascienza</th>\n",
       "      <th>vicino</th>\n",
       "      <th>cielo</th>\n",
       "      <th>angelo</th>\n",
       "      <th>gioia</th>\n",
       "      <th>entusiasmo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>498.000000</td>\n",
       "      <td>489.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family-Sex&amp;Relationships</th>\n",
       "      <td>0.077583</td>\n",
       "      <td>0.071336</td>\n",
       "      <td>0.053017</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.042209</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>0.069515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.079370</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.062677</td>\n",
       "      <td>0.037298</td>\n",
       "      <td>0.024232</td>\n",
       "      <td>0.054202</td>\n",
       "      <td>0.043390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction&amp;Literature</th>\n",
       "      <td>0.465981</td>\n",
       "      <td>0.471666</td>\n",
       "      <td>0.377122</td>\n",
       "      <td>0.420234</td>\n",
       "      <td>0.325622</td>\n",
       "      <td>0.412439</td>\n",
       "      <td>0.400174</td>\n",
       "      <td>0.384013</td>\n",
       "      <td>0.410636</td>\n",
       "      <td>0.497394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385654</td>\n",
       "      <td>0.395387</td>\n",
       "      <td>0.430943</td>\n",
       "      <td>0.404410</td>\n",
       "      <td>0.300122</td>\n",
       "      <td>0.518016</td>\n",
       "      <td>0.424262</td>\n",
       "      <td>0.365695</td>\n",
       "      <td>0.415673</td>\n",
       "      <td>0.388628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humor</th>\n",
       "      <td>0.016840</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.031213</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>0.025319</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>0.016331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023820</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.027124</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.024281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>0.041606</td>\n",
       "      <td>0.063047</td>\n",
       "      <td>0.023806</td>\n",
       "      <td>0.155903</td>\n",
       "      <td>0.029198</td>\n",
       "      <td>0.042920</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167196</td>\n",
       "      <td>0.068632</td>\n",
       "      <td>0.037682</td>\n",
       "      <td>0.046965</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.048887</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>0.047353</td>\n",
       "      <td>0.066561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceFiction&amp;Fantasy</th>\n",
       "      <td>0.122917</td>\n",
       "      <td>0.056813</td>\n",
       "      <td>0.128966</td>\n",
       "      <td>0.134404</td>\n",
       "      <td>0.056636</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.137805</td>\n",
       "      <td>0.187869</td>\n",
       "      <td>0.153997</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148319</td>\n",
       "      <td>0.156112</td>\n",
       "      <td>0.083934</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>0.443558</td>\n",
       "      <td>0.078312</td>\n",
       "      <td>0.147298</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0.030744</td>\n",
       "      <td>0.016683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>0.093787</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053776</td>\n",
       "      <td>0.029639</td>\n",
       "      <td>0.011704</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.009853</td>\n",
       "      <td>0.012518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>0.094860</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.016705</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>0.037910</td>\n",
       "      <td>0.014372</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.026792</td>\n",
       "      <td>0.066776</td>\n",
       "      <td>0.048576</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.029833</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>0.053807</td>\n",
       "      <td>0.098713</td>\n",
       "      <td>0.060331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-fiction</th>\n",
       "      <td>0.008622</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.016767</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.021512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biography</th>\n",
       "      <td>0.018007</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.022155</td>\n",
       "      <td>0.037153</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.027841</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.027955</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048289</td>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.025773</td>\n",
       "      <td>0.050706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mystery&amp;Thrillers</th>\n",
       "      <td>0.023860</td>\n",
       "      <td>0.113707</td>\n",
       "      <td>0.119173</td>\n",
       "      <td>0.048287</td>\n",
       "      <td>0.302206</td>\n",
       "      <td>0.094747</td>\n",
       "      <td>0.134313</td>\n",
       "      <td>0.085447</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.084172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>0.047603</td>\n",
       "      <td>0.092669</td>\n",
       "      <td>0.053594</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>0.136158</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>0.102838</td>\n",
       "      <td>0.035962</td>\n",
       "      <td>0.077176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialScience</th>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>0.015111</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.008942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Political</th>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039924</td>\n",
       "      <td>0.010081</td>\n",
       "      <td>0.013034</td>\n",
       "      <td>0.033289</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime</th>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.048773</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.011087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horror</th>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.009654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FreeTime</th>\n",
       "      <td>0.014366</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.010315</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.013997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.107984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Children&amp;Teens</th>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.050129</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.059050</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.103596</td>\n",
       "      <td>0.019257</td>\n",
       "      <td>0.030642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.065156</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>0.025783</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.017960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comics&amp;GraphicNovels</th>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.011979</td>\n",
       "      <td>0.042839</td>\n",
       "      <td>0.033309</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>0.047796</td>\n",
       "      <td>0.038550</td>\n",
       "      <td>0.030033</td>\n",
       "      <td>0.019749</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016877</td>\n",
       "      <td>0.069637</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>0.044239</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>0.041024</td>\n",
       "      <td>0.039985</td>\n",
       "      <td>0.098449</td>\n",
       "      <td>0.011456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philosophy</th>\n",
       "      <td>0.015951</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.014224</td>\n",
       "      <td>0.025687</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>0.016168</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.020931</td>\n",
       "      <td>0.036169</td>\n",
       "      <td>0.015887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health-Mind&amp;Body</th>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.016028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Professional&amp;Technical</th>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.022275</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.013673</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002879</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.028425</td>\n",
       "      <td>0.038270</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.013298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science&amp;Nature</th>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.012914</td>\n",
       "      <td>0.023090</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>0.013055</td>\n",
       "      <td>0.014830</td>\n",
       "      <td>0.015186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               amore       donna       amico      guerra  \\\n",
       "total                     498.000000  489.000000  342.000000  280.000000   \n",
       "Family-Sex&Relationships    0.077583    0.071336    0.053017    0.023265   \n",
       "Fiction&Literature          0.465981    0.471666    0.377122    0.420234   \n",
       "Humor                       0.016840    0.022616    0.031213    0.010151   \n",
       "History                     0.041606    0.063047    0.023806    0.155903   \n",
       "ScienceFiction&Fantasy      0.122917    0.056813    0.128966    0.134404   \n",
       "Travel                      0.010429    0.004777    0.013312    0.016067   \n",
       "Romance                     0.094860    0.067545    0.049130    0.016181   \n",
       "Non-fiction                 0.008622    0.009733    0.011121    0.006699   \n",
       "Biography                   0.018007    0.030568    0.022155    0.037153   \n",
       "Mystery&Thrillers           0.023860    0.113707    0.119173    0.048287   \n",
       "SocialScience               0.004732    0.012127    0.002910    0.008760   \n",
       "Political                   0.002393    0.002673    0.002544    0.028978   \n",
       "Crime                       0.003655    0.016611    0.015129    0.006342   \n",
       "Horror                      0.005421    0.001785    0.004354    0.002031   \n",
       "FreeTime                    0.014366    0.010027    0.019066    0.011982   \n",
       "Children&Teens              0.030928    0.006799    0.050129    0.013388   \n",
       "Comics&GraphicNovels        0.022945    0.011979    0.042839    0.033309   \n",
       "Philosophy                  0.015951    0.009150    0.012136    0.012090   \n",
       "Health-Mind&Body            0.008408    0.011112    0.008082    0.003879   \n",
       "Professional&Technical      0.007071    0.004376    0.007512    0.006069   \n",
       "Science&Nature              0.003426    0.001554    0.006286    0.004828   \n",
       "\n",
       "                                caso     bambino       morte     ragazzo  \\\n",
       "total                     259.000000  287.000000  313.000000  296.000000   \n",
       "Family-Sex&Relationships    0.023596    0.039989    0.035387    0.042209   \n",
       "Fiction&Literature          0.325622    0.412439    0.400174    0.384013   \n",
       "Humor                       0.025319    0.032527    0.010045    0.015073   \n",
       "History                     0.029198    0.042920    0.055023    0.026359   \n",
       "ScienceFiction&Fantasy      0.056636    0.098300    0.137805    0.187869   \n",
       "Travel                      0.005088    0.010130    0.006077    0.005721   \n",
       "Romance                     0.016705    0.020649    0.025556    0.037910   \n",
       "Non-fiction                 0.016767    0.005724    0.013274    0.005156   \n",
       "Biography                   0.011683    0.028759    0.027841    0.010319   \n",
       "Mystery&Thrillers           0.302206    0.094747    0.134313    0.085447   \n",
       "SocialScience               0.010007    0.015111    0.006198    0.002840   \n",
       "Political                   0.014050    0.008144    0.012973    0.005701   \n",
       "Crime                       0.048773    0.017204    0.024112    0.016097   \n",
       "Horror                      0.005036    0.009023    0.013531    0.005848   \n",
       "FreeTime                    0.004571    0.010315    0.009053    0.005978   \n",
       "Children&Teens              0.007964    0.059050    0.013568    0.103596   \n",
       "Comics&GraphicNovels        0.034410    0.047796    0.038550    0.030033   \n",
       "Philosophy                  0.020695    0.004800    0.010146    0.004765   \n",
       "Health-Mind&Body            0.012000    0.011936    0.018115    0.007593   \n",
       "Professional&Technical      0.014467    0.022275    0.006316    0.013673   \n",
       "Science&Nature              0.015210    0.008160    0.001945    0.003797   \n",
       "\n",
       "                             viaggio    famiglia  ...     popolo   compagno  \\\n",
       "total                     240.000000  333.000000  ...  89.000000  82.000000   \n",
       "Family-Sex&Relationships    0.028915    0.069515  ...   0.005180   0.048247   \n",
       "Fiction&Literature          0.410636    0.497394  ...   0.385654   0.395387   \n",
       "Humor                       0.019540    0.016331  ...   0.023820   0.020268   \n",
       "History                     0.044810    0.070389  ...   0.167196   0.068632   \n",
       "ScienceFiction&Fantasy      0.153997    0.073314  ...   0.148319   0.156112   \n",
       "Travel                      0.093787    0.004235  ...   0.053776   0.029639   \n",
       "Romance                     0.014372    0.043332  ...   0.006921   0.026792   \n",
       "Non-fiction                 0.014925    0.004812  ...   0.007417   0.005030   \n",
       "Biography                   0.027955    0.034424  ...   0.048289   0.019483   \n",
       "Mystery&Thrillers           0.062775    0.084172  ...   0.011586   0.047603   \n",
       "SocialScience               0.015423    0.008942  ...   0.018000   0.001016   \n",
       "Political                   0.002144    0.007543  ...   0.039924   0.010081   \n",
       "Crime                       0.007368    0.010944  ...        NaN   0.006567   \n",
       "Horror                      0.002932    0.004765  ...   0.003977   0.005330   \n",
       "FreeTime                    0.011281    0.005570  ...   0.007668   0.009191   \n",
       "Children&Teens              0.019257    0.030642  ...   0.012645   0.065156   \n",
       "Comics&GraphicNovels        0.019749    0.010063  ...   0.016877   0.069637   \n",
       "Philosophy                  0.017676    0.006015  ...   0.021618   0.004268   \n",
       "Health-Mind&Body            0.014846    0.004485  ...   0.013251   0.001524   \n",
       "Professional&Technical      0.010441    0.008460  ...   0.002879   0.006113   \n",
       "Science&Nature              0.007170    0.004654  ...   0.005005   0.003925   \n",
       "\n",
       "                          relazione      crisi  fantascienza     vicino  \\\n",
       "total                     79.000000  76.000000     74.000000  66.000000   \n",
       "Family-Sex&Relationships   0.079370   0.054967      0.009798   0.062677   \n",
       "Fiction&Literature         0.430943   0.404410      0.300122   0.518016   \n",
       "Humor                      0.034480   0.056701      0.011735   0.027124   \n",
       "History                    0.037682   0.046965      0.005690   0.048887   \n",
       "ScienceFiction&Fantasy     0.083934   0.032689      0.443558   0.078312   \n",
       "Travel                     0.011704   0.012762      0.000808   0.001515   \n",
       "Romance                    0.066776   0.048576      0.003448   0.029833   \n",
       "Non-fiction                0.003093   0.017514      0.014521   0.001459   \n",
       "Biography                  0.017859   0.011307      0.006164   0.004798   \n",
       "Mystery&Thrillers          0.092669   0.053594      0.078507   0.136158   \n",
       "SocialScience              0.016385   0.046775      0.003488   0.009591   \n",
       "Political                  0.013034   0.033289      0.016167   0.002407   \n",
       "Crime                      0.018069   0.013704      0.009382   0.016601   \n",
       "Horror                          NaN   0.007682      0.003187   0.005859   \n",
       "FreeTime                   0.008693   0.013997           NaN   0.003409   \n",
       "Children&Teens             0.006826   0.017954      0.003218   0.016315   \n",
       "Comics&GraphicNovels       0.012547   0.032010      0.044239   0.014367   \n",
       "Philosophy                 0.014224   0.025687      0.011580   0.016168   \n",
       "Health-Mind&Body           0.010375   0.008058      0.007197   0.004098   \n",
       "Professional&Technical     0.028425   0.038270      0.012890   0.000722   \n",
       "Science&Nature             0.012914   0.023090      0.014302   0.001684   \n",
       "\n",
       "                              cielo     angelo      gioia  entusiasmo  \n",
       "total                     66.000000  59.000000  57.000000   41.000000  \n",
       "Family-Sex&Relationships   0.037298   0.024232   0.054202    0.043390  \n",
       "Fiction&Literature         0.424262   0.365695   0.415673    0.388628  \n",
       "Humor                      0.021404   0.024735   0.021127    0.024281  \n",
       "History                    0.044705   0.019869   0.047353    0.066561  \n",
       "ScienceFiction&Fantasy     0.147298   0.210616   0.030744    0.016683  \n",
       "Travel                     0.016991   0.000394   0.009853    0.012518  \n",
       "Romance                    0.039689   0.053807   0.098713    0.060331  \n",
       "Non-fiction                0.007938   0.005008   0.010121    0.021512  \n",
       "Biography                  0.014151   0.004592   0.025773    0.050706  \n",
       "Mystery&Thrillers          0.067922   0.102838   0.035962    0.077176  \n",
       "SocialScience              0.017231   0.013015   0.006968    0.001161  \n",
       "Political                       NaN        NaN        NaN    0.018510  \n",
       "Crime                      0.013578   0.011435   0.004920    0.011087  \n",
       "Horror                     0.001377   0.000924   0.007246    0.009654  \n",
       "FreeTime                   0.008623   0.013996   0.010517    0.107984  \n",
       "Children&Teens             0.030971   0.025783   0.021729    0.017960  \n",
       "Comics&GraphicNovels       0.041024   0.039985   0.098449    0.011456  \n",
       "Philosophy                 0.014711   0.020931   0.036169    0.015887  \n",
       "Health-Mind&Body           0.024036   0.049089   0.040548    0.016028  \n",
       "Professional&Technical     0.011509        NaN   0.009103    0.013298  \n",
       "Science&Nature             0.015283   0.013055   0.014830    0.015186  \n",
       "\n",
       "[22 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(percentage_dict)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c403877-0d9a-4c0e-9a7e-6bb8f11ecd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nota: eliminare tutti i libri con genere Comics&GraphicNovels ? Da valutare\n",
    "\n",
    "#Adesso che abbiamo il dataframe del peso delle varie keyword, possiamo creare il vettore \"generi\" dei libri sui\n",
    "#quali calcolare la similarità.\n",
    "\n",
    "#VETTORI GENERI = [0,0,....,0]\n",
    "#DFgenres.show(200, False)\n",
    "#Il vettore genere è lungo 21 elementi e contiene, nella i-esima posizione, un valore che indica quanto il libro\n",
    "#appartiene a quel genere.\n",
    "\n",
    "mapping = {'Fiction&Literature': 0,\n",
    "              'Family-Sex&Relationships': 1,\n",
    "              'Humor': 2,\n",
    "              'History': 3,\n",
    "              'ScienceFiction&Fantasy': 4,\n",
    "              'Romance': 5,\n",
    "              'Travel': 6,\n",
    "              'Mystery&Thrillers': 7,\n",
    "              'FreeTime': 8,\n",
    "              'Non-fiction': 9,\n",
    "              'Biography': 10,\n",
    "              'SocialScience': 11,\n",
    "              'Political': 12,\n",
    "              'Crime': 13,\n",
    "              'Children&Teens': 14,\n",
    "              'Philosophy': 15,\n",
    "              'Horror': 16,\n",
    "              'Health-Mind&Body': 17,\n",
    "              'Professional&Technical': 18,\n",
    "              'Science&Nature': 19,\n",
    "              'Comics&GraphicNovels': 20}\n",
    "\n",
    "DF_final_pandas = DF_final.toPandas()\n",
    "\n",
    "DF_mapped = pd.Series(DF_final_pandas['genre'].values)\n",
    "\n",
    "def mapSeries(line):\n",
    "    \n",
    "    line = line.replace(\"'\", '\"')\n",
    "    line = json.loads(line)\n",
    "    for key in line:\n",
    "        line[key] = float(line[key])\n",
    "        \n",
    "    total = sum(line.values())\n",
    "    genre_vector = [0 for i in range(21)]\n",
    "    \n",
    "    for key in line:\n",
    "        value_genre = line[key] / total\n",
    "        genre_vector[mapping[key]] = value_genre\n",
    "    \n",
    "    return genre_vector\n",
    "\n",
    "DF_transformed = DF_mapped.map(mapSeries)\n",
    "\n",
    "DF_final_pandas_new = DF_final_pandas.assign(genre_vector = DF_transformed)\n",
    "DF_final_pandas_new['genre_vector'] = DF_final_pandas_new['genre_vector'].astype(str)\n",
    "\n",
    "DF_final_new = spark.createDataFrame(DF_final_pandas_new)\n",
    "\n",
    "DF_final_new = DF_final_new.withColumn('genre_vector', regexp_replace('genre_vector', '\\[', ''))\n",
    "DF_final_new = DF_final_new.withColumn('genre_vector', regexp_replace('genre_vector', '\\]', ''))\n",
    "#DF_final_new = DF_final_new.withColumn('genre_vector', array(DF_final_new.genre_vector))\n",
    "\n",
    "DF_final_new = DF_final_new.withColumn('genre_vector', split(col('genre_vector'), \",\"))\n",
    "\n",
    "#NOTA: eliminare comics? Come migliorare le keywords?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3810bb-0d7f-4ac5-b6ba-dacaac14ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_final_new = DF_final_new.drop('_c0').drop('content2')\n",
    "\n",
    "DF_final_new.toPandas().to_csv('for_now_final_DF_with_genre_vector.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952f9d7b-e299-4b79-90fc-d093c790f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_final = spark.read.csv('for_now_final_DF_with_genre_vector.csv', header=True)\n",
    "#DF_final.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544f68dc-9f17-42a2-b060-a09af70609a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amore</th>\n",
       "      <th>donna</th>\n",
       "      <th>amico</th>\n",
       "      <th>guerra</th>\n",
       "      <th>caso</th>\n",
       "      <th>bambino</th>\n",
       "      <th>morte</th>\n",
       "      <th>ragazzo</th>\n",
       "      <th>viaggio</th>\n",
       "      <th>famiglia</th>\n",
       "      <th>...</th>\n",
       "      <th>popolo</th>\n",
       "      <th>compagno</th>\n",
       "      <th>relazione</th>\n",
       "      <th>crisi</th>\n",
       "      <th>fantascienza</th>\n",
       "      <th>vicino</th>\n",
       "      <th>cielo</th>\n",
       "      <th>angelo</th>\n",
       "      <th>gioia</th>\n",
       "      <th>entusiasmo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>498.000000</td>\n",
       "      <td>489.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family-Sex&amp;Relationships</th>\n",
       "      <td>0.077583</td>\n",
       "      <td>0.071336</td>\n",
       "      <td>0.053017</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.042209</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>0.069515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.079370</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.062677</td>\n",
       "      <td>0.037298</td>\n",
       "      <td>0.024232</td>\n",
       "      <td>0.054202</td>\n",
       "      <td>0.043390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction&amp;Literature</th>\n",
       "      <td>0.465981</td>\n",
       "      <td>0.471666</td>\n",
       "      <td>0.377122</td>\n",
       "      <td>0.420234</td>\n",
       "      <td>0.325622</td>\n",
       "      <td>0.412439</td>\n",
       "      <td>0.400174</td>\n",
       "      <td>0.384013</td>\n",
       "      <td>0.410636</td>\n",
       "      <td>0.497394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385654</td>\n",
       "      <td>0.395387</td>\n",
       "      <td>0.430943</td>\n",
       "      <td>0.404410</td>\n",
       "      <td>0.300122</td>\n",
       "      <td>0.518016</td>\n",
       "      <td>0.424262</td>\n",
       "      <td>0.365695</td>\n",
       "      <td>0.415673</td>\n",
       "      <td>0.388628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humor</th>\n",
       "      <td>0.016840</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.031213</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>0.025319</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>0.016331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023820</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.027124</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.024281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>0.041606</td>\n",
       "      <td>0.063047</td>\n",
       "      <td>0.023806</td>\n",
       "      <td>0.155903</td>\n",
       "      <td>0.029198</td>\n",
       "      <td>0.042920</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167196</td>\n",
       "      <td>0.068632</td>\n",
       "      <td>0.037682</td>\n",
       "      <td>0.046965</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.048887</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>0.047353</td>\n",
       "      <td>0.066561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceFiction&amp;Fantasy</th>\n",
       "      <td>0.122917</td>\n",
       "      <td>0.056813</td>\n",
       "      <td>0.128966</td>\n",
       "      <td>0.134404</td>\n",
       "      <td>0.056636</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.137805</td>\n",
       "      <td>0.187869</td>\n",
       "      <td>0.153997</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148319</td>\n",
       "      <td>0.156112</td>\n",
       "      <td>0.083934</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>0.443558</td>\n",
       "      <td>0.078312</td>\n",
       "      <td>0.147298</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0.030744</td>\n",
       "      <td>0.016683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>0.093787</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053776</td>\n",
       "      <td>0.029639</td>\n",
       "      <td>0.011704</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.009853</td>\n",
       "      <td>0.012518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>0.094860</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.016705</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>0.037910</td>\n",
       "      <td>0.014372</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.026792</td>\n",
       "      <td>0.066776</td>\n",
       "      <td>0.048576</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.029833</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>0.053807</td>\n",
       "      <td>0.098713</td>\n",
       "      <td>0.060331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-fiction</th>\n",
       "      <td>0.008622</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.016767</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.021512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biography</th>\n",
       "      <td>0.018007</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.022155</td>\n",
       "      <td>0.037153</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.027841</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.027955</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048289</td>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.025773</td>\n",
       "      <td>0.050706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mystery&amp;Thrillers</th>\n",
       "      <td>0.023860</td>\n",
       "      <td>0.113707</td>\n",
       "      <td>0.119173</td>\n",
       "      <td>0.048287</td>\n",
       "      <td>0.302206</td>\n",
       "      <td>0.094747</td>\n",
       "      <td>0.134313</td>\n",
       "      <td>0.085447</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.084172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>0.047603</td>\n",
       "      <td>0.092669</td>\n",
       "      <td>0.053594</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>0.136158</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>0.102838</td>\n",
       "      <td>0.035962</td>\n",
       "      <td>0.077176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialScience</th>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>0.015111</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.008942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Political</th>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039924</td>\n",
       "      <td>0.010081</td>\n",
       "      <td>0.013034</td>\n",
       "      <td>0.033289</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime</th>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.048773</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.011087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horror</th>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.009654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FreeTime</th>\n",
       "      <td>0.014366</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.010315</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.013997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.107984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Children&amp;Teens</th>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.050129</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.059050</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.103596</td>\n",
       "      <td>0.019257</td>\n",
       "      <td>0.030642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.065156</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>0.025783</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.017960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comics&amp;GraphicNovels</th>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.011979</td>\n",
       "      <td>0.042839</td>\n",
       "      <td>0.033309</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>0.047796</td>\n",
       "      <td>0.038550</td>\n",
       "      <td>0.030033</td>\n",
       "      <td>0.019749</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016877</td>\n",
       "      <td>0.069637</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>0.044239</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>0.041024</td>\n",
       "      <td>0.039985</td>\n",
       "      <td>0.098449</td>\n",
       "      <td>0.011456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philosophy</th>\n",
       "      <td>0.015951</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.014224</td>\n",
       "      <td>0.025687</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>0.016168</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.020931</td>\n",
       "      <td>0.036169</td>\n",
       "      <td>0.015887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health-Mind&amp;Body</th>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.016028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Professional&amp;Technical</th>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.022275</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.013673</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002879</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.028425</td>\n",
       "      <td>0.038270</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.013298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science&amp;Nature</th>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.012914</td>\n",
       "      <td>0.023090</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>0.013055</td>\n",
       "      <td>0.014830</td>\n",
       "      <td>0.015186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               amore       donna       amico      guerra  \\\n",
       "total                     498.000000  489.000000  342.000000  280.000000   \n",
       "Family-Sex&Relationships    0.077583    0.071336    0.053017    0.023265   \n",
       "Fiction&Literature          0.465981    0.471666    0.377122    0.420234   \n",
       "Humor                       0.016840    0.022616    0.031213    0.010151   \n",
       "History                     0.041606    0.063047    0.023806    0.155903   \n",
       "ScienceFiction&Fantasy      0.122917    0.056813    0.128966    0.134404   \n",
       "Travel                      0.010429    0.004777    0.013312    0.016067   \n",
       "Romance                     0.094860    0.067545    0.049130    0.016181   \n",
       "Non-fiction                 0.008622    0.009733    0.011121    0.006699   \n",
       "Biography                   0.018007    0.030568    0.022155    0.037153   \n",
       "Mystery&Thrillers           0.023860    0.113707    0.119173    0.048287   \n",
       "SocialScience               0.004732    0.012127    0.002910    0.008760   \n",
       "Political                   0.002393    0.002673    0.002544    0.028978   \n",
       "Crime                       0.003655    0.016611    0.015129    0.006342   \n",
       "Horror                      0.005421    0.001785    0.004354    0.002031   \n",
       "FreeTime                    0.014366    0.010027    0.019066    0.011982   \n",
       "Children&Teens              0.030928    0.006799    0.050129    0.013388   \n",
       "Comics&GraphicNovels        0.022945    0.011979    0.042839    0.033309   \n",
       "Philosophy                  0.015951    0.009150    0.012136    0.012090   \n",
       "Health-Mind&Body            0.008408    0.011112    0.008082    0.003879   \n",
       "Professional&Technical      0.007071    0.004376    0.007512    0.006069   \n",
       "Science&Nature              0.003426    0.001554    0.006286    0.004828   \n",
       "\n",
       "                                caso     bambino       morte     ragazzo  \\\n",
       "total                     259.000000  287.000000  313.000000  296.000000   \n",
       "Family-Sex&Relationships    0.023596    0.039989    0.035387    0.042209   \n",
       "Fiction&Literature          0.325622    0.412439    0.400174    0.384013   \n",
       "Humor                       0.025319    0.032527    0.010045    0.015073   \n",
       "History                     0.029198    0.042920    0.055023    0.026359   \n",
       "ScienceFiction&Fantasy      0.056636    0.098300    0.137805    0.187869   \n",
       "Travel                      0.005088    0.010130    0.006077    0.005721   \n",
       "Romance                     0.016705    0.020649    0.025556    0.037910   \n",
       "Non-fiction                 0.016767    0.005724    0.013274    0.005156   \n",
       "Biography                   0.011683    0.028759    0.027841    0.010319   \n",
       "Mystery&Thrillers           0.302206    0.094747    0.134313    0.085447   \n",
       "SocialScience               0.010007    0.015111    0.006198    0.002840   \n",
       "Political                   0.014050    0.008144    0.012973    0.005701   \n",
       "Crime                       0.048773    0.017204    0.024112    0.016097   \n",
       "Horror                      0.005036    0.009023    0.013531    0.005848   \n",
       "FreeTime                    0.004571    0.010315    0.009053    0.005978   \n",
       "Children&Teens              0.007964    0.059050    0.013568    0.103596   \n",
       "Comics&GraphicNovels        0.034410    0.047796    0.038550    0.030033   \n",
       "Philosophy                  0.020695    0.004800    0.010146    0.004765   \n",
       "Health-Mind&Body            0.012000    0.011936    0.018115    0.007593   \n",
       "Professional&Technical      0.014467    0.022275    0.006316    0.013673   \n",
       "Science&Nature              0.015210    0.008160    0.001945    0.003797   \n",
       "\n",
       "                             viaggio    famiglia  ...     popolo   compagno  \\\n",
       "total                     240.000000  333.000000  ...  89.000000  82.000000   \n",
       "Family-Sex&Relationships    0.028915    0.069515  ...   0.005180   0.048247   \n",
       "Fiction&Literature          0.410636    0.497394  ...   0.385654   0.395387   \n",
       "Humor                       0.019540    0.016331  ...   0.023820   0.020268   \n",
       "History                     0.044810    0.070389  ...   0.167196   0.068632   \n",
       "ScienceFiction&Fantasy      0.153997    0.073314  ...   0.148319   0.156112   \n",
       "Travel                      0.093787    0.004235  ...   0.053776   0.029639   \n",
       "Romance                     0.014372    0.043332  ...   0.006921   0.026792   \n",
       "Non-fiction                 0.014925    0.004812  ...   0.007417   0.005030   \n",
       "Biography                   0.027955    0.034424  ...   0.048289   0.019483   \n",
       "Mystery&Thrillers           0.062775    0.084172  ...   0.011586   0.047603   \n",
       "SocialScience               0.015423    0.008942  ...   0.018000   0.001016   \n",
       "Political                   0.002144    0.007543  ...   0.039924   0.010081   \n",
       "Crime                       0.007368    0.010944  ...        NaN   0.006567   \n",
       "Horror                      0.002932    0.004765  ...   0.003977   0.005330   \n",
       "FreeTime                    0.011281    0.005570  ...   0.007668   0.009191   \n",
       "Children&Teens              0.019257    0.030642  ...   0.012645   0.065156   \n",
       "Comics&GraphicNovels        0.019749    0.010063  ...   0.016877   0.069637   \n",
       "Philosophy                  0.017676    0.006015  ...   0.021618   0.004268   \n",
       "Health-Mind&Body            0.014846    0.004485  ...   0.013251   0.001524   \n",
       "Professional&Technical      0.010441    0.008460  ...   0.002879   0.006113   \n",
       "Science&Nature              0.007170    0.004654  ...   0.005005   0.003925   \n",
       "\n",
       "                          relazione      crisi  fantascienza     vicino  \\\n",
       "total                     79.000000  76.000000     74.000000  66.000000   \n",
       "Family-Sex&Relationships   0.079370   0.054967      0.009798   0.062677   \n",
       "Fiction&Literature         0.430943   0.404410      0.300122   0.518016   \n",
       "Humor                      0.034480   0.056701      0.011735   0.027124   \n",
       "History                    0.037682   0.046965      0.005690   0.048887   \n",
       "ScienceFiction&Fantasy     0.083934   0.032689      0.443558   0.078312   \n",
       "Travel                     0.011704   0.012762      0.000808   0.001515   \n",
       "Romance                    0.066776   0.048576      0.003448   0.029833   \n",
       "Non-fiction                0.003093   0.017514      0.014521   0.001459   \n",
       "Biography                  0.017859   0.011307      0.006164   0.004798   \n",
       "Mystery&Thrillers          0.092669   0.053594      0.078507   0.136158   \n",
       "SocialScience              0.016385   0.046775      0.003488   0.009591   \n",
       "Political                  0.013034   0.033289      0.016167   0.002407   \n",
       "Crime                      0.018069   0.013704      0.009382   0.016601   \n",
       "Horror                          NaN   0.007682      0.003187   0.005859   \n",
       "FreeTime                   0.008693   0.013997           NaN   0.003409   \n",
       "Children&Teens             0.006826   0.017954      0.003218   0.016315   \n",
       "Comics&GraphicNovels       0.012547   0.032010      0.044239   0.014367   \n",
       "Philosophy                 0.014224   0.025687      0.011580   0.016168   \n",
       "Health-Mind&Body           0.010375   0.008058      0.007197   0.004098   \n",
       "Professional&Technical     0.028425   0.038270      0.012890   0.000722   \n",
       "Science&Nature             0.012914   0.023090      0.014302   0.001684   \n",
       "\n",
       "                              cielo     angelo      gioia  entusiasmo  \n",
       "total                     66.000000  59.000000  57.000000   41.000000  \n",
       "Family-Sex&Relationships   0.037298   0.024232   0.054202    0.043390  \n",
       "Fiction&Literature         0.424262   0.365695   0.415673    0.388628  \n",
       "Humor                      0.021404   0.024735   0.021127    0.024281  \n",
       "History                    0.044705   0.019869   0.047353    0.066561  \n",
       "ScienceFiction&Fantasy     0.147298   0.210616   0.030744    0.016683  \n",
       "Travel                     0.016991   0.000394   0.009853    0.012518  \n",
       "Romance                    0.039689   0.053807   0.098713    0.060331  \n",
       "Non-fiction                0.007938   0.005008   0.010121    0.021512  \n",
       "Biography                  0.014151   0.004592   0.025773    0.050706  \n",
       "Mystery&Thrillers          0.067922   0.102838   0.035962    0.077176  \n",
       "SocialScience              0.017231   0.013015   0.006968    0.001161  \n",
       "Political                       NaN        NaN        NaN    0.018510  \n",
       "Crime                      0.013578   0.011435   0.004920    0.011087  \n",
       "Horror                     0.001377   0.000924   0.007246    0.009654  \n",
       "FreeTime                   0.008623   0.013996   0.010517    0.107984  \n",
       "Children&Teens             0.030971   0.025783   0.021729    0.017960  \n",
       "Comics&GraphicNovels       0.041024   0.039985   0.098449    0.011456  \n",
       "Philosophy                 0.014711   0.020931   0.036169    0.015887  \n",
       "Health-Mind&Body           0.024036   0.049089   0.040548    0.016028  \n",
       "Professional&Technical     0.011509        NaN   0.009103    0.013298  \n",
       "Science&Nature             0.015283   0.013055   0.014830    0.015186  \n",
       "\n",
       "[22 rows x 100 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_final = spark.read.csv(\"for_now_final_DF_with_genre_vector.csv\", header=True)\n",
    "#DF_final.show(1, False)\n",
    "\n",
    "#Dizionario keyword scelte : percentuale di generi\n",
    "from pyspark.sql.functions import col, array_contains, size\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#Oltre alla nostra lista delle keyword, dovremo usare il solito DF_final.\n",
    "#print(chosen_N)\n",
    "#DF_final = spark.read.csv('anobii_genres/for_now_final_DF4.csv', header=True)\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', '\\[', ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', '\\]', ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\'\", ''))\n",
    "DF_final = DF_final.withColumn('top_keywords', regexp_replace('top_keywords', \"\\, \", ','))\n",
    "#DF_final_new = DF_final_new.withColumn('genre_vector', array(DF_final_new.genre_vector))\n",
    "\n",
    "DF_final = DF_final.withColumn('top_keywords', split(col('top_keywords'), \",\"))\n",
    "\n",
    "#DF_final.show(1, False)\n",
    "#LISTA GIà CREATA\n",
    "chosen_N = ['amore', 'donna', 'amico', 'guerra', 'caso', 'bambino', 'morte', 'ragazzo', 'viaggio', 'famiglia', 'realtà', 'avventura', 'paese', 'idea', 'città', 'padre', 'indagine', 'problema', 'ragazza', 'capo', 'sogno', 're', 'arte', 'ricerca', 'signore', 'giornalista', 'sangue', 'paura', 'cultura', 'occhio', 'gioco', 'raccolta', 'passato', 'tragedia', 'nemico', 'esistenza', 'delitto', 'sistema', 'battaglia', 'lotta', 'genio', 'memoria', 'film', 'macchina', 'scienza', 'notte', 'ordine', 'assassino', 'riflessione', 'studio', 'saga', 'scuola', 'animale', 'diritto', 'acqua', 'quartiere', 'scontro', 'morto', 'poesia', 'classico', 'male', 'ombra', 'orrore', 'artista', 'cronaca', 'inglese', 'compito', 'sentimento', 'mistero', 'destino', 'avvocato', 'bar', 'semplicità', 'figlia', 'stato', 'vento', 'commedia', 'moglie', 'omicidio', 'mare', 'studente', 'notizia', 'chiesa', 'ciclo', 'narratore', 'passione', 'rapporto', 'polizia', 'amicizia', 'lettera', 'popolo', 'compagno', 'relazione', 'crisi', 'fantascienza', 'vicino', 'cielo', 'angelo', 'gioia', 'entusiasmo']\n",
    "#chosen_N = ['amore', 'donna', 'amico', 'morte', 'bambino', 'guerra', 'caso', 'realtà', 'ragazzo', 'figlio', 'paese', 'città', 'idea', 'avventura', 'famiglia', 'viaggio', 'mistero', 'problema', 'ricerca', 'potere', 'raccolta', 'paura', 'polizia', 'padre', 'delitto', 'arte', 'saga', 'giornalista', 'passato', 'film', 'battaglia', 'gioco', 'universo', 'notte', 'capo', 'ragazza', 'pensiero', 'movimento', 'cuore', 'male', 'tragedia', 'speranza', 'morto', 'droga', 'processo', 'animale', 'poesia', 'genio', 'signore', 'mare', 'killer', 'coppia', 'riflessione', 'scoperta', 'aiuto', 'faccia', 'psicologia', 'esistenza', 'cultura', 'dio', 'studio', 'cronaca', 'voce', 'artista', 'trattato', 'assassino', 'natura', 'cane', 'caccia', 'civiltà', 'fantascienza', 'quadro', 'moglie', 'passione', 'strada', 'figlia', 'provincia', 'sangue', 'orrore', 'compagno', 'stato', 'sfida', 'ordine', 'angelo', 'voglia', 'oro', 'rischio', 'rivelazione', 'rapporto', 'sentimento', 'lettera', 'vittima', 'amante', 'relazione', 'testa', 'popolo', 'comunità', 'ciclo', 'ispirazione', 'entusiasmo']\n",
    "def genresPercentage(dataframe, _list):\n",
    "    \n",
    "    DF_final_pandas = dataframe.toPandas() #Local dataframe\n",
    "    percentage_dict = {}\n",
    "    \n",
    "    for keyword in _list:\n",
    "        masking = DF_final_pandas.top_keywords.apply(lambda x: keyword in x)\n",
    "        DF_final_keyword = DF_final_pandas[masking] #Dataframe contenente solo le tuple che contengono la keyword\n",
    "\n",
    "        genre_list = DF_final_keyword['genre'].to_list() #Estraiamo il dizionario con i generi per ogni tupla con la keyword\n",
    "\n",
    "        percentage_dict[keyword] = {'total': len(genre_list)}\n",
    "\n",
    "        for dictionary in genre_list:\n",
    "            dictionary = dictionary.replace(\"'\", '\"')\n",
    "            dictionary = json.loads(dictionary)\n",
    "            \n",
    "            for key in dictionary:\n",
    "\n",
    "                if key in percentage_dict[keyword]:\n",
    "                    percentage_dict[keyword][key] += float(dictionary[key])/sum(list(map(float, dictionary.values()))) #WEIGHTED\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    percentage_dict[keyword][key] = float(dictionary[key])/sum(list(map(float, dictionary.values())))\n",
    "\n",
    "        for key in percentage_dict[keyword]:\n",
    "\n",
    "            if key != 'total':\n",
    "                percentage_dict[keyword][key] = (percentage_dict[keyword][key]) / percentage_dict[keyword]['total']\n",
    "    return percentage_dict\n",
    "        \n",
    "percentage_dict = genresPercentage(DF_final, chosen_N)\n",
    "#print(percentage_dict)\n",
    "\n",
    "dataframe = pd.DataFrame(percentage_dict)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c149648-1e02-4c93-8e52-557a99f0dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAZIONE FILE JSON SECONDO I CANONI DEL TEAM VR\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DF_final_pandas = DF_final.toPandas()\n",
    "DF_final_pandas\n",
    "#print(type(DF_final_pandas['genre_vector']))\n",
    "#DF_final_pandas.groupby(['book_id']).apply(lambda x: x[['genre_vector']].to_dict()).reset_index().to_json(orient='records')\n",
    "\n",
    "#DF_final_pandas.to_json('books.json')\n",
    "#JSON_ = DF_final_pandas.to_dict('records')\n",
    "#print(JSON_[0:5])\n",
    "column_list = []\n",
    "column_keywords_list =  []\n",
    "column_dict_out = []\n",
    "\n",
    "for genre_string in DF_final_pandas['genre_vector']:\n",
    "    \n",
    "    genre_string = genre_string.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "    genre_list = [float(element.replace(\"  \", \"\")) for element in genre_string.split(\",\")]\n",
    "    column_list.append(genre_list)  \n",
    "\n",
    "#for keywords_string in DF_final_pandas['top_keywords']:\n",
    "#    print(keywords_string)\n",
    "#    keywords_string = keywords_string.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "#    keywords_list = [str(element.replace(\" \", \"\")) for element in keywords_string.split(\",\")]\n",
    "#    column_keywords_list.append(keywords_list)\n",
    "\n",
    "for genre_string_dict in DF_final_pandas['genre']: \n",
    "    column_dict = []\n",
    "    genre_string_dict = genre_string_dict.replace(\"'\", '\"')\n",
    "    genre_dict = json.loads(genre_string_dict)\n",
    "    for key in genre_dict:\n",
    "        genre_string = key + \":\" + genre_dict[key]\n",
    "        column_dict.append(genre_string)\n",
    "    column_dict_out.append(column_dict)\n",
    "\n",
    "#print(column_dict_out)\n",
    "DF_final_pandas['genre_vector'] = pd.Series(column_list).values\n",
    "#DF_final_pandas['top_keywords'] = pd.Series(column_keywords_list).values\n",
    "DF_final_pandas['genre'] = pd.Series(column_dict_out).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9ce01-784d-41d7-b408-9c670ee52762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DF_final_pandas.groupby(['book_id']).apply(lambda x: x[['genre_vector']].to_dict('records')).reset_index().to_json(orient='records')\n",
    "#DF_final_pandas.to_json('books.json')\n",
    "\n",
    "dict_ = DF_final_pandas.to_dict()\n",
    "#3426 books\n",
    "\n",
    "#Rearrange the json\n",
    "output_dict = [] #Containing elements with the structure \"book_id\" : \"metadata\"\n",
    "#print(output_dict)\n",
    "with open('books3.json', 'w') as f:\n",
    "    for _id in range(3426):\n",
    "        metadata_dict = {}\n",
    "        for key in dict_:\n",
    "            metadata_dict[key] = dict_[key][_id]\n",
    "            #print(metadata_dict)\n",
    "        #print(metadata_dict['genre'])\n",
    "        output_dict.append(metadata_dict)\n",
    "    json.dump([output_dict], f)\n",
    "        \n",
    "    \n",
    "        \n",
    "#print(output_dict[1])\n",
    "#with open('books.json', 'w') as f:\n",
    "#    json.dump([], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6965652-2d5c-414a-97f0-c1a0541530cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04220918609962281, 0.38401293471235437, 0.015073279637666095, 0.026359271413449275, 1.1878691272146844, 0.005721318750558432, 0.037909893876787684, 0.005155944608543797, 0.010319103301259439, 0.08544732911257419, 0.002840337237227488, 0.00570101315599197, 0.016097342059650174, 0.005848369265480944, 0.0059784899924284676, 0.10359566739038538, 0.03003311045472484, 0.0047654521300289744, 0.007593218028074882, 0.013672686803298662, 0.0037969247552079107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/jupyter/kernels/pyspark_yarn/bdkernelyarn.py:71: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "#Creation of the user vector\n",
    "\n",
    "#Let's simulate choices of a genre and keyword input\n",
    "user_vector = [0 for i in range(21)]\n",
    "chosen_genres = []\n",
    "chosen_keywords = []\n",
    "discarded_genres= []\n",
    "discarded_keywords = []\n",
    "\n",
    "mapping = {'Fiction&Literature': 0,\n",
    "              'Family-Sex&Relationships': 1,\n",
    "              'Humor': 2,\n",
    "              'History': 3,\n",
    "              'ScienceFiction&Fantasy': 4,\n",
    "              'Romance': 5,\n",
    "              'Travel': 6,\n",
    "              'Mystery&Thrillers': 7,\n",
    "              'FreeTime': 8,\n",
    "              'Non-fiction': 9,\n",
    "              'Biography': 10,\n",
    "              'SocialScience': 11,\n",
    "              'Political': 12,\n",
    "              'Crime': 13,\n",
    "              'Children&Teens': 14,\n",
    "              'Philosophy': 15,\n",
    "              'Horror': 16,\n",
    "              'Health-Mind&Body': 17,\n",
    "              'Professional&Technical': 18,\n",
    "              'Science&Nature': 19,\n",
    "              'Comics&GraphicNovels': 20}\n",
    "\n",
    "from operator import add\n",
    "from operator import sub\n",
    "\n",
    "def modifyUserVector(_input, vector, dataframe):\n",
    "    #This function increases the corresponding place in the vector with a value mapped in the dataframe\n",
    "    if _input[0]==\"genre\":\n",
    "        genre_index = mapping[_input[1]]\n",
    "        if _input[2]==\"chosen\":\n",
    "            vector[genre_index] += 1\n",
    "            chosen_genres.append(_input[1])\n",
    "        else:\n",
    "            vector[genre_index] -= 1\n",
    "            discarded_genres.append(_input[1])\n",
    "        \n",
    "    elif _input[0]==\"keyword\":\n",
    "        _list_genre = dataframe[_input[1]].to_list()[1:]\n",
    "        #print(_list_genre)\n",
    "        if _input[2]==\"chosen\":\n",
    "            vector = list( map(add, _list_genre, vector) )\n",
    "            chosen_keywords.append(_input[1])\n",
    "        else:\n",
    "            vector = list( map(sub, _list_genre, vector) )\n",
    "            discarded_keywords.append(_input[1])\n",
    "    return vector\n",
    "\n",
    "input_ = (\"keyword\", \"ragazzo\", \"chosen\")\n",
    "input_2 = (\"genre\", \"ScienceFiction&Fantasy\", \"chosen\")\n",
    "    \n",
    "user_vector = modifyUserVector(input_, user_vector, dataframe)\n",
    "user_vector = modifyUserVector(input_2, user_vector, dataframe)\n",
    "\n",
    "#KEYWORD AND GENRE FILTERING\n",
    "DF_final_pandas_filtered = DF_final_pandas\n",
    "for keyword in chosen_keywords:\n",
    "    masking = DF_final_pandas.top_keywords.apply(lambda x: keyword in x)\n",
    "    DF_final_pandas_filtered = DF_final_pandas_filtered[masking]\n",
    "\n",
    "for genre in chosen_genres:\n",
    "    masking = DF_final_pandas.genre_vector.apply(lambda x: x[mapping[genre]] != 0.0)\n",
    "    DF_final_pandas_filtered = DF_final_pandas_filtered[masking]\n",
    "\n",
    "print(user_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f56cae26-3710-4a72-9b02-5bcb9dc48605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2681429', 0.09517494888329525), ('677211', 0.09519619566309412), ('3650266', 0.15017990931547454), ('841218', 0.10766107997962249), ('852130', 0.12367667007938188)]\n"
     ]
    }
   ],
   "source": [
    "#We have the mutable user vector. Now we should load all the genre_vectors of the books and find the top 10 with more similarity.\n",
    "\n",
    "#To evaluate the similarity, we can use the scipy spatial module\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "book_id_vector_list = DF_final_pandas_filtered[\"book_id\"].to_list()\n",
    "genre_vector_list = DF_final_pandas_filtered[\"genre_vector\"].to_list()\n",
    "#print(genre_vector_list[0][0])\n",
    "book_and_genre = list(zip(book_id_vector_list, genre_vector_list))\n",
    "\n",
    "book_and_similarities = []\n",
    "for _tuple in book_and_genre:\n",
    "    #Calculate similarity between user and genre_vector\n",
    "    sim = spatial.distance.cosine(user_vector, _tuple[1])\n",
    "    book_and_similarities.append((_tuple[0], sim))\n",
    "\n",
    "#Reordering of the similarities list and extraction of the first 10\n",
    "def orderSim(element):\n",
    "    return element[1]\n",
    "import random\n",
    "book_and_similarities.sort(key=orderSim)\n",
    "top10sim = random.sample(book_and_similarities, k=10)\n",
    "#top10sim = book_and_similarities[0:10]\n",
    "\n",
    "top10sim = [element for element in top10sim if element[1] < 0.20]\n",
    "print(top10sim)\n",
    "\n",
    "#With top10sim we can find a \"fake\" list of liked books by the user (supposing a state of cold start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c86eb942-1bb4-4a1a-b2d1-d1ae3aed81e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert</td>\n",
       "      <td>2681429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert</td>\n",
       "      <td>677211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td>3650266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robert</td>\n",
       "      <td>841218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert</td>\n",
       "      <td>852130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  person_id  book_id\n",
       "0    Robert  2681429\n",
       "1    Robert   677211\n",
       "2    Robert  3650266\n",
       "3    Robert   841218\n",
       "4    Robert   852130"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's create the list of fake ratings\n",
    "    \n",
    "#The rating is 5, the user name is a placeholder (let's say, \"Robert\")\n",
    "fake_votes = pd.DataFrame(columns=[\"person_id\", \"book_id\"])\n",
    "if False:\n",
    "    for index, element in enumerate(top10sim):\n",
    "        #print(element[0])\n",
    "        if element[1] >=0.0 and element[1] < 0.20:\n",
    "            fake_votes.loc[index] = ['Robert', element[0], 5]\n",
    "        if element[1] >=0.20 and element[1] < 0.40:\n",
    "            fake_votes.loc[index] = ['Robert', element[0], 4]\n",
    "        if element[1] >=0.40 and element[1] < 0.60:\n",
    "            fake_votes.loc[index] = ['Robert', element[0], 3]\n",
    "        if element[1] >=0.60 and element[1] < 0.80:\n",
    "            fake_votes.loc[index] = ['Robert', element[0], 2]\n",
    "        if element[1] >=0.80 and element[1] <= 1.0:\n",
    "            fake_votes.loc[index] = ['Robert', element[0], 1]\n",
    "            \n",
    "for index, element in enumerate(top10sim):\n",
    "    fake_votes.loc[index] = ['Robert', element[0]]\n",
    "fake_votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74c577af-2c9c-47fb-a6cb-ba3f8885bdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>_c0</th>\n",
       "      <th>author</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>binding</th>\n",
       "      <th>book_index</th>\n",
       "      <th>content</th>\n",
       "      <th>data_type</th>\n",
       "      <th>description</th>\n",
       "      <th>edition</th>\n",
       "      <th>encrypt_item_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>image_url</th>\n",
       "      <th>isbn</th>\n",
       "      <th>no_of_page</th>\n",
       "      <th>product_type</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>title</th>\n",
       "      <th>top_keywords</th>\n",
       "      <th>total_count</th>\n",
       "      <th>total_review</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>total_wishlist</th>\n",
       "      <th>genre_vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2681429</th>\n",
       "      <td>Robert</td>\n",
       "      <td>3109</td>\n",
       "      <td>Melissa Marr</td>\n",
       "      <td>3.5026178010471205</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>5120</td>\n",
       "      <td>Seth non avrebbe mai immaginato di desiderare ...</td>\n",
       "      <td>anobii</td>\n",
       "      <td>Seth non avrebbe mai immaginato di desiderare ...</td>\n",
       "      <td>1</td>\n",
       "      <td>01fba033f55ff1b389</td>\n",
       "      <td>[Romance:15.0, Fiction&amp;Literature:15.0, Scienc...</td>\n",
       "      <td>https://media.anobii.com/covers/01fba033f55ff1...</td>\n",
       "      <td>8876250638</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>Fazi (Lain)</td>\n",
       "      <td>immortale tentazione</td>\n",
       "      <td>fragile eternity</td>\n",
       "      <td>[creatura, ragazzo, re, regina, fianco, arrivo...</td>\n",
       "      <td>382</td>\n",
       "      <td>67</td>\n",
       "      <td>382</td>\n",
       "      <td>125</td>\n",
       "      <td>[0.1595744680851064, 0.0, 0.0, 0.0, 0.68085106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677211</th>\n",
       "      <td>Robert</td>\n",
       "      <td>806</td>\n",
       "      <td>A. Benvenuti</td>\n",
       "      <td>3.7628032345013476</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>16063</td>\n",
       "      <td>&lt;/b&gt;Le cose non sono sempre quel che sembrano....</td>\n",
       "      <td>bct</td>\n",
       "      <td>&lt;/b&gt;Le cose non sono sempre quel che sembrano....</td>\n",
       "      <td>None</td>\n",
       "      <td>01e2962aeeb025a907</td>\n",
       "      <td>[Children&amp;Teens:19.0, Fiction&amp;Literature:19.0,...</td>\n",
       "      <td>https://media.anobii.com/covers/01e2962aeeb025...</td>\n",
       "      <td>8884517893</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Maite Carranza</td>\n",
       "      <td>il clan della lupa</td>\n",
       "      <td>[strega, madre, magia, potere, socio, finale, ...</td>\n",
       "      <td>371</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.15966386554621848, 0.0, 0.0, 0.0, 0.6806722...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650266</th>\n",
       "      <td>Robert</td>\n",
       "      <td>2035</td>\n",
       "      <td>Lisa J. Smith</td>\n",
       "      <td>3.4175824175824174</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>9184</td>\n",
       "      <td>Una scatola acquistata per curiosità in un neg...</td>\n",
       "      <td>anobii</td>\n",
       "      <td>Una scatola acquistata per curiosità in un neg...</td>\n",
       "      <td>1</td>\n",
       "      <td>016f2a7d5ebf06ac7b</td>\n",
       "      <td>[Children&amp;Teens:9.0, Fiction&amp;Literature:6.0, S...</td>\n",
       "      <td>https://media.anobii.com/covers/016f2a7d5ebf06...</td>\n",
       "      <td>8854118737</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-28</td>\n",
       "      <td>Newton Compton (Vertigo, 71)</td>\n",
       "      <td>l'ultima mossa</td>\n",
       "      <td>il gioco proibito</td>\n",
       "      <td>[negozio, disposizione, inferno, cammino, indi...</td>\n",
       "      <td>455</td>\n",
       "      <td>86</td>\n",
       "      <td>123</td>\n",
       "      <td>43</td>\n",
       "      <td>[0.16666666666666666, 0.0, 0.0, 0.0, 0.5833333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841218</th>\n",
       "      <td>Robert</td>\n",
       "      <td>1126</td>\n",
       "      <td>Robert A. Heinlein</td>\n",
       "      <td>3.7589285714285716</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>19854</td>\n",
       "      <td>La capitale dei Nove Mondi è il centro delluni...</td>\n",
       "      <td>anobii</td>\n",
       "      <td>La capitale dei Nove Mondi è il centro dell'un...</td>\n",
       "      <td>None</td>\n",
       "      <td>011cb03f7ab40bb009</td>\n",
       "      <td>[Travel:2.0, Fiction&amp;Literature:16.0, ScienceF...</td>\n",
       "      <td>https://media.anobii.com/covers/011cb03f7ab40b...</td>\n",
       "      <td>A000011373</td>\n",
       "      <td>314</td>\n",
       "      <td>1</td>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>Mondadori (Urania Collezione)</td>\n",
       "      <td>urania collezione 027</td>\n",
       "      <td>cittadino della galassia</td>\n",
       "      <td>[erede, mercato, capitale, pianeta, comprensio...</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>112</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.2711864406779661, 0.0, 0.0, 0.0, 0.69491525...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852130</th>\n",
       "      <td>Robert</td>\n",
       "      <td>869</td>\n",
       "      <td>Alessandro Zabini</td>\n",
       "      <td>3.961864406779661</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>19896</td>\n",
       "      <td>Max Nudge Angel Gasman Fang e Iggy sono il ris...</td>\n",
       "      <td>anobii</td>\n",
       "      <td>Max, Nudge, Angel, Gasman, Fang e Iggy sono il...</td>\n",
       "      <td>1</td>\n",
       "      <td>01fb684cbaba2a2d6c</td>\n",
       "      <td>[FreeTime:11.0, Children&amp;Teens:23.0, ScienceFi...</td>\n",
       "      <td>https://media.anobii.com/covers/01fb684cbaba2a...</td>\n",
       "      <td>8842914967</td>\n",
       "      <td>421</td>\n",
       "      <td>1</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>Nord</td>\n",
       "      <td>maximum ride, vol.2</td>\n",
       "      <td>la scuola è finita</td>\n",
       "      <td>[ragazzo, risultato, voce, capolavore, ospedal...</td>\n",
       "      <td>236</td>\n",
       "      <td>25</td>\n",
       "      <td>236</td>\n",
       "      <td>28</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.6458333333333334, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        person_id   _c0              author      average_rating    binding  \\\n",
       "book_id                                                                      \n",
       "2681429    Robert  3109        Melissa Marr  3.5026178010471205  Hardcover   \n",
       "677211     Robert   806        A. Benvenuti  3.7628032345013476  Paperback   \n",
       "3650266    Robert  2035       Lisa J. Smith  3.4175824175824174  Paperback   \n",
       "841218     Robert  1126  Robert A. Heinlein  3.7589285714285716  Paperback   \n",
       "852130     Robert   869   Alessandro Zabini   3.961864406779661  Hardcover   \n",
       "\n",
       "        book_index                                            content  \\\n",
       "book_id                                                                 \n",
       "2681429       5120  Seth non avrebbe mai immaginato di desiderare ...   \n",
       "677211       16063  </b>Le cose non sono sempre quel che sembrano....   \n",
       "3650266       9184  Una scatola acquistata per curiosità in un neg...   \n",
       "841218       19854  La capitale dei Nove Mondi è il centro delluni...   \n",
       "852130       19896  Max Nudge Angel Gasman Fang e Iggy sono il ris...   \n",
       "\n",
       "        data_type                                        description edition  \\\n",
       "book_id                                                                        \n",
       "2681429    anobii  Seth non avrebbe mai immaginato di desiderare ...       1   \n",
       "677211        bct  </b>Le cose non sono sempre quel che sembrano....    None   \n",
       "3650266    anobii  Una scatola acquistata per curiosità in un neg...       1   \n",
       "841218     anobii  La capitale dei Nove Mondi è il centro dell'un...    None   \n",
       "852130     anobii  Max, Nudge, Angel, Gasman, Fang e Iggy sono il...       1   \n",
       "\n",
       "            encrypt_item_id  \\\n",
       "book_id                       \n",
       "2681429  01fba033f55ff1b389   \n",
       "677211   01e2962aeeb025a907   \n",
       "3650266  016f2a7d5ebf06ac7b   \n",
       "841218   011cb03f7ab40bb009   \n",
       "852130   01fb684cbaba2a2d6c   \n",
       "\n",
       "                                                     genre  \\\n",
       "book_id                                                      \n",
       "2681429  [Romance:15.0, Fiction&Literature:15.0, Scienc...   \n",
       "677211   [Children&Teens:19.0, Fiction&Literature:19.0,...   \n",
       "3650266  [Children&Teens:9.0, Fiction&Literature:6.0, S...   \n",
       "841218   [Travel:2.0, Fiction&Literature:16.0, ScienceF...   \n",
       "852130   [FreeTime:11.0, Children&Teens:23.0, ScienceFi...   \n",
       "\n",
       "                                                 image_url        isbn  \\\n",
       "book_id                                                                  \n",
       "2681429  https://media.anobii.com/covers/01fba033f55ff1...  8876250638   \n",
       "677211   https://media.anobii.com/covers/01e2962aeeb025...  8884517893   \n",
       "3650266  https://media.anobii.com/covers/016f2a7d5ebf06...  8854118737   \n",
       "841218   https://media.anobii.com/covers/011cb03f7ab40b...  A000011373   \n",
       "852130   https://media.anobii.com/covers/01fb684cbaba2a...  8842914967   \n",
       "\n",
       "        no_of_page product_type publication_date  \\\n",
       "book_id                                            \n",
       "2681429        381            1       2009-11-01   \n",
       "677211        None            1             None   \n",
       "3650266        240            1       2011-04-28   \n",
       "841218         314            1       2005-04-01   \n",
       "852130         421            1       2007-06-01   \n",
       "\n",
       "                             publisher              sub_title  \\\n",
       "book_id                                                         \n",
       "2681429                    Fazi (Lain)   immortale tentazione   \n",
       "677211                            None         Maite Carranza   \n",
       "3650266   Newton Compton (Vertigo, 71)         l'ultima mossa   \n",
       "841218   Mondadori (Urania Collezione)  urania collezione 027   \n",
       "852130                            Nord    maximum ride, vol.2   \n",
       "\n",
       "                            title  \\\n",
       "book_id                             \n",
       "2681429          fragile eternity   \n",
       "677211         il clan della lupa   \n",
       "3650266         il gioco proibito   \n",
       "841218   cittadino della galassia   \n",
       "852130         la scuola è finita   \n",
       "\n",
       "                                              top_keywords total_count  \\\n",
       "book_id                                                                  \n",
       "2681429  [creatura, ragazzo, re, regina, fianco, arrivo...         382   \n",
       "677211   [strega, madre, magia, potere, socio, finale, ...         371   \n",
       "3650266  [negozio, disposizione, inferno, cammino, indi...         455   \n",
       "841218   [erede, mercato, capitale, pianeta, comprensio...         112   \n",
       "852130   [ragazzo, risultato, voce, capolavore, ospedal...         236   \n",
       "\n",
       "        total_review total_votes total_wishlist  \\\n",
       "book_id                                           \n",
       "2681429           67         382            125   \n",
       "677211          None        None           None   \n",
       "3650266           86         123             43   \n",
       "841218             9         112             24   \n",
       "852130            25         236             28   \n",
       "\n",
       "                                              genre_vector  \n",
       "book_id                                                     \n",
       "2681429  [0.1595744680851064, 0.0, 0.0, 0.0, 0.68085106...  \n",
       "677211   [0.15966386554621848, 0.0, 0.0, 0.0, 0.6806722...  \n",
       "3650266  [0.16666666666666666, 0.0, 0.0, 0.0, 0.5833333...  \n",
       "841218   [0.2711864406779661, 0.0, 0.0, 0.0, 0.69491525...  \n",
       "852130   [0.0, 0.0, 0.0, 0.0, 0.6458333333333334, 0.0, ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that we have the filtered books dataset, we can use the recommender system to have an output\n",
    "\n",
    "#Let's find the metadata of the extracted books\n",
    "pd.set_option(\"display.max_rows\", 1000, \"display.max_columns\", 1000)\n",
    "DF_pandas_joined = fake_votes.set_index('book_id').join(DF_final_pandas.set_index('book_id'))\n",
    "DF_pandas_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "349c9aae-ee5c-4b08-9f5a-7518ef6d005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d94cbbed8141aa8e33492f414fe50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=690.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3803ad145c4bfa883b2da45cca254f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=190.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dd4c4a2c994f22b6c2756f716ea1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=3988.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c50dab579f4546aea29df95a48cf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=550.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3258e5198da481eb60569dfbe07c158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=122.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2388f6f8bde400f82f4141afc8b496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=265486777.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a456fb7746504fe9a10b2e8f663fb390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=53.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99adfa7b553247baabaced069ef75f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7575386a23f2470985b8dda3af91f6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576294b4d933488ea59816fd00318681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f45bdb3790a4ff4a03437eb55172a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd57b552af8d4ac0a79b3cbc35efa0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f13f3dd0ce043c79fe9a2285291a58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's keep just what we need: description, rating, book_id, person_id\n",
    "\n",
    "DFpandas_joined_drop = DF_pandas_joined[[\"person_id\", \"description\"]]\n",
    "#DFpandas_joined_drop\n",
    "\n",
    "#Let's transform the description into a vector with a sentence transformer like BERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "text_data = DFpandas_joined_drop['description'].to_list()\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(text_data, show_progress_bar=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dc3d09f9-a10e-4af1-8196-a610b784f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_dataset = zip(DF_final_pandas_filtered['title'], DF_final_pandas_filtered['description'])\n",
    "#print(list(text_data_dataset)[0])\n",
    "embed_list = []\n",
    "for text in text_data_dataset:\n",
    "    embed_list.append((text[0], model.encode(text[1], show_progress_bar=False)))\n",
    "\n",
    "#print(embed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8a2540b1-057d-436d-a93f-8f73067c2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested book is cronache del mondo emerso\n",
      "Suggested book is le streghe\n",
      "Suggested book is il dono\n",
      "Suggested book is il libraio di selinunte\n",
      "Suggested book is l'esperimento angel\n"
     ]
    }
   ],
   "source": [
    "#Let's compare our fake log of read books with the dataset\n",
    "#print(embed_list)\n",
    "for book in embeddings:\n",
    "    similarity_ = []\n",
    "    for book2 in embed_list:\n",
    "        #print(len(book2))\n",
    "        sim = spatial.distance.cosine(book, book2[1])\n",
    "        similarity_.append((book2[0], sim))\n",
    "        similarity_.sort(key=orderSim)\n",
    "    print(f\"Suggested book is {similarity_[1][0]}\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301f9b9-a6b2-4caf-b1fc-1aaee004bb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
