{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stock-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE CON I MIEI TENTATIVI DI PREPROCESSING SUL DATABASE DI ANOBii\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "file = \"/data/SMARTDATA/books/anobii/author_item.csv\"\n",
    "DFauthorbooks = spark.read.csv(file,header=True) #MOSTRA I LIBRI RELATIVI AGLI AUTORI\n",
    "#DFauthorbooks.show(1, False)\n",
    "\n",
    "#LIBRI DEGLI AUTORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regulated-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/data/SMARTDATA/books/anobii/author_display.csv\"\n",
    "DFdisplay = spark.read.csv(file,header=True) #MOSTRA GLI AUTORI DEL DATABASE\n",
    "#DFdisplay.show()\n",
    "#DFauthorbooks_withnames = DFauthorbooks.join(DFdisplay, DFauthorbooks.author_id == DFdisplay.author_id).drop(DFdisplay.author_id)\n",
    "#AUTORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infrared-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quanto pare author_display è una tabella contenente tutti gli autori del database di anobii mentre author_items\n",
    "#contiene i diversi libri associati ad ogni autore. \n",
    "#DUBBIO: cosa è author original? Cerchiamo una tupla in cui author_original NON è NULL:\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/item.csv\"\n",
    "DFitems_anobii = spark.read.csv(file,header=True) #MOSTRA I LIBRI DEL DATABASE\n",
    "#DFitems.show()\n",
    "\n",
    "#DFitemsauthor = DFitems_anobii.join(DFauthorbooks_withnames, DFitems_anobii.item_id == DFauthorbooks.item_id).drop(DFitems_anobii.item_id)\n",
    "#DFitemsauthor.filter(DFitemsauthor.author_id_original != \"null\").show(20, False)\n",
    "#DFitemsauthor.show(1, False)\n",
    "#AUTHOR: JOHN FOWLES\n",
    "#DFdisplay.filter(DFdisplay.author_id == \"360704\").show(20, False)\n",
    "#DFdisplay.filter(DFdisplay.author_id_original == \"72565\").show(20, False)\n",
    "\n",
    "#IPOTESI: Sembra che author_id_original sia semplicemente l'id di un vecchio database non a nostra disposizione\n",
    "#dato che non è un autore (author_id) presente in questo DB\n",
    "\n",
    "#Possiamo anche notare come language sia il linguaggio del libro mentre language_correct sia il linguaggio del\n",
    "#libro originale (notare la tupla item_id 1002883)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pregnant-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/data/SMARTDATA/books/anobii/language_mapping.csv\"\n",
    "DFlanguages = spark.read.csv(file,header=True)\n",
    "#DFlanguages.show()\n",
    "\n",
    "#Dobbiamo tenere solo i libri con language=11 #FILTRO LINGUAGGIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excessive-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/data/SMARTDATA/books/anobii/item_recommend.csv\"\n",
    "DFrecommends = spark.read.csv(file,header=True)\n",
    "#DFrecommends.show(50, False)\n",
    "\n",
    "#Questa tabella contiene gli ISBN riferiti all'item Id (quindi item_id non è unico?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "waiting-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per coerenza, procediamo a un filtraggio equivalente a quello fatto per le biblioteche:\n",
    "#FILTRAGGI BASE\n",
    "\n",
    "#FILTRAGGIO 1: solo libri italiani (filtrare language==11)\n",
    "#FILTRAGGIO 2: solo monografie e manoscritti (questo a dire il vero è fattibile in maniera un po' diversa dal caso\n",
    "#delle biblioteche poiché l'attributo \"binding\" indica la rilegatura del libro). Per iniziare, considererò\n",
    "#\"paperback\" (copertina flessibile) e \"hardcover\" (copertina rigida) NOTA: probabilmente ulteriori scremature\n",
    "#da questo punto di vista saranno possibili\n",
    "#FILTRAGGIO 3: eliminare fumetti e magazine (non sono sicuro della loro presenza, controllo)\n",
    "#UPDATE: non solo ci sono ma sono pure tantissimi! Filtraggio doveroso\n",
    "\n",
    "#DFitems.filter(DFitems.title.contains(\"Topolino\")).show(20, False)\n",
    "\n",
    "#FILTRAGGI AVANZATI\n",
    "\n",
    "#FILTRAGGIO 4: Libri con più di 50 voti (Dove prendere i voti?) In item_comment_vote ci sono i commenti ai libri\n",
    "#fatti da vari utenti. Tuttavia, non vedo i voti. item_comment_feedback contiene i commenti veri e propri.\n",
    "#In item_comment_vote sembrano esserci due campi \"total_yes\" e \"total_no\": che siano loro i voti? (Consigliato\n",
    "#Non consigliato). UPDATE: Noto che in item_comment_vote è presente l'average rating: è già qualcosa, tuttavia\n",
    "#vorrei trovare i voti dei singoli utenti.\n",
    "\n",
    "#FILTRAGGIO 5: Pulizie eventuali, su anobii i titoli sembrano già puliti quindi è necessario semplicemente mappare\n",
    "#l'id dell'autore con il nome dell'autore.\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/item_comment_vote.csv\" #MOSTRA INFORMAZIONI SUL VOTO DEGLI UTENTI\n",
    "DFvotes = spark.read.csv(file,header=True)\n",
    "#DFvotes.show(50, False)\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/item_comment_feedback.csv\" #MOSTRA IL VOTO MEDIO\n",
    "DFfeedback = spark.read.csv(file,header=True)\n",
    "#DFfeedback.show(50, False)\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/person_item_recommend.csv\" #MOSTRA I COMMENTI DEGLI UTENTI RELATIVI A UN LIBRO\n",
    "DFcomments = spark.read.csv(file,header=True)\n",
    "#DFcomments.show(50, False)\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/person_item_progress.csv\" #MOSTRA INFORMAZIONI SULLE RACCOMANDAZIONI\n",
    "DFprogress = spark.read.csv(file,header=True)\n",
    "#DFprogress.show(50, False)\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/link_person_item_comment.csv\" #MOSTRA INFORMAZIONI VARIE QUALI LA PRESENZA DI\n",
    "#SPOILER IN UN COMMENTO O LA PRESENZA DI PROFANITà\n",
    "DFinfo = spark.read.csv(file,header=True)\n",
    "#DFinfo.show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optional-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INIZIAMO A FILTRARE USANDO LA ROADMAP DELLA CELLA POCO SOPRA\n",
    "\n",
    "#FILTRAGGIO 1: linguaggio italiano\n",
    "#Mi basterà filtrare da item.csv tutte le tuple con language diverso da 11.\n",
    "\n",
    "DFfilteredlanguageitems = DFitems_anobii.filter(DFitems_anobii.language == \"11\")\n",
    "#DFfilteredlanguageitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 2: solo \"hardcover\" e \"paperback\"\n",
    "\n",
    "DFfilteredbindingitems = DFfilteredlanguageitems.filter((DFfilteredlanguageitems.binding == \"Paperback\") | (DFfilteredlanguageitems.binding == \"Hardcover\"))\n",
    "#DFfilteredbindingitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 3: eliminare i fumetti (per quanto possibile)\n",
    "#Userò come parole chiave personaggi famosi Disney quali Topolino e Paperino e i fumetti Bonelli (che fra gli italiani\n",
    "#sono probabilmente i più gettonati)\n",
    "\n",
    "DFfilteredmagazineitems = DFfilteredbindingitems.filter(~(DFfilteredbindingitems.title.contains(\"Topolino\")) | (DFfilteredbindingitems.title.contains(\"Paperino\"))|(DFfilteredbindingitems.title.contains(\"Tex\"))|(DFfilteredbindingitems.title.contains(\"Dylan Dog\"))|(DFfilteredbindingitems.title.contains(\"Nathan Never\"))|(DFfilteredbindingitems.title.contains(\"Zagor\")))\n",
    "#DFfilteredmagazineitems.show(20, False)\n",
    "\n",
    "#FILTRAGGIO 4: libri con più di X votes. Per vedere i voti di ogni libro dovrei usare il dataframe link_person_item.csv.\n",
    "#Come possiamo notare, alcune recensione hanno valore 0: questo non vuol dire che il libro è stato valutato\n",
    "#atrocemente ma che l'utente ha letto il libro senza inserire voti.\n",
    "\n",
    "#Dobbiamo dunque scremare tali voti da questo dataframe.\n",
    "\n",
    "file = \"/data/SMARTDATA/books/anobii/link_person_item.csv\" #MOSTRA IL VOTO DI UN UTENTE A UN LIBRO\n",
    "DFstars = spark.read.csv(file,header=True)\n",
    "#DFstars.show(50, False)\n",
    "\n",
    "DFstarsfilteredno0 = DFstars.filter(DFstars.item_review > 0)\n",
    "#DFstarsfilteredno0.show(20,False)\n",
    "\n",
    "#Contiamo i libri con più voti e visualizziamoli in ordine discendente\n",
    "\n",
    "DFgrouped = DFstarsfilteredno0.groupby(\"item_id\").count().withColumnRenamed(\"count\",\"total_votes\").sort(\"total_votes\", ascending=False)\n",
    "#I voti dei libri più votati si aggirano intorno ai 10^4 come ordine di grandezza per cui per ora scelgo 300 come\n",
    "#soglia di voti necessaria a rimanere nel dataframe (soggetto a cambiamenti)\n",
    "DFgroupedfiltered = DFgrouped.filter(DFgrouped['total_votes'] > 300)\n",
    "DFjoinbookstars = DFfilteredmagazineitems.join(DFgroupedfiltered, DFfilteredmagazineitems.item_id == DFgroupedfiltered.item_id).drop(DFgroupedfiltered.item_id)\n",
    "\n",
    "#FILTRAGGIO 5: Pulizie varie ed eventuali\n",
    "#DFfilteredlessthan10items.filter(DFfilteredlessthan10items.total_review > 100).show()\n",
    "\n",
    "#NOTA: prendo i titoli con le minuscole per facilità di merging con le biblioteche\n",
    "DFfiltered = DFjoinbookstars.select(\"item_id\", lower(DFjoinbookstars.title), lower(DFjoinbookstars.sub_title), \"isbn\", \"average_rating\", \"total_review\", \"total_wishlist\", DFjoinbookstars['total_votes']).withColumnRenamed(\"lower(title)\", \"title\").withColumnRenamed(\"lower(sub_title)\", \"sub_title\")\n",
    "#DFfiltered.sort(\"total_votes\", ascending=False).show(20, False)\n",
    "#DFfiltered.count() #2516 books (maybe too few? Let's try with 300 WITH 300: 4485, seems fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's join votes and books to get titles (not considering votes with 0)\n",
    "\n",
    "DFvoteswithtitles = DFstarsfilteredno0.join(DFfiltered, DFfiltered.item_id == DFstarsfilteredno0.item_id).select(DFstarsfilteredno0.item_id, \"person_id\", \"title\", str(\"sub_title\"), \"item_review\", \"total_review\", \"average_rating\", \"total_votes\", \"isbn\")\n",
    "#DFvoteswithtitles.filter((DFvoteswithtitles.title == \"il fuggiasco\") & (DFvoteswithtitles.person_id == \"262165\")).show()\n",
    "#DFvoteswithtitles.filter(DFvoteswithtitles.title == \"la custode di libri\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "challenging-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lightfm import LightFM\n",
    "from pyspark.sql.functions import row_number, lit, dense_rank\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "#Per ora proverò a costruire una CSR (matrice sparsa compressa) dal dataset di anobii (senza join con le biblioteche,\n",
    "#solo per testare). La CSR è costruita in modo tale da avere tre vettori numpy, uno per i dati e due per gli indici\n",
    "#riga-colonna con cui poi ricostruire la matrice originale.\n",
    "\n",
    "#Come posso passare dal dataset a questa matrice? La maniera più efficiente (ed è meglio adottarle dato le grandezze\n",
    "#dei dataset) è quella di creare la matrice direttamente, senza prima crearne una vuota da riempire.\n",
    "\n",
    "#Supposto che le righe di tale matrice siano gli utenti e le colonne siano i libri (e i dati siano per ora solo\n",
    "#i voti, poi potremo aggiungere feature per i metadati) allora posso provare ad aggiungere al mio dataframe delle \n",
    "#colonne indicanti gli indici futuri di riga colonna, usando degli id incrementali.\n",
    "\n",
    "#Proviamo. (Forse con gli RDD è meglio)\n",
    "RDDvoteswithtitles = DFvoteswithtitles.rdd.cache()\n",
    "#RDDpair = RDDvoteswithtitles.map(lambda x: (x.person_id, list(x))).sortByKey()#.collect()\n",
    "\n",
    "def create_user_dictionary(rdd): #Used to assign an integer id to each user of the rdd (to get the rows of the CSR)\n",
    "    rdd = rdd.map(lambda x: (str(x.person_id), list(x))).sortByKey()\n",
    "    user_dictionary = rdd.countByKey()\n",
    "    i = 0\n",
    "    for key in user_dictionary.keys():\n",
    "        user_dictionary[key] = i\n",
    "        i += 1\n",
    "    return user_dictionary\n",
    "\n",
    "def create_book_dictionary(rdd): #Used to assign an integer id to each book of the rdd (to get the column of the CSR)\n",
    "    rdd = rdd.map(lambda x: (x.book_id, list(x))).sortByKey()\n",
    "    book_dictionary = rdd.countByKey()\n",
    "    i = 0\n",
    "    for key in book_dictionary.keys():\n",
    "        book_dictionary[key] = i\n",
    "        i += 1\n",
    "    return book_dictionary\n",
    "\n",
    "#user_dictionary = create_user_dictionary(RDDvoteswithtitles)\n",
    "#book_dictionary = create_book_dictionary(RDDvoteswithtitles)\n",
    "\n",
    "def addDataType1(line):\n",
    "    return Row(line.item_id, line.person_id, line.title, str(line.sub_title), line.item_review, line.total_review, line.average_rating, str(line.total_votes), line.isbn, \"anobii\")\n",
    "RDDmapped = RDDvoteswithtitles.map(addDataType1)\n",
    "\n",
    "#print(RDDmapped.take(20))\n",
    "\n",
    "DFwithindexes = RDDmapped.toDF([\"item_id\", \"person_id\", \"title\", \"sub_title\",  \"item_review\", \"total_review\", \"average_rating\", \"total_votes\", \"isbn\", \"data_type\"], sampleRatio = 0.9) #Per evitare il problema del sampling in sub_title\n",
    "#DFwithindexes.show(20, False)\n",
    "#DFvoteswithtitles.filter((DFvoteswithtitles.title == \"il fuggiasco\") & (DFvoteswithtitles.person_id == \"262165\")).show()\n",
    "#DFwithindexes contiene i voti di ogni utente dato il libro, altre info utili e gli indici di riga e di colonna\n",
    "#della eventuale compressed sparse matrix per una facile ed efficiente creazione (per i sistemi di raccomandazione)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "desperate-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Togli oggetti inutili: Prima 290125  - Dopo 257054\n",
      "Togli libri in lingua straniera: Prima 290125  - Dopo 228059\n",
      "Togli Fumetti: Prima 290125  - Dopo 227662\n",
      "prefiltrati: 210633 - filtrati: 17702\n",
      "prefiltrati: 210633 - filtrati: 17929\n",
      "prefiltrati: 998403 - filtrati: 215446\n",
      "prefiltrati: 5484078 - filtrati: 2154533\n",
      "i libri sono: 17929\n"
     ]
    }
   ],
   "source": [
    "#Fatto ciò, riprendiamo il database delle biblioteche filtrato (come si importa un altro file .ipynb?)\n",
    "\n",
    "%run cleanedData.ipynb #Così a quanto pare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prescribed-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per adesso, concentriamoci su DFloand_definitive (i prestiti scremati).\n",
    "DFauthorsandbooks = DFdisplay.join(DFauthorbooks, DFdisplay.author_id == DFauthorbooks.author_id).drop(DFdisplay.author_id).filter(DFdisplay.language==\"11\")\n",
    "#DFauthorsandbooks.filter(DFauthorsandbooks.item_id == \"1254828\").show()\n",
    "#DFauthorsandbooks.filter(DFauthorsandbooks.item_id==\"651589\").show(20, False)\n",
    "DFwithindexes_and_authors = DFwithindexes.join(DFauthorsandbooks, DFwithindexes.item_id == DFauthorsandbooks.item_id).select(DFwithindexes.item_id, \"person_id\", \"title\", \"sub_title\", \"author_name\", \"item_review\", \"total_review\", \"average_rating\", \"total_votes\", \"isbn\", \"data_type\")\n",
    "#Prendo i prestiti (che alla fine, è ciò che mi interessa) con i titoli che, per una formattazione più agevole, rendo minuscoli.\n",
    "DFloans_with_titles = DFloans_definitive.join(DFmanifestations_definitive, DFloans_definitive.manifestation_id_new == DFmanifestations_definitive.manifestation_id_new).select(DFloans_definitive.manifestation_id_new, \"patron_id_md5\", \"author\", lower(DFmanifestations_definitive.title)).withColumnRenamed(\"lower(title)\", \"title\")\n",
    "\n",
    "#DFwithindexes_and_authors.filter((DFwithindexes_and_authors.title == \"il fuggiasco\") & (DFwithindexes_and_authors.person_id == \"262165\")).show()\n",
    "#Di cosa ho bisogno in loans_definitive per renderlo \"simile\" alle tuple di anobii? Certamente avrò bisogno del titolo, poi del manifestation_id (item_id), del cliente della biblioteca\n",
    "#(orrispettivo di person_id), di assegnare una item_review provvisoria (abbiamo stabilito, almeno per ora, 4 NOTA: l'average rating lo setto a NULL nelle tuple o lo ricalcolo considerando le \n",
    "#biblioteche? Da vedere), di settare le total_review (per ora a NULL?), i total_votes, l'isbn e lo user e book index (a NULL?)\n",
    "\n",
    "#Il problema di molti campi che lascerei a NULL è che in alcuni casi i titoli non sono esattamente gli stessi a causa della formattazione dei database delle biblioteche. Per stemperare il problema,\n",
    "#proverò a scindere titolo e sottotitoli in due campi differenti (alla maniera di anobii) e a pulire eventuali simboli strani (ad esempio, i numeri all'inizio di alcuni titoli).\n",
    "\n",
    "RDDloans_with_titles = DFloans_with_titles.rdd\n",
    "\n",
    "def title_and_subtitle(line): #Usata per splittare titolo e sottotitolo nel database delle biblioteche e renderli più simili ad anobii\n",
    "    if len(line.title.split(\" : \")) > 1: #Il libro possiede un sottotitolo\n",
    "        title = line.title.split(\" : \")[0]\n",
    "        sub_title = line.title.split(\" : \")[1]\n",
    "        \n",
    "        return Row(line.manifestation_id_new, line.patron_id_md5, title, sub_title, line.author, 4, \"None\", \"None\", \"None\", \"None\",\"biblioteche\") #NE APPROFITTO ANCHE PER INSERIRE VOTO E CAMPI PER IL MERGE (PER ORA NULL)\n",
    "    else: #Il libro non possiede un sottotitolo\n",
    "        return Row(line.manifestation_id_new, line.patron_id_md5, line.title, \"None\", line.author, 4, \"None\", \"None\", \"None\", \"None\", \"biblioteche\")\n",
    "    \n",
    "RDDloans_for_merge = RDDloans_with_titles.map(title_and_subtitle)  \n",
    "\n",
    "DFloans_to_merge = RDDloans_for_merge.toDF([\"item_id\", \"person_id\", \"title\", \"author_name\", \"sub_title\", \"item_review\", \"total_review\", \"average_rating\", \"total_votes\", \"isbn\", \"data_type\"])\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "DFmerged = DFwithindexes_and_authors.union(DFloans_to_merge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "solar-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MERGE AVVENUTO (sperando di averlo implementato bene...)\n",
    "\n",
    "#Adesso vorrei cercare di sostituire i book_id delle biblioteche con quelli del corrispettivo titolo di anobii. Ciò aiuterà quando creerò la colonna degli indici per la compressed sparse matrix.\n",
    "#Inoltre, vorrei che la funzione aggiornasse le metriche delle tuple delle biblioteche, per ora cosparse di \"None\".\n",
    "\n",
    "#Come discusso nel meeting, vorrei inoltre eliminare dal mio dataframe mergiato tutti quei titoli con una sola parola (che di solito si riferiscono a guide turistiche).\n",
    "\n",
    "#Infine, vorrei inserire gli indici di colonna e di riga della compressed sparse matrix in modo da iniziare a preparare l'input per la libreria di raccomandazione (ho già implementato le funzioni\n",
    "#per farlo, a patto che le chiavi usate per il dizionario non siano erronee).\n",
    "\n",
    "#FASE 1: i libri con lo stesso titolo dovrebbero avere lo stesso book_id (e le metriche dovrebbero essere aggiornate di conseguenza).\n",
    "#Ciò è fattibile con la window di SQL e una partitionby. Controlliamo, con un sampling casuale, se ci sono due libri provenienti dai due diversi database (controllare data_type).\n",
    "\n",
    "#DFmerged.filter(DFmerged.title == \"la custode di libri\").show() #TROVATO ERRORE DI RIPETIZIONE TUPLE A CAUSA DELLA MANCATA SCREMATURA DEL LINGUAGGIO DURANTE IL JOIN PER LE INFO SULL'AUTORE (SISTEMATO)\n",
    "\n",
    "#ERRORE 2: alcuni libri di anobii sono ripetuti due volte con un autore diverso (ma perché?), da sistemare con un'aggregazione\n",
    "#Prima, però. filtriamo via i titoli con una parola (dovrebbero essere pochi rispetto alla cardinalità totale e ci semplifica la vita con i libri omonimi)\n",
    "\n",
    "DFmerged_no_onewordtitles = DFmerged.filter(DFmerged.title.contains(\" \")) #Se contiene uno spazio allora non è una sola parola (sperando di aver formattato bene)\n",
    "#DFmerged_no_onewordtitles.filter(DFmerged_no_onewordtitles.title == \"barcellona\").show()\n",
    "#DFmerged.filter(DFmerged.title == \"barcellona\").show()\n",
    "\n",
    "#Procediamo adesso con la sostituzione dei book_id delle biblioteche con quelli di anobii\n",
    "DFmerged_aggregated = DFmerged_no_onewordtitles.select(\"*\", F.min(DFmerged_no_onewordtitles.item_id).over(Window.partitionBy(DFmerged_no_onewordtitles.title)).alias(\"book_id\"))\n",
    "\n",
    "#Aggreghiamo adesso ripetizioni di uno stessa coppia (item_id, user_id) (PER ELIMINARE LE TUPLE RIPETUTE, VEDESI ERRORE 2 DOVUTO ALLA FORMATTAZIONE DI ANOBII)\n",
    "#DFmerged_aggregated.filter(DFmerged_aggregated.title == \"la custode di libri\").show()\n",
    "\n",
    "#Troviamo \"new_author\" formattato in maniera coerente fra i database mergiati\n",
    "DFmerged_aggregated_norepetition = DFmerged_aggregated.select(\"*\", F.min(DFmerged_aggregated.author_name).over(Window.partitionBy(DFmerged_aggregated.book_id)).alias(\"new_author\"))\n",
    "\n",
    "#DFmerged_aggregated_norepetition.filter(DFmerged_aggregated_norepetition.title == \"la custode di libri\").show()\n",
    "\n",
    "#Infine, eliminiamo le ridondanze con dropDuplicates.\n",
    "\n",
    "DFmerged_aggregated_norepetition_unique = DFmerged_aggregated_norepetition.dropDuplicates(['book_id', 'person_id'])\n",
    "#DFmerged_aggregated_norepetition_unique.filter(DFmerged_aggregated_norepetition_unique.title == \"la custode di libri\").show()\n",
    "\n",
    "#Infine, droppiamo gli attributi che non servono più e rinominiamo quelli nuovi e più utili\n",
    "DFmerged_final = DFmerged_aggregated_norepetition_unique.drop(\"item_id\").drop(\"author_name\")\n",
    "#ERRORE 2 SISTEMATO\n",
    "#DFmerged_final.filter(DFmerged_final.book_id == \"1254828\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFmerged_final.orderBy(rand()).show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "limiting-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adesso che il merge e gli attributi sono un po' più solidi possiamo provare a calcolare qualche metrica anche per le tuple della biblioteca\n",
    "#Prima però vorrei controllare una cosa\n",
    "\n",
    "#DFmerged_final.filter(DFmerged_final.title == \"1 ora\").show()\n",
    "#DFmerged_final.filter(DFmerged_final.title == \"un'ora\").show()\n",
    "\n",
    "#Ho provato superficialmente a considerare libri con un numero nel titolo per controllare se fossero formattati diversamente, continuerò il controllo più tardi\n",
    "#Una metrica da considerare per le tuple \"biblioteche\" potrebbe essere il numero di prestiti per un certo libro (sapere che un prestito è avvenuto 20 o 2000 volte è certamente un'info notevole)\n",
    "\n",
    "#Nota: sarebbe consono anche aggiornare le metriche di anobii post raggruppamento\n",
    "DFmerged_count = DFmerged_final.select(\"*\", F.count(\"*\").over(Window.partitionBy(DFmerged_final.book_id, DFmerged_final.data_type)).alias(\"total_count\"))\n",
    "#DFmerged_count.orderBy(rand()).show(50, False)\n",
    "DFmerged_average = DFmerged_count.select(\"*\", F.avg(DFmerged_final.item_review).over(Window.partitionBy(DFmerged_final.book_id, DFmerged_final.data_type)).alias(\"new_average_rating\"))\n",
    "DFmerged_maxreviews = DFmerged_average.select(\"*\", F.max(DFmerged_final.total_review).over(Window.partitionBy(DFmerged_final.book_id, DFmerged_final.data_type)).alias(\"new_total_review\"))\n",
    "\n",
    "#NOTA: l'average rating per le tuple delle biblioteche sarà, almeno per ora, ovviamente 4.0 per via dei voti fissi. new_number_reviews rappresenta il numero massimo di recensioni in caso di incongruenze.\n",
    "#DFmerged_sumreviews.orderBy(rand()).show(50, False)\n",
    "\n",
    "#Infine, inseriamo l'isbn anche per le tuple delle biblioteche  che abbiano una corrispondenza in anobii(potrebbe servire ed è anche utile per ricercare eventuali libri dalla formattazione errata)\n",
    "DFmerged_isbn = DFmerged_maxreviews.select(\"*\", F.min(DFmerged_maxreviews.isbn).over(Window.partitionBy(DFmerged_final.book_id)).alias(\"new_isbn\"))\n",
    "#DFmerged_isbn.orderBy(rand()).show(20, False)  \n",
    "\n",
    "#DFmerged_isbn.orderBy(rand()).filter(DFmerged_isbn.title == \"i giorni dell'arcobaleno\").show(50) #LIBRO PRESENTE SOLO NELLE BIBLIOTECHE TROVATO CON ISBN == None\n",
    "\n",
    "#Adesso droppiamo gli attributi inutili e rinominiamo quelli utili\n",
    "\n",
    "DFmerged_almost_definitive = DFmerged_isbn.drop(\"total_review\").drop(\"total_votes\").drop(\"average_rating\").drop(\"isbn\").withColumnRenamed(\"total_count\", \"total_votes_or_loans\").withColumnRenamed(\"new_average_rating\", \"average_rating\").withColumnRenamed(\"new_isbn\", \"isbn\").withColumnRenamed(\"new_total_review\", \"total_review\").withColumnRenamed(\"new_author\", \"author\")\n",
    "\n",
    "#DFmerged_almost_definitive.orderBy(rand()).show(20, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adesso è il momento di attaccare gli indici di riga e colonna per la compressed sparse matrix tramite le funzioni che ho creato qualche cella fa.\n",
    "\n",
    "#DFmerged_almost_definitive.filter(typeof(DFmerged_almost_definitive.person_id) != str).show()\n",
    "RDDmerged_almost_definitive = DFmerged_almost_definitive.rdd\n",
    "#from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "book_dictionary = create_book_dictionary(RDDmerged_almost_definitive)\n",
    "user_dictionary = create_user_dictionary(RDDmerged_almost_definitive)\n",
    "#n_items = take(20, user_dictionary.items())\n",
    "\n",
    "#print(n_items)\n",
    "def addRowColumnId(line): #Funzione usata per aggiungere gli indici per ogni libro e utente\n",
    "    book_number = book_dictionary[line.book_id] \n",
    "    user_number = user_dictionary[line.person_id]\n",
    "    \n",
    "    return Row(line.person_id, line.title, str(line.sub_title), line.item_review, line.data_type, line.book_id, line.author, line.total_votes_or_loans, line.average_rating, line.total_review, line.isbn, str(user_number), str(book_number))\n",
    "RDDdefinitive = RDDmerged_almost_definitive.map(addRowColumnId)\n",
    "\n",
    "DFdefinitive = RDDdefinitive.toDF([\"person_id\", \"title\", \"sub_title\", \"item_review\", \"data_type\", \"book_id\", \"author\", \"total_votes_or_loans\", \"average_rating\", \"total_review\", \"isbn\", \"user_index\", \"book_index\"])\n",
    "DFdefinitive.show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-small",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
